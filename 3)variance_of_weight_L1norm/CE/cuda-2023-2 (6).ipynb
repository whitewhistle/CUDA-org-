{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3989074,"sourceType":"datasetVersion","datasetId":2367101}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-24T12:06:37.411666Z","iopub.execute_input":"2024-01-24T12:06:37.412422Z","iopub.status.idle":"2024-01-24T12:06:37.770880Z","shell.execute_reply.started":"2024-01-24T12:06:37.412384Z","shell.execute_reply":"2024-01-24T12:06:37.770023Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/smoker-status-prediction-using-biosignals/train_dataset.csv\n/kaggle/input/smoker-status-prediction-using-biosignals/test_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\n\nimport argparse, os, shutil, time, random, math\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.utils.data as data\nimport torch.nn.functional as F\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:37.772397Z","iopub.execute_input":"2024-01-24T12:06:37.772775Z","iopub.status.idle":"2024-01-24T12:06:39.217517Z","shell.execute_reply.started":"2024-01-24T12:06:37.772750Z","shell.execute_reply":"2024-01-24T12:06:39.216569Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"bcl.py","metadata":{}},{"cell_type":"code","source":"\"\"\"\nAuthor: Yonglong Tian (yonglong@mit.edu)\nDate: May 07, 2020\n\"\"\"\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport math\nimport torch.nn.functional as F\nimport numpy as np\n\n\n\nclass BalSCL(nn.Module):\n    def __init__(self, cls_num_list=None, temperature=0.1):\n        super(BalSCL, self).__init__()\n        self.temperature = temperature\n        self.cls_num_list = cls_num_list\n\n    def forward(self, centers1, features, targets, ):\n\n        device = (torch.device('cuda')\n                  if features.is_cuda\n                  else torch.device('cpu'))\n        batch_size = features.shape[0]\n        targets = targets.contiguous().view(-1, 1)\n        targets_centers = torch.arange(len(self.cls_num_list), device=device).view(-1, 1)\n        targets = torch.cat([targets.repeat(2, 1), targets_centers], dim=0)\n        batch_cls_count = torch.eye(len(self.cls_num_list))[targets].sum(dim=0).squeeze()\n\n        mask = torch.eq(targets[:2 * batch_size], targets.T).float().to(device)\n        logits_mask = torch.scatter(\n            torch.ones_like(mask),\n            1,\n            torch.arange(batch_size * 2).view(-1, 1).to(device),\n            0\n        )\n        mask = mask * logits_mask\n        \n        # class-complement\n        features = torch.cat(torch.unbind(features, dim=1), dim=0)\n        features = torch.cat([features, centers1], dim=0)\n        logits = features[:2 * batch_size].mm(features.T)\n        logits = torch.div(logits, self.temperature)\n\n        # For numerical stability\n        logits_max, _ = torch.max(logits, dim=1, keepdim=True)\n        logits = logits - logits_max.detach()\n\n        # class-averaging\n        exp_logits = torch.exp(logits) * logits_mask\n        per_ins_weight = torch.tensor([batch_cls_count[i] for i in targets], device=device).view(1, -1).expand(\n            2 * batch_size, 2 * batch_size + len(self.cls_num_list)) - mask\n        exp_logits_sum = exp_logits.div(per_ins_weight).sum(dim=1, keepdim=True)\n        \n        log_prob = logits - torch.log(exp_logits_sum)\n        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n\n        loss = - mean_log_prob_pos\n        loss = loss.view(2, batch_size).mean()\n        return loss\n\n\n\n\nclass LogitAdjust(nn.Module):\n\n    def __init__(self, cls_num_list, tau=1, weight=None):\n        super(LogitAdjust, self).__init__()\n        cls_num_list = torch.cuda.FloatTensor(cls_num_list)\n        cls_p_list = cls_num_list / cls_num_list.sum()\n        m_list = tau * torch.log(cls_p_list)\n        self.m_list = m_list.view(1, -1)\n        self.weight = weight\n\n    def forward(self, x, target):\n        x_m = x + self.m_list\n        return F.cross_entropy(x_m, target, weight=self.weight)\n\n\nclass BCLLoss(nn.Module):\n    def __init__(self, cls_num_list, tau=1, weight=None, temperature = 0.1, alpha=2.0, beta=0.6 ):\n        super(BCLLoss, self).__init__()\n        self.criterion_ce = LogitAdjust(cls_num_list).cuda()\n        self.criterion_scl = BalSCL(cls_num_list, temperature).cuda()\n        self.alpha = alpha\n        self.beta = beta\n        \n    def forward(self, centers,  logits, features, targets):\n        scl_loss = self.criterion_scl(centers, features, targets)\n        ce_loss = self.criterion_ce(logits, targets)\n\n        return self.alpha * ce_loss + self.beta * scl_loss\n\n\n        \n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:39.218958Z","iopub.execute_input":"2024-01-24T12:06:39.219478Z","iopub.status.idle":"2024-01-24T12:06:39.239848Z","shell.execute_reply.started":"2024-01-24T12:06:39.219440Z","shell.execute_reply":"2024-01-24T12:06:39.238945Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"bs.py","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BS(nn.Module):\n    def __init__(self, dist):\n        super().__init__()\n        dist = torch.from_numpy(np.array(dist)).float().cuda()\n        self.prob = dist / sum(dist)\n        self.log_prior = torch.log(self.prob).unsqueeze(0)\n        \n    def forward(self, logits, targets, epoch=None, reduction='mean'):\n        adjusted_logits = logits + self.log_prior\n        return F.cross_entropy(adjusted_logits, targets, reduction = reduction)\n        \n        \n        # targets = F.one_hot(targets, num_classes=logits.size(1))\n        # logits = logits + torch.log(self.prob.view(1, -1).expand(logits.shape[0], -1)).cuda()\n        \n        # if reduction == 'none':\n        #     return -(torch.sum(F.log_softmax(logits, dim=1) * targets, dim=1))\n        # else:\n        #     return -torch.mean(torch.sum(F.log_softmax(logits, dim=1) * targets, dim=1))","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:39.242240Z","iopub.execute_input":"2024-01-24T12:06:39.242517Z","iopub.status.idle":"2024-01-24T12:06:39.256278Z","shell.execute_reply.started":"2024-01-24T12:06:39.242492Z","shell.execute_reply":"2024-01-24T12:06:39.255424Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"ce drw","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass CE_DRW(nn.Module):\n    \n    def __init__(self, cls_num_list, reweight_epoch=160):\n        super(CE_DRW, self).__init__()\n        self.cls_num_list = cls_num_list\n        self.reweight_epoch= reweight_epoch\n        \n    def drw(self, epoch):\n        idx = epoch // self.reweight_epoch\n        betas = [0, 0.9999]\n        effective_num = 1.0 - np.power(betas[idx], self.cls_num_list)\n        per_cls_weights = (1.0 - betas[idx]) / np.array(effective_num)\n        per_cls_weights = per_cls_weights / np.sum(per_cls_weights) * len(self.cls_num_list)\n        per_cls_weights = torch.FloatTensor(per_cls_weights).cuda()\n        self.weight = per_cls_weights\n\n    def forward(self, x, target, epoch, reduction='mean'):\n        self.drw(epoch)\n        return F.cross_entropy(x, target, weight=self.weight, reduction=reduction)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:39.257468Z","iopub.execute_input":"2024-01-24T12:06:39.257823Z","iopub.status.idle":"2024-01-24T12:06:39.270827Z","shell.execute_reply.started":"2024-01-24T12:06:39.257791Z","shell.execute_reply":"2024-01-24T12:06:39.269823Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"ce","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CE(nn.Module):\n    def __init__(self, weight=None):\n        super().__init__()\n        self.weight = weight\n    def forward(self, logits, targets, epoch=None, reduction='mean'):\n        # targets = F.one_hot(targets, num_classes=logits.size(1))\n        # if reduction == 'mean':\n        #     return -torch.mean(torch.sum(F.log_softmax(logits, dim=1) * targets, dim=1))\n        # else:\n        #     return -(torch.sum(F.log_softmax(logits, dim=1) * targets, dim=1))\n\n        return F.cross_entropy(logits, targets, weight = self.weight, reduction = reduction)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:39.272138Z","iopub.execute_input":"2024-01-24T12:06:39.272468Z","iopub.status.idle":"2024-01-24T12:06:39.281880Z","shell.execute_reply.started":"2024-01-24T12:06:39.272436Z","shell.execute_reply":"2024-01-24T12:06:39.280861Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"ldam drw","metadata":{}},{"cell_type":"code","source":"import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nclass LDAM_DRW(nn.Module):\n    def __init__(self, cls_num_list, reweight_epoch, max_m=0.5, s=30):\n        super(LDAM_DRW, self).__init__()\n        self.cls_num_list = cls_num_list\n        self.reweight_epoch = reweight_epoch\n        m_list = 1.0 / np.sqrt(np.sqrt(cls_num_list))\n        m_list = m_list * (max_m / np.max(m_list))\n        m_list = torch.cuda.FloatTensor(m_list)\n        self.m_list = m_list\n        assert s > 0\n        self.s = s\n      \n    def drw(self, epoch):\n        idx = epoch // self.reweight_epoch\n        betas = [0, 0.9999]\n        effective_num = 1.0 - np.power(betas[idx], self.cls_num_list)\n        per_cls_weights = (1.0 - betas[idx]) / np.array(effective_num)\n        per_cls_weights = per_cls_weights / np.sum(per_cls_weights) * len(self.cls_num_list)\n        per_cls_weights = torch.FloatTensor(per_cls_weights).cuda()\n        self.weight = per_cls_weights\n\n\n    def forward(self, x, target, epoch=None, reduction='mean'):\n        self.drw(epoch)\n        index = torch.zeros_like(x, dtype=torch.uint8)\n        index.scatter_(1, target.data.view(-1, 1), 1)\n        \n        index_float = index.type(torch.cuda.FloatTensor)\n        batch_m = torch.matmul(self.m_list[None, :], index_float.transpose(0,1))\n        batch_m = batch_m.view((-1, 1))\n        x_m = x - batch_m\n    \n        output = torch.where(index, x_m, x)\n        return F.cross_entropy(self.s*output, target, weight=self.weight, reduction=reduction)","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:39.283018Z","iopub.execute_input":"2024-01-24T12:06:39.283957Z","iopub.status.idle":"2024-01-24T12:06:39.299939Z","shell.execute_reply.started":"2024-01-24T12:06:39.283912Z","shell.execute_reply":"2024-01-24T12:06:39.299078Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"ncl","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ndef NBOD(inputs, factor):\n\n    classifier_num = len(inputs)\n    if classifier_num == 1:\n        return 0\n    logits_softmax = []\n    logits_logsoftmax = []\n    for i in range(classifier_num):\n        logits_softmax.append(F.softmax(inputs[i], dim=1))\n        logits_logsoftmax.append(torch.log(logits_softmax[i] + 1e-9))\n\n    loss_mutual = 0\n    for i in range(classifier_num):\n        for j in range(classifier_num):\n            if i == j:\n                continue\n            loss_mutual += factor * F.kl_div(logits_logsoftmax[i], logits_softmax[j],reduction='batchmean')\n    loss_mutual /= (classifier_num - 1)\n    return  loss_mutual\n\nclass NIL_NBOD(nn.Module):\n    def __init__(self, args, num_class_list):\n        super(NIL_NBOD, self).__init__()\n        self.args = args\n        self.num_class_list = num_class_list\n        self.bsce_weight = torch.FloatTensor(self.num_class_list).cuda()\n\n\n        self.multi_classifier_diversity_factor = 0.6\n        self.multi_classifier_diversity_factor_hcm = 0.6\n        self.ce_ratio = 1.0\n        self.hcm_ratio = 1.0\n        if self.args.dataset == 'cifar100':\n            self.hcm_N = 30\n        elif self.args.dataset == 'imgnet':\n            self.hcm_N = 300\n        elif self.args.dataset == 'places':\n            self.hcm_N = 122\n        elif self.args.dataset == 'inat':\n            self.hcm_N = 2442\n\n\n\n    def forward(self, inputs, targets, **kwargs):\n        \"\"\"\n        Args:\n            inputs: prediction matrix (before softmax) with shape (classifier_num, batch_size, num_classes)\n            targets: ground truth labels with shape (classifier_num, batch_size)\n        \"\"\"\n        classifier_num = len(inputs)\n        loss_HCM = 0\n        loss = 0\n        los_ce = 0\n\n        inputs_HCM_balance = []\n        inputs_balance = []\n        class_select = inputs[0].scatter(1, targets[0].unsqueeze(1), 999999)\n        class_select_include_target = class_select.sort(descending=True, dim=1)[1][:, :self.hcm_N]\n        mask = torch.zeros_like(inputs[0]).scatter(1, class_select_include_target, 1)\n        for i in range(classifier_num):\n\n            logits = inputs[i] + self.bsce_weight.unsqueeze(0).expand(inputs[i].shape[0], -1).log()\n            inputs_balance.append(logits)\n            inputs_HCM_balance.append(logits * mask)\n\n            los_ce += F.cross_entropy(logits, targets[0])\n            loss_HCM += F.cross_entropy(inputs_HCM_balance[i], targets[0])\n\n        loss += NBOD(inputs_balance, factor=self.multi_classifier_diversity_factor)\n        loss += NBOD(inputs_HCM_balance, factor=self.multi_classifier_diversity_factor_hcm)\n        loss += los_ce * self.ce_ratio + loss_HCM * self.hcm_ratio\n        return loss\n\n    def update(self, epoch):\n        \"\"\"\n        Args:\n           code can be added for progressive loss.\n        \"\"\"\n        pass\n\n\nif __name__ == '__main__':\n    pass","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:39.302822Z","iopub.execute_input":"2024-01-24T12:06:39.303138Z","iopub.status.idle":"2024-01-24T12:06:39.319666Z","shell.execute_reply.started":"2024-01-24T12:06:39.303110Z","shell.execute_reply":"2024-01-24T12:06:39.318757Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"ride","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nimport random\n\nclass RIDE(nn.Module):\n    def __init__(self, cls_num_list=None, base_diversity_temperature=1.0, max_m=0.5, s=30, reweight=True, reweight_epoch=-1, \n        base_loss_factor=1.0, additional_diversity_factor=-0.2, reweight_factor=0.05):\n        super().__init__()\n        self.base_loss = F.cross_entropy\n        self.base_loss_factor = base_loss_factor\n        if not reweight:\n            self.reweight_epoch = -1\n        else:\n            self.reweight_epoch = reweight_epoch\n\n        # LDAM is a variant of cross entropy and we handle it with self.m_list.\n        if cls_num_list is None:\n            # No cls_num_list is provided, then we cannot adjust cross entropy with LDAM.\n\n            self.m_list = None\n            self.per_cls_weights_enabled = None\n            self.per_cls_weights_enabled_diversity = None\n        else:\n            # We will use LDAM loss if we provide cls_num_list.\n\n            m_list = 1.0 / np.sqrt(np.sqrt(cls_num_list))\n            m_list = m_list * (max_m / np.max(m_list))\n            m_list = torch.tensor(m_list, dtype=torch.float, requires_grad=False)\n            self.m_list = m_list\n            self.s = s\n            assert s > 0\n            \n            if reweight_epoch != -1:\n                idx = 1 # condition could be put in order to set idx\n                betas = [0, 0.9999]\n                effective_num = 1.0 - np.power(betas[idx], cls_num_list)\n                per_cls_weights = (1.0 - betas[idx]) / np.array(effective_num)\n                per_cls_weights = per_cls_weights / np.sum(per_cls_weights) * len(cls_num_list)\n                self.per_cls_weights_enabled = torch.tensor(per_cls_weights, dtype=torch.float, requires_grad=False)\n            else:\n                self.per_cls_weights_enabled = None\n\n            cls_num_list = np.array(cls_num_list) / np.sum(cls_num_list)\n            C = len(cls_num_list)\n            per_cls_weights = C * cls_num_list * reweight_factor + 1 - reweight_factor\n\n            # Experimental normalization: This is for easier hyperparam tuning, the effect can be described in the learning rate so the math formulation keeps the same.\n            # At the same time, the 1 - max trick that was previously used is not required since weights are already adjusted.\n            per_cls_weights = per_cls_weights / np.max(per_cls_weights)\n\n            assert np.all(per_cls_weights > 0), \"reweight factor is too large: out of bounds\"\n            # save diversity per_cls_weights\n            self.per_cls_weights_enabled_diversity = torch.tensor(per_cls_weights, dtype=torch.float, requires_grad=False).cuda()\n\n        self.base_diversity_temperature = base_diversity_temperature\n        self.additional_diversity_factor = additional_diversity_factor\n\n    def to(self, device):\n        super().to(device)\n        if self.m_list is not None:\n            self.m_list = self.m_list.to(device)\n        \n        if self.per_cls_weights_enabled is not None:\n            self.per_cls_weights_enabled = self.per_cls_weights_enabled.to(device)\n\n        if self.per_cls_weights_enabled_diversity is not None:\n            self.per_cls_weights_enabled_diversity = self.per_cls_weights_enabled_diversity.to(device)\n\n        return self\n\n    def _hook_before_epoch(self, epoch):\n        if self.reweight_epoch != -1:\n            self.epoch = epoch\n\n            if epoch > self.reweight_epoch:\n                self.per_cls_weights_base = self.per_cls_weights_enabled\n                self.per_cls_weights_diversity = self.per_cls_weights_enabled_diversity\n            else:\n                self.per_cls_weights_base = None\n                self.per_cls_weights_diversity = None\n\n    def get_final_output(self, output_logits, target):\n        x = output_logits\n\n        index = torch.zeros_like(x, dtype=torch.uint8, device=x.device)\n        index.scatter_(1, target.data.view(-1, 1), 1)\n        \n        index_float = index.float()\n        batch_m = torch.matmul(self.m_list[None, :], index_float.transpose(0,1))\n        \n        batch_m = batch_m.view((-1, 1))\n        x_m = x - batch_m * self.s\n\n        final_output = torch.where(index, x_m, x)\n        return final_output\n\n    def forward(self, output_logits, target, extra_info=None, reduction='mean'):\n        if extra_info is None:\n            return self.base_loss(output_logits, target)\n\n        if reduction == 'none':\n            loss = torch.zeros_like(target).float()\n        else:\n            loss = 0\n\n\n        # Adding RIDE Individual Loss for each expert\n        for logits_item in extra_info['logits']:\n            ride_loss_logits = output_logits if self.additional_diversity_factor == 0 else logits_item\n            if self.m_list is None:\n                loss += self.base_loss_factor * self.base_loss(ride_loss_logits, target, reduction=reduction)\n            else:\n                final_output = self.get_final_output(ride_loss_logits, target)\n                loss += self.base_loss_factor * self.base_loss(final_output, target, weight=self.per_cls_weights_base, reduction=reduction)\n            \n            base_diversity_temperature = self.base_diversity_temperature\n\n            if self.per_cls_weights_diversity is not None:\n                diversity_temperature = base_diversity_temperature * self.per_cls_weights_diversity.view((1, -1))\n                temperature_mean = diversity_temperature.mean().item()\n            else:\n                diversity_temperature = base_diversity_temperature\n                temperature_mean = base_diversity_temperature\n            \n            output_dist = F.log_softmax(logits_item / diversity_temperature, dim=1)\n            with torch.no_grad():\n                # Using the mean takes only linear instead of quadratic time in computing and has only a slight difference so using the mean is preferred here\n                mean_output_dist = F.softmax(output_logits / diversity_temperature, dim=1)\n            \n            loss += self.additional_diversity_factor * temperature_mean * temperature_mean * F.kl_div(output_dist, mean_output_dist, reduction='batchmean')\n        \n        return loss\n\nclass RIDEWithDistill(nn.Module):\n    def __init__(self, cls_num_list=None, additional_distill_loss_factor=1.0, distill_temperature=1.5, ride_loss_factor=1.0, **kwargs):\n        super().__init__()\n        self.ride_loss = RIDE(cls_num_list=cls_num_list, **kwargs)\n        self.distill_temperature = distill_temperature\n\n        self.ride_loss_factor = ride_loss_factor\n        self.additional_distill_loss_factor = additional_distill_loss_factor\n\n    def to(self, device):\n        super().to(device)\n        self.ride_loss = self.ride_loss.to(device)\n        return self\n\n    def _hook_before_epoch(self, epoch):\n        self.ride_loss._hook_before_epoch(epoch)\n\n    def forward(self, student, target=None, teacher=None, extra_info=None):\n        output_logits = student\n        if extra_info is None:\n            return self.ride_loss(output_logits, target)\n\n        loss = 0\n        num_experts = len(extra_info['logits'])\n        for logits_item in extra_info['logits']:\n            loss += self.ride_loss_factor * self.ride_loss(output_logits, target, extra_info)\n            distill_temperature = self.distill_temperature\n\n            student_dist = F.log_softmax(student / distill_temperature, dim=1)\n            with torch.no_grad():\n                teacher_dist = F.softmax(teacher / distill_temperature, dim=1)\n            \n            distill_loss = F.kl_div(student_dist, teacher_dist, reduction='batchmean')\n            distill_loss = distill_temperature * distill_temperature * distill_loss\n            loss += self.additional_distill_loss_factor * distill_loss\n        return loss","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:39.320961Z","iopub.execute_input":"2024-01-24T12:06:39.321398Z","iopub.status.idle":"2024-01-24T12:06:39.352485Z","shell.execute_reply.started":"2024-01-24T12:06:39.321363Z","shell.execute_reply":"2024-01-24T12:06:39.351744Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"common.py\n","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function\n\nimport argparse, os, shutil, random, math\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\n\n!pip install progress\n# added on my own\nimport progress \n#end\n\nfrom progress.bar import Bar as Bar\n\ndef make_imb_data(max_num, class_num, gamma):\n    mu = np.power(1/gamma, 1/(class_num - 1))\n    class_num_list = []\n    for i in range(class_num):\n        if i == (class_num - 1):\n            class_num_list.append(int(max_num / gamma))\n        else:\n            class_num_list.append(int(max_num * np.power(mu, i)))\n    print(class_num_list)\n    return list(class_num_list)\n\ndef hms_string(sec_elapsed):\n    h = int(sec_elapsed / (60 * 60))\n    m = int((sec_elapsed % (60 * 60)) / 60)\n    s = sec_elapsed % 60.\n    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n\ndef save_checkpoint(state, epoch, checkpoint='none', filename='checkpoint.pth.tar'):\n    filepath = os.path.join(checkpoint, filename)\n    torch.save(state, filepath)\n    \n    if epoch % 100 == 0:\n        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_' + str(epoch) + '.pth.tar'))\n        \ndef linear_rampup(current, rampup_length=0):\n    if rampup_length == 0:\n        return 1.0\n    else:\n        current = np.clip(current / rampup_length, 0.0, 1.0)\n        return float(current)\n    \ndef adjust_learning_rate(optimizer, epoch, scheduler, args):\n    if scheduler == None:\n        if args.epochs == 200:\n            epoch = epoch + 1\n            if epoch <= args.warmup:\n                lr = args.lr * epoch / args.warmup\n            elif epoch > 180:\n                lr = args.lr * args.lr_decay ** 2\n            elif epoch > 160:\n                lr = args.lr * args.lr_decay\n            else:\n                lr = args.lr\n\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = lr\n            return lr\n\n        elif args.epochs == 400:\n            if args.loss_fn == 'bcl':\n                epoch = epoch + 1\n                if epoch <= args.warmup:\n                    lr = args.lr * epoch / args.warmup\n                elif epoch > 380:\n                    lr = args.lr * args.lr_decay ** 2\n                elif epoch > 360:\n                    lr = args.lr * args.lr_decay\n                else:\n                    lr = args.lr\n\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = lr\n                return lr\n            else:\n                epoch = epoch + 1\n                if epoch <= args.warmup:\n                    lr = args.lr * epoch / args.warmup\n                elif epoch > 360:\n                    lr = args.lr * args.lr_decay ** 2\n                elif epoch > 320:\n                    lr = args.lr * args.lr_decay\n                else:\n                    lr = args.lr\n\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = lr\n                return lr\n        else:\n            return args.lr\n    else:\n        scheduler.step()\n        return optimizer.param_groups[0]['lr']\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:39.356772Z","iopub.execute_input":"2024-01-24T12:06:39.357071Z","iopub.status.idle":"2024-01-24T12:06:51.051880Z","shell.execute_reply.started":"2024-01-24T12:06:39.357046Z","shell.execute_reply":"2024-01-24T12:06:51.050755Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: progress in /opt/conda/lib/python3.10/site-packages (1.6)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"loss.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom bisect import bisect_right\n\n\n\n#from utils.common import adjust_learning_rate\n\nfrom torch.optim import lr_scheduler\n\ndef get_optimizer(args, model):\n    _model = model['model'] if args.loss_fn == 'ncl' else model\n    return optim.SGD(_model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.wd,\n                     nesterov=args.nesterov)\n\ndef get_scheduler(args, optimizer):\n    if args.scheduler == 'cosine':\n        return lr_scheduler.CosineAnnealingLR(optimizer, args.epochs, eta_min = 0)\n    elif args.scheduler == 'warmup':\n        return None\n\ndef get_loss(args, N_SAMPLES_PER_CLASS):\n    if args.loss_fn == 'ce':\n        train_criterion = CE()\n    elif args.loss_fn == 'ce_drw':\n        train_criterion = CE_DRW(cls_num_list=N_SAMPLES_PER_CLASS, reweight_epoch=160)\n    elif args.loss_fn == 'bs':\n        train_criterion = BS(N_SAMPLES_PER_CLASS)\n    elif args.loss_fn == 'ldam_drw':\n        train_criterion = LDAM_DRW(cls_num_list=N_SAMPLES_PER_CLASS, reweight_epoch=160, max_m=0.5, s=30).cuda()\n    elif args.loss_fn == 'ride':\n        if args.num_experts == 3 and args.ride_distill:\n            train_criterion = RIDEWithDistill(cls_num_list=N_SAMPLES_PER_CLASS, additional_diversity_factor=-0.45, reweight=True, reweight_epoch=160)\n        else:\n            train_criterion = RIDE(cls_num_list=N_SAMPLES_PER_CLASS, additional_diversity_factor=-0.45, reweight=True, reweight_epoch=160)\n        train_criterion = train_criterion.to(torch.device('cuda'))\n    elif args.loss_fn == 'ncl':\n        train_criterion = NIL_NBOD(args, N_SAMPLES_PER_CLASS)\n\n    elif args.loss_fn == 'bcl':\n        train_criterion = BCLLoss(N_SAMPLES_PER_CLASS)\n\n    else:\n        raise NotImplementedError\n        \n\n    return train_criterion\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.053294Z","iopub.execute_input":"2024-01-24T12:06:51.053612Z","iopub.status.idle":"2024-01-24T12:06:51.065153Z","shell.execute_reply.started":"2024-01-24T12:06:51.053575Z","shell.execute_reply":"2024-01-24T12:06:51.064255Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"cuda.py","metadata":{}},{"cell_type":"code","source":"import torch as t\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom torch.utils.data.dataset import Dataset\n\nimport random\nimport PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\nimport numpy as np\nimport torch\nfrom PIL import Image\n\ndef CUDA(img,m,n, rand=True, max_d=30):\n    _augment_list = augment_list()\n    ops = random.choices(_augment_list, k=n)\n    m = float(m) / max_d\n    for op, minval, maxval in ops:\n        val = (float(m)) * float(maxval - minval) + minval\n        img = op(img, val)\n    return img\n\ndef Flip(img, _):\n    return PIL.ImageOps.flip(img)\n\ndef Mirror(img, _):\n    return PIL.ImageOps.mirror(img)\n\ndef EdgeEnhance(img, _):\n    return img.filter(PIL.ImageFilter.EDGE_ENHANCE)\n\ndef Detail(img, _):\n    return img.filter(PIL.ImageFilter.DETAIL)\n\ndef Smooth(img, _):\n    return img.filter(PIL.ImageFilter.SMOOTH)\n    \ndef AutoContrast(img, _):\n    return PIL.ImageOps.autocontrast(img)\n\ndef Equalize(img, _):\n    return PIL.ImageOps.equalize(img)\n\ndef Invert(img, _):\n    return PIL.ImageOps.invert(img)\n\ndef GaussianBlur(img, v):\n    # assert 0 <= v <= 5\n    filter = PIL.ImageFilter.GaussianBlur(v)\n    return img.filter(filter)\n\ndef ResizeCrop(img, v):\n    # assert 1 <= v <= 2\n    width, height = img.size\n    enlarge = img.resize((int(width*v), int(height*v)), Image.ANTIALIAS)\n    left = int(width*v)//2 - width//2\n    right = int(width*v)//2 + width//2\n    top = int(height*v)//2 - height//2\n    bottom = int(height*v)//2 + height//2\n    return enlarge.crop((left, top, right, bottom))\n\ndef Rotate(img, v):  # [-30, 30]\n    # assert -30 <= v <= 30\n    if random.random() > 0.5:\n        v = -v\n    return img.rotate(v)\n\ndef Posterize(img, v):  # [4, 8]\n    v = int(v)\n    v = max(1, v)\n    return PIL.ImageOps.posterize(img, v)\n\ndef Solarize(img, v):  # [0, 256]\n    # assert 0 <= v <= 256\n    return PIL.ImageOps.solarize(img, v)\n\ndef SolarizeAdd(img, addition=0, threshold=128):\n    img_np = np.array(img).astype(int)\n    img_np = img_np + addition\n    img_np = np.clip(img_np, 0, 255)\n    img_np = img_np.astype(np.uint8)\n    img = Image.fromarray(img_np)\n    return PIL.ImageOps.solarize(img, threshold)\n\ndef Color(img, v):  # [0.1,1.9]\n    # assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Color(img).enhance(v)\n\ndef Contrast(img, v):  # [0.1,1.9]Æ’\n    # assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Contrast(img).enhance(v)\n\ndef Brightness(img, v):  # [0.1,1.9]\n    # assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Brightness(img).enhance(v)\n\ndef Sharpness(img, v):  # [0.1,1.9]\n    # assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n\ndef ShearX(img, v):  # [-0.3, 0.3]\n    # assert -0.3 <= v <= 0.3\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n\ndef ShearY(img, v):  # [-0.3, 0.3]\n    # assert -0.3 <= v <= 0.3\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n\ndef TranslateXabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    # assert 0 <= v\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n\ndef TranslateYabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    # assert 0 <= v\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n\ndef augment_list():  \n    l = [\n        (Flip, 0, 1),\n        (Mirror, 0, 1),\n        (EdgeEnhance, 0, 1),\n        (Detail, 0, 1),\n        (Smooth, 0, 1),\n        (AutoContrast, 0, 1),\n        (Equalize, 0, 1),\n        (Invert, 0, 1),\n        (GaussianBlur, 0, 2),\n        (ResizeCrop,1, 1.5),\n        (Rotate, 0, 30),\n        (Posterize, 0, 4),\n        (Solarize, 0, 256),\n        (SolarizeAdd, 0, 110),\n        (Color, 0.1, 1.9),\n        (Contrast, 0.1, 1.9),\n        (Brightness, 0.1, 1.9),\n        (Sharpness, 0.1, 1.9),\n        (ShearX, 0., 0.3),\n        (ShearY, 0., 0.3),\n        (TranslateXabs, 0., 100),\n        (TranslateYabs, 0., 100),\n    ]\n\n    \n\n    return l\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.066583Z","iopub.execute_input":"2024-01-24T12:06:51.066864Z","iopub.status.idle":"2024-01-24T12:06:51.094435Z","shell.execute_reply.started":"2024-01-24T12:06:51.066839Z","shell.execute_reply":"2024-01-24T12:06:51.093587Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"autoaug.py\n","metadata":{}},{"cell_type":"code","source":"from PIL import Image, ImageEnhance, ImageOps\nimport numpy as np\nimport random\nimport torch\n\n\n\nclass Cutout(object):\n    def __init__(self, n_holes, length):\n        self.n_holes = n_holes\n        self.length = length\n\n    def __call__(self, img):\n        h = img.size(1)\n        w = img.size(2)\n\n        mask = np.ones((h, w), np.float32)\n\n        for n in range(self.n_holes):\n            y = np.random.randint(h)\n            x = np.random.randint(w)\n\n            y1 = np.clip(y - self.length // 2, 0, h)\n            y2 = np.clip(y + self.length // 2, 0, h)\n            x1 = np.clip(x - self.length // 2, 0, w)\n            x2 = np.clip(x + self.length // 2, 0, w)\n\n            mask[y1: y2, x1: x2] = 0.\n\n        mask = torch.from_numpy(mask)\n        mask = mask.expand_as(img)\n        img = img * mask\n\n        return img\n\nclass ImageNetPolicy(object):\n    \"\"\" Randomly choose one of the best 24 Sub-policies on ImageNet.\n        Example:\n        >>> policy = ImageNetPolicy()\n        >>> transformed = policy(image)\n        Example as a PyTorch Transform:\n        >>> transform=transforms.Compose([\n        >>>     transforms.Resize(256),\n        >>>     ImageNetPolicy(),\n        >>>     transforms.ToTensor()])\n    \"\"\"\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.4, \"posterize\", 8, 0.6, \"rotate\", 9, fillcolor),\n            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor),\n            SubPolicy(0.6, \"posterize\", 7, 0.6, \"posterize\", 6, fillcolor),\n            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n\n            SubPolicy(0.4, \"equalize\", 4, 0.8, \"rotate\", 8, fillcolor),\n            SubPolicy(0.6, \"solarize\", 3, 0.6, \"equalize\", 7, fillcolor),\n            SubPolicy(0.8, \"posterize\", 5, 1.0, \"equalize\", 2, fillcolor),\n            SubPolicy(0.2, \"rotate\", 3, 0.6, \"solarize\", 8, fillcolor),\n            SubPolicy(0.6, \"equalize\", 8, 0.4, \"posterize\", 6, fillcolor),\n\n            SubPolicy(0.8, \"rotate\", 8, 0.4, \"color\", 0, fillcolor),\n            SubPolicy(0.4, \"rotate\", 9, 0.6, \"equalize\", 2, fillcolor),\n            SubPolicy(0.0, \"equalize\", 7, 0.8, \"equalize\", 8, fillcolor),\n            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n\n            SubPolicy(0.8, \"rotate\", 8, 1.0, \"color\", 2, fillcolor),\n            SubPolicy(0.8, \"color\", 8, 0.8, \"solarize\", 7, fillcolor),\n            SubPolicy(0.4, \"sharpness\", 7, 0.6, \"invert\", 8, fillcolor),\n            SubPolicy(0.6, \"shearX\", 5, 1.0, \"equalize\", 9, fillcolor),\n            SubPolicy(0.4, \"color\", 0, 0.6, \"equalize\", 3, fillcolor),\n\n            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor)\n        ]\n\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return \"AutoAugment ImageNet Policy\"\n\n\nclass CIFAR10Policy(object):\n    \"\"\" Randomly choose one of the best 25 Sub-policies on CIFAR10.\n        Example:\n        >>> policy = CIFAR10Policy()\n        >>> transformed = policy(image)\n        Example as a PyTorch Transform:\n        >>> transform=transforms.Compose([\n        >>>     transforms.Resize(256),\n        >>>     CIFAR10Policy(),\n        >>>     transforms.ToTensor()])\n    \"\"\"\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n\n            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n\n            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n\n            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n            SubPolicy(0.2, \"equalize\", 8, 0.6, \"equalize\", 4, fillcolor),\n            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n\n            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor)\n        ]\n\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return \"AutoAugment CIFAR10 Policy\"\n\n\nclass SVHNPolicy(object):\n    \"\"\" Randomly choose one of the best 25 Sub-policies on SVHN.\n        Example:\n        >>> policy = SVHNPolicy()\n        >>> transformed = policy(image)\n        Example as a PyTorch Transform:\n        >>> transform=transforms.Compose([\n        >>>     transforms.Resize(256),\n        >>>     SVHNPolicy(),\n        >>>     transforms.ToTensor()])\n    \"\"\"\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.9, \"shearX\", 4, 0.2, \"invert\", 3, fillcolor),\n            SubPolicy(0.9, \"shearY\", 8, 0.7, \"invert\", 5, fillcolor),\n            SubPolicy(0.6, \"equalize\", 5, 0.6, \"solarize\", 6, fillcolor),\n            SubPolicy(0.9, \"invert\", 3, 0.6, \"equalize\", 3, fillcolor),\n            SubPolicy(0.6, \"equalize\", 1, 0.9, \"rotate\", 3, fillcolor),\n\n            SubPolicy(0.9, \"shearX\", 4, 0.8, \"autocontrast\", 3, fillcolor),\n            SubPolicy(0.9, \"shearY\", 8, 0.4, \"invert\", 5, fillcolor),\n            SubPolicy(0.9, \"shearY\", 5, 0.2, \"solarize\", 6, fillcolor),\n            SubPolicy(0.9, \"invert\", 6, 0.8, \"autocontrast\", 1, fillcolor),\n            SubPolicy(0.6, \"equalize\", 3, 0.9, \"rotate\", 3, fillcolor),\n\n            SubPolicy(0.9, \"shearX\", 4, 0.3, \"solarize\", 3, fillcolor),\n            SubPolicy(0.8, \"shearY\", 8, 0.7, \"invert\", 4, fillcolor),\n            SubPolicy(0.9, \"equalize\", 5, 0.6, \"translateY\", 6, fillcolor),\n            SubPolicy(0.9, \"invert\", 4, 0.6, \"equalize\", 7, fillcolor),\n            SubPolicy(0.3, \"contrast\", 3, 0.8, \"rotate\", 4, fillcolor),\n\n            SubPolicy(0.8, \"invert\", 5, 0.0, \"translateY\", 2, fillcolor),\n            SubPolicy(0.7, \"shearY\", 6, 0.4, \"solarize\", 8, fillcolor),\n            SubPolicy(0.6, \"invert\", 4, 0.8, \"rotate\", 4, fillcolor),\n            SubPolicy(0.3, \"shearY\", 7, 0.9, \"translateX\", 3, fillcolor),\n            SubPolicy(0.1, \"shearX\", 6, 0.6, \"invert\", 5, fillcolor),\n\n            SubPolicy(0.7, \"solarize\", 2, 0.6, \"translateY\", 7, fillcolor),\n            SubPolicy(0.8, \"shearY\", 4, 0.8, \"invert\", 8, fillcolor),\n            SubPolicy(0.7, \"shearX\", 9, 0.8, \"translateY\", 3, fillcolor),\n            SubPolicy(0.8, \"shearY\", 5, 0.7, \"autocontrast\", 3, fillcolor),\n            SubPolicy(0.7, \"shearX\", 2, 0.1, \"invert\", 5, fillcolor)\n        ]\n\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return \"AutoAugment SVHN Policy\"\n\n\nclass SubPolicy(object):\n    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n        ranges = {\n            \"shearX\": np.linspace(0, 0.3, 10),\n            \"shearY\": np.linspace(0, 0.3, 10),\n            \"translateX\": np.linspace(0, 150 / 331, 10),\n            \"translateY\": np.linspace(0, 150 / 331, 10),\n            \"rotate\": np.linspace(0, 30, 10),\n            \"color\": np.linspace(0.0, 0.9, 10),\n            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(int),\n            \"solarize\": np.linspace(256, 0, 10),\n            \"contrast\": np.linspace(0.0, 0.9, 10),\n            \"sharpness\": np.linspace(0.0, 0.9, 10),\n            \"brightness\": np.linspace(0.0, 0.9, 10),\n            \"autocontrast\": [0] * 10,\n            \"equalize\": [0] * 10,\n            \"invert\": [0] * 10\n        }\n\n        # from https://stackoverflow.com/questions/5252170/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand\n        def rotate_with_fill(img, magnitude):\n            rot = img.convert(\"RGBA\").rotate(magnitude)\n            return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(img.mode)\n\n        func = {\n            \"shearX\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n                Image.BICUBIC, fillcolor=fillcolor),\n            \"shearY\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n                Image.BICUBIC, fillcolor=fillcolor),\n            \"translateX\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),\n                fillcolor=fillcolor),\n            \"translateY\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),\n                fillcolor=fillcolor),\n            \"rotate\": lambda img, magnitude: rotate_with_fill(img, magnitude),\n            \"color\": lambda img, magnitude: ImageEnhance.Color(img).enhance(1 + magnitude * random.choice([-1, 1])),\n            \"posterize\": lambda img, magnitude: ImageOps.posterize(img, magnitude),\n            \"solarize\": lambda img, magnitude: ImageOps.solarize(img, magnitude),\n            \"contrast\": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"sharpness\": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"brightness\": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"autocontrast\": lambda img, magnitude: ImageOps.autocontrast(img),\n            \"equalize\": lambda img, magnitude: ImageOps.equalize(img),\n            \"invert\": lambda img, magnitude: ImageOps.invert(img)\n        }\n\n        self.p1 = p1\n        self.operation1 = func[operation1]\n        self.magnitude1 = ranges[operation1][magnitude_idx1]\n        self.p2 = p2\n        self.operation2 = func[operation2]\n        self.magnitude2 = ranges[operation2][magnitude_idx2]\n\n\n    def __call__(self, img):\n        if random.random() < self.p1: img = self.operation1(img, self.magnitude1)\n        if random.random() < self.p2: img = self.operation2(img, self.magnitude2)\n        return img","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.095838Z","iopub.execute_input":"2024-01-24T12:06:51.096129Z","iopub.status.idle":"2024-01-24T12:06:51.149683Z","shell.execute_reply.started":"2024-01-24T12:06:51.096078Z","shell.execute_reply":"2024-01-24T12:06:51.148976Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"randaug.py\n","metadata":{}},{"cell_type":"code","source":"# code in this file is adpated from rpmcruz/autoaugment\n# https://github.com/rpmcruz/autoaugment/blob/master/transformations.py\nimport random\n\nimport PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\nimport numpy as np\nimport torch\nfrom PIL import Image\n\n\ndef ShearX(img, v):  # [-0.3, 0.3]\n    assert -0.3 <= v <= 0.3\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0))\n\n\ndef ShearY(img, v):  # [-0.3, 0.3]\n    assert -0.3 <= v <= 0.3\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0))\n\n\ndef TranslateX(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert -0.45 <= v <= 0.45\n    if random.random() > 0.5:\n        v = -v\n    v = v * img.size[0]\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n\n\ndef TranslateXabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert 0 <= v\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0))\n\n\ndef TranslateY(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert -0.45 <= v <= 0.45\n    if random.random() > 0.5:\n        v = -v\n    v = v * img.size[1]\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n\n\ndef TranslateYabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert 0 <= v\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v))\n\n\ndef Rotate(img, v):  # [-30, 30]\n    assert -30 <= v <= 30\n    if random.random() > 0.5:\n        v = -v\n    return img.rotate(v)\n\n\ndef AutoContrast(img, _):\n    return PIL.ImageOps.autocontrast(img)\n\n\ndef Invert(img, _):\n    return PIL.ImageOps.invert(img)\n\n\ndef Equalize(img, _):\n    return PIL.ImageOps.equalize(img)\n\n\ndef Flip(img, _):  # not from the paper\n    return PIL.ImageOps.mirror(img)\n\n\ndef Solarize(img, v):  # [0, 256]\n    assert 0 <= v <= 256\n    return PIL.ImageOps.solarize(img, v)\n\n\ndef SolarizeAdd(img, addition=0, threshold=128):\n    img_np = np.array(img).astype(int)\n    img_np = img_np + addition\n    img_np = np.clip(img_np, 0, 255)\n    img_np = img_np.astype(np.uint8)\n    img = Image.fromarray(img_np)\n    return PIL.ImageOps.solarize(img, threshold)\n\n\ndef Posterize(img, v):  # [4, 8]\n    v = int(v)\n    v = max(1, v)\n    return PIL.ImageOps.posterize(img, v)\n\n\ndef Contrast(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Contrast(img).enhance(v)\n\n\ndef Color(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Color(img).enhance(v)\n\n\ndef Brightness(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Brightness(img).enhance(v)\n\n\ndef Sharpness(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n\n\n\n\ndef CutoutAbs(img, v):  # [0, 60] => percentage: [0, 0.2]\n    # assert 0 <= v <= 20\n    if v < 0:\n        return img\n    w, h = img.size\n    x0 = np.random.uniform(w)\n    y0 = np.random.uniform(h)\n\n    x0 = int(max(0, x0 - v / 2.))\n    y0 = int(max(0, y0 - v / 2.))\n    x1 = min(w, x0 + v)\n    y1 = min(h, y0 + v)\n\n    xy = (x0, y0, x1, y1)\n    color = (125, 123, 114)\n    # color = (0, 0, 0)\n    img = img.copy()\n    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n    return img\n\n\ndef SamplePairing(imgs):  # [0, 0.4]\n    def f(img1, v):\n        i = np.random.choice(len(imgs))\n        img2 = PIL.Image.fromarray(imgs[i])\n        return PIL.Image.blend(img1, img2, v)\n\n    return f\n\n\ndef Identity(img, v):\n    return img\n\n\ndef augment_list():  # 16 oeprations and their ranges\n    # https://github.com/google-research/uda/blob/master/image/randaugment/policies.py#L57\n    # l = [\n    #     (Identity, 0., 1.0),\n    #     (ShearX, 0., 0.3),  # 0\n    #     (ShearY, 0., 0.3),  # 1\n    #     (TranslateX, 0., 0.33),  # 2\n    #     (TranslateY, 0., 0.33),  # 3\n    #     (Rotate, 0, 30),  # 4\n    #     (AutoContrast, 0, 1),  # 5\n    #     (Invert, 0, 1),  # 6\n    #     (Equalize, 0, 1),  # 7\n    #     (Solarize, 0, 110),  # 8\n    #     (Posterize, 4, 8),  # 9\n    #     # (Contrast, 0.1, 1.9),  # 10\n    #     (Color, 0.1, 1.9),  # 11\n    #     (Brightness, 0.1, 1.9),  # 12\n    #     (Sharpness, 0.1, 1.9),  # 13\n    #     # (Cutout, 0, 0.2),  # 14\n    #     # (SamplePairing(imgs), 0, 0.4),  # 15\n    # ]\n\n    # https://github.com/tensorflow/tpu/blob/8462d083dd89489a79e3200bcc8d4063bf362186/models/official/efficientnet/autoaugment.py#L505\n    l = [\n        (AutoContrast, 0, 1),\n        (Equalize, 0, 1),\n        (Invert, 0, 1),\n        (Rotate, 0, 30),\n        (Posterize, 0, 4),\n        (Solarize, 0, 256),\n        (SolarizeAdd, 0, 110),\n        (Color, 0.1, 1.9),\n        (Contrast, 0.1, 1.9),\n        (Brightness, 0.1, 1.9),\n        (Sharpness, 0.1, 1.9),\n        (ShearX, 0., 0.3),\n        (ShearY, 0., 0.3),\n        (CutoutAbs, 0, 40),\n        (TranslateXabs, 0., 100),\n        (TranslateYabs, 0., 100),\n    ]\n\n    return l\n\n\nclass Lighting(object):\n    \"\"\"Lighting noise(AlexNet - style PCA - based noise)\"\"\"\n\n    def __init__(self, alphastd, eigval, eigvec):\n        self.alphastd = alphastd\n        self.eigval = torch.Tensor(eigval)\n        self.eigvec = torch.Tensor(eigvec)\n\n    def __call__(self, img):\n        if self.alphastd == 0:\n            return img\n\n        alpha = img.new().resize_(3).normal_(0, self.alphastd)\n        rgb = self.eigvec.type_as(img).clone() \\\n            .mul(alpha.view(1, 3).expand(3, 3)) \\\n            .mul(self.eigval.view(1, 3).expand(3, 3)) \\\n            .sum(1).squeeze()\n\n        return img.add(rgb.view(3, 1, 1).expand_as(img))\n\n\nclass CutoutDefault(object):\n    \"\"\"\n    Reference : https://github.com/quark0/darts/blob/master/cnn/utils.py\n    \"\"\"\n    def __init__(self, length):\n        self.length = length\n\n    def __call__(self, img):\n        h, w = img.size(1), img.size(2)\n        mask = np.ones((h, w), np.float32)\n        y = np.random.randint(h)\n        x = np.random.randint(w)\n\n        y1 = np.clip(y - self.length // 2, 0, h)\n        y2 = np.clip(y + self.length // 2, 0, h)\n        x1 = np.clip(x - self.length // 2, 0, w)\n        x2 = np.clip(x + self.length // 2, 0, w)\n\n        mask[y1: y2, x1: x2] = 0.\n        mask = torch.from_numpy(mask)\n        mask = mask.expand_as(img)\n        img *= mask\n        return img\n\n\nclass RandAugment:\n    def __init__(self, n, m):\n        self.n = n\n        self.m = m      # [0, 30]\n        self.augment_list = augment_list()\n\n    def __call__(self, img):\n        ops = random.choices(self.augment_list, k=self.n)\n        for op, minval, maxval in ops:\n            val = (float(self.m) / 30) * float(maxval - minval) + minval\n            img = op(img, val)\n\n        return img\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.151002Z","iopub.execute_input":"2024-01-24T12:06:51.151269Z","iopub.status.idle":"2024-01-24T12:06:51.188131Z","shell.execute_reply.started":"2024-01-24T12:06:51.151246Z","shell.execute_reply":"2024-01-24T12:06:51.187208Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"cutout.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\n\n\n\nclass Cutout(object):\n    def __init__(self, n_holes, length):\n        self.n_holes = n_holes\n        self.length = length\n\n    def __call__(self, img):\n        h = img.size(1)\n        w = img.size(2)\n\n        mask = np.ones((h, w), np.float32)\n\n        for n in range(self.n_holes):\n            y = np.random.randint(h)\n            x = np.random.randint(w)\n\n            y1 = np.clip(y - self.length // 2, 0, h)\n            y2 = np.clip(y + self.length // 2, 0, h)\n            x1 = np.clip(x - self.length // 2, 0, w)\n            x2 = np.clip(x + self.length // 2, 0, w)\n\n            mask[y1: y2, x1: x2] = 0.\n\n        mask = torch.from_numpy(mask)\n        mask = mask.expand_as(img)\n        img = img * mask\n\n        return img\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.189093Z","iopub.execute_input":"2024-01-24T12:06:51.189327Z","iopub.status.idle":"2024-01-24T12:06:51.202438Z","shell.execute_reply.started":"2024-01-24T12:06:51.189306Z","shell.execute_reply":"2024-01-24T12:06:51.201620Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"transformer.py","metadata":{}},{"cell_type":"code","source":"from torchvision.transforms import transforms\nfrom PIL import ImageFilter\nimport random\n#from aug.cutout import *\n\ncifar10_mean = (0.4914, 0.4822, 0.4465)\ncifar10_std = (0.2023, 0.1994, 0.2010)\n\n\n\nclass GaussianBlur(object):\n    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n\n    def __init__(self, sigma=[.1, 2.]):\n        self.sigma = sigma\n\n    def __call__(self, x):\n        sigma = random.uniform(self.sigma[0], self.sigma[1])\n        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n        return x\n\n\n\ndef get_transform(loss_fn, cutout = False):\n    # Augmentations.\n    if loss_fn in ['ce', 'ldam_drw', 'bs', 'ce_drw', 'ride']:\n        train_before = [\n                transforms.RandomCrop(32, padding=4),\n                transforms.RandomHorizontalFlip(),\n            ]\n        \n        if cutout:\n            train_after = [\n                transforms.ToTensor(),\n                Cutout(n_holes = 1, length = 16),\n                transforms.Normalize(cifar10_mean, cifar10_std),\n                ]\n        else:\n            train_after = [\n                transforms.ToTensor(),\n                transforms.Normalize(cifar10_mean, cifar10_std),\n                ]\n\n        transform_train = [[transforms.Compose(train_before), transforms.Compose(train_after)]]\n\n    elif loss_fn in ['ncl']:\n        regular_train_before = [\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            ]\n\n        if cutout:\n            regular_train_after = [\n                transforms.ToTensor(),\n                Cutout(n_holes = 1, length = 16),\n                transforms.Normalize(cifar10_mean, cifar10_std),\n                ]\n        else:\n            regular_train_after = [\n                transforms.ToTensor(),\n                transforms.Normalize(cifar10_mean, cifar10_std),\n                ]\n\n\n        sim_cifar_before = [\n            transforms.RandomResizedCrop(size=32, scale=(0.2, 1.)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomApply([\n                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n            ], p=0.8),\n            transforms.RandomGrayscale(p=0.2),\n            transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n            ]\n        sim_cifar_after = [\n            transforms.ToTensor(),\n            transforms.Normalize(cifar10_mean, cifar10_std),\n            ]\n        transform_train = [\n            [transforms.Compose(regular_train_before), \n            transforms.Compose(regular_train_after)], \n            [transforms.Compose(sim_cifar_before), \n            transforms.Compose(sim_cifar_after)],\n            ]\n\n\n    \n    elif loss_fn in ['bcl']:\n        regular_train_before = [\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            ]\n\n        if cutout:\n            regular_train_after = [\n                transforms.ToTensor(),\n                Cutout(n_holes = 1, length = 16),\n                transforms.Normalize(cifar10_mean, cifar10_std),\n                ]\n        else:\n            regular_train_after = [\n                transforms.ToTensor(),\n                transforms.Normalize(cifar10_mean, cifar10_std),\n                ]\n        \n        sim_cifar_before = [\n            transforms.RandomResizedCrop(size=32, scale=(0.2, 1.)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomApply([\n                transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n            ], p=0.8),\n            transforms.RandomGrayscale(p=0.2),\n            transforms.RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n            ]\n        sim_cifar_after = [\n            transforms.ToTensor(),\n            transforms.Normalize(cifar10_mean, cifar10_std),\n            ]\n\n        transform_train = [\n            [transforms.Compose(regular_train_before), \n            transforms.Compose(regular_train_after)], \n            [transforms.Compose(sim_cifar_before), \n            transforms.Compose(sim_cifar_after)], \n            [transforms.Compose(sim_cifar_before), \n            transforms.Compose(sim_cifar_after)],\n            ]\n\n    transform_val = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(cifar10_mean, cifar10_std)\n    ])\n    \n    return transform_train, transform_val\n    \n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.203680Z","iopub.execute_input":"2024-01-24T12:06:51.204237Z","iopub.status.idle":"2024-01-24T12:06:51.224324Z","shell.execute_reply.started":"2024-01-24T12:06:51.204201Z","shell.execute_reply":"2024-01-24T12:06:51.223498Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"cifar100.py\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nimport random\n\nimport torchvision\nimport torch\n\nfrom torch.utils.data import Dataset\n\nfrom torchvision.transforms import transforms\n\n\n\n    \ndef get_cifar100(root, args):\n    transform_train, transform_val = get_transform(args.loss_fn, cutout = args.cutout)\n\n    train_dataset = CIFAR100_train(root, args, imb_ratio = args.imb_ratio, train=True, transform = transform_train, aug_prob=args.aug_prob)\n    test_dataset = CIFAR100_val(root, transform=transform_val)\n    print (f\"#Train: {len(train_dataset)}, #Test: {len(test_dataset)}\")\n    return train_dataset, test_dataset\n    \nclass test_CIFAR100(Dataset):\n    def __init__(self, indices, state, cifar_dataset):\n        self.indices = indices\n        self.state = state\n        self.dataset = cifar_dataset\n\n    def __getitem__(self,idx):\n        data, label, _ = self.dataset.get_item(self.indices[idx], self.state[idx], train=False)\n        return data, label, self.indices[idx], self.state[idx]\n    \n    def __len__(self):\n        return len(self.indices)\n\nclass CIFAR100_train(torchvision.datasets.CIFAR100):\n    def __init__(self, root , args, aug_prob, imb_type='exp', imb_ratio=100, train=True, transform=None, target_transform=None, download=True):\n        super(CIFAR100_train,self).__init__(root, train=train, transform=transform, target_transform = target_transform, download= download)\n\n        np.random.seed(0)\n        self.args = args\n        self.cls_num = 100\n        self.img_num_list = self.get_img_num_per_cls(self.cls_num, imb_type, 1./imb_ratio)\n        self.transform_train = transform\n        self.gen_imbalanced_data(self.img_num_list)\n        \n\n        if 'autoaug_cifar' in args.aug_type:\n            print('autoaug_cifar')\n            self.aug_transform = transforms.Compose([CIFAR10Policy()])\n        elif 'autoaug_svhn' in args.aug_type:\n            print('autoaug_svhn')\n            self.aug_transform = transforms.Compose([SVHNPolicy()])\n        elif 'autoaug_imagenet' in args.aug_type:\n            print('autoaug_imagenet')\n            self.aug_transform = transforms.Compose([ImageNetPolicy()])\n        #elif 'dada_cifar' in args.aug_type:\n            print('dada_cifar')\n            self.aug_transform = transforms.Compose([dada_cifar()])\n        #elif 'dada_imagenet' in args.aug_type:\n            print('dada_imagenet')\n            self.aug_transform = transforms.Compose([dada_imagenet()])\n        #elif 'faa_cifar' in args.aug_type:\n            print('faa_cifar')\n            self.aug_transform = transforms.Compose([faa_cifar()])\n        #elif 'faa_imagenet' in args.aug_type:\n            print('faa_imagenet')\n            self.aug_transform = transforms.Compose([faa_imagenet()])\n        elif 'randaug' in args.aug_type:\n            print('randaug')\n            self.aug_transform = transforms.Compose([RandAugment(2, 14)])\n        elif 'none' in args.aug_type:\n            self.aug_transform = transforms.Compose([])\n        else:\n            raise NotImplementedError\n        \n\n\n\n        # max_mag = 10\n        # max_ops = 10\n        max_mag = 10\n        max_ops = 10\n        self.min_state = 0\n        self.max_state = max(max_mag, max_ops) + 1\n        \n        states = torch.arange(self.min_state, self.max_state)\n        if self.max_state == 1:\n            self.ops = torch.tensor([0])\n            self.mag = torch.tensor([0])\n            \n        elif max_mag > max_ops:\n            self.ops = (states * max_ops / max_mag).ceil().int()\n            self.mag = states.int()\n        else:\n            self.mag = (states * max_mag / max_ops).ceil().int()\n            self.ops = states.int()\n        \n        print(f\"Magnitude set = {self.mag}\")\n        print(f\"Operation set = {self.ops}\")\n\n        self.curr_state = torch.zeros(len(self.data))\n        self.score_tmp = torch.zeros((len(self.targets), self.max_state))\n        self.num_test = torch.zeros((len(self.targets), self.max_state))\n        self.aug_prob = aug_prob\n\n\n\n    def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):\n        img_max = len(self.data) / cls_num\n        img_num_per_cls = []\n        if imb_type == 'exp':\n            for cls_idx in range(cls_num):\n                num = img_max * (imb_factor ** (cls_idx / (cls_num - 1.0)))\n                img_num_per_cls.append(int(num))\n        else:\n            img_num_per_cls.extend([int(img_max)] * cls_num)\n        return img_num_per_cls\n\n\n    def gen_imbalanced_data(self, img_num_per_cls):\n        new_data = []\n        new_targets = []\n        #changed from np.int64\n        targets_np = np.array(self.targets, dtype=int)\n        classes = np.unique(targets_np)\n        # np.random.shuffle(classes)\n\n        self.num_per_cls_dict = dict()\n        for the_class, the_img_num in zip(classes, img_num_per_cls):\n            self.num_per_cls_dict[the_class] = the_img_num\n            idx = np.where(targets_np == the_class)[0]\n            np.random.shuffle(idx)\n            selec_idx = idx[:the_img_num]\n            # print(selec_idx)\n            new_data.append(self.data[selec_idx, ...])\n            new_targets.extend([the_class, ] * the_img_num)\n        new_data = np.vstack(new_data)\n        self.data = new_data\n        self.targets = new_targets\n\n    def get_cls_num_list(self):\n        cls_num_list = []\n        for i in range(self.cls_num):\n            cls_num_list.append(self.num_per_cls_dict[i])\n        return cls_num_list\n\n    def sim_aug(self, img, state, type):\n        if type == 'cuda':\n            return  CUDA(img, self.mag[state], self.ops[state], max_d = self.args.max_d)\n        else:\n            return img\n        \n\n    \n    def get_item(self, index, state, train=True):\n        img, target = self.data[index], self.targets[index]\n        img = Image.fromarray(img)\n        \n        if train:\n            if len(self.transform_train) == 1:\n                img = self.transform_train[0][0](img)\n                img = self.aug_transform(img)\n                img = CUDA(img, self.mag[state], self.ops[state])\n                img = self.transform_train[0][1](img)\n                return img, target, index\n\n            elif len(self.transform_train) == 2:\n                img1 = self.transform_train[0][0](img)\n                img1 = self.aug_transform(img1)\n                img1 = CUDA(img1, self.mag[state], self.ops[state], max_d = self.args.max_d)\n                img1 = self.transform_train[0][1](img1)\n\n                img2 = self.transform_train[1][0](img)\n                img2 = self.sim_aug(img2, state, self.args.sim_type)\n                img2 = self.transform_train[1][1](img2)\n                \n                return (img1, img2), target, index\n                \n            elif len(self.transform_train) == 3:\n                img1 = self.transform_train[0][0](img)\n                img1 = self.aug_transform(img1)\n                img1 = CUDA(img1, self.mag[state], self.ops[state], max_d = self.args.max_d)\n                img1 = self.transform_train[0][1](img1)\n\n                img2 = self.transform_train[1][0](img)\n                img2 = self.sim_aug(img2, state, self.args.sim_type)\n                img2 = self.transform_train[1][1](img2)\n                \n                img3 = self.transform_train[2][0](img)\n                img3 = self.sim_aug(img3, state, self.args.sim_type)\n                img3 = self.transform_train[2][1](img3)\n                return (img1, img2, img3), target, index\n\n        else:\n            img = self.transform_train[0][0](img)\n            img = self.aug_transform(img)\n            img = CUDA(img, self.mag[state], self.ops[state], rand=False , max_d = self.args.max_d)\n            img = self.transform_train[0][1](img)\n            return img, target, index\n        \n    def __getitem__(self, index):\n        state = self.curr_state[index].int() if torch.rand(1) < self.aug_prob else 0\n        \n        img, target, index = self.get_item(index, state, train=True)\n        return img, target, index\n    \n    def update_scores(self, correct, index, state):\n        for s in np.unique(state):\n            pos = np.where(state == s)\n            score_result = np.bincount(index[pos], correct[pos], len(self.score_tmp))\n            num_test_result = np.bincount(index[pos], np.ones(len(index))[pos], len(self.score_tmp))\n            self.score_tmp[:,s] += score_result\n            self.num_test[:,s] += num_test_result\n            \n\n    def update(self):\n        # Increase\n        pos = torch.where((self.score_tmp == self.num_test) & (self.num_test != 0))\n        self.curr_state[pos] += 1\n        \n        # Decrease\n        pos = torch.where(self.score_tmp != self.num_test)\n        self.curr_state[pos] -= 1\n        \n        \n        self.curr_state = torch.clamp(self.curr_state, self.min_state, self.max_state-1)\n        self.score_tmp *= 0\n        self.num_test *= 0\n        \n    \nclass CIFAR100_val(torchvision.datasets.CIFAR100):\n    def __init__(self, root, transform=None, indexs=None,\n                 target_transform=None, download=True):\n        super(CIFAR100_val, self).__init__(root, train=False, transform=transform, target_transform=target_transform,download=download)\n        \n        if indexs is not None:\n            self.data = self.data[indexs]\n            self.targets = np.array(self.targets)[indexs]\n        self.data = [Image.fromarray(img) for img in self.data]\n        \n    def __getitem__(self, index):\n        img, target = self.data[index], self.targets[index]\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        return img, target, index","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.225571Z","iopub.execute_input":"2024-01-24T12:06:51.225864Z","iopub.status.idle":"2024-01-24T12:06:51.270554Z","shell.execute_reply.started":"2024-01-24T12:06:51.225820Z","shell.execute_reply":"2024-01-24T12:06:51.269718Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"accuracy.py","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function, absolute_import\n\nimport errno\nimport os\nimport sys\nimport time\nimport math\n\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torch.autograd import Variable\n\n\n__all__ = ['accuracy', 'AverageMeter']\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\n       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.271747Z","iopub.execute_input":"2024-01-24T12:06:51.272076Z","iopub.status.idle":"2024-01-24T12:06:51.284513Z","shell.execute_reply.started":"2024-01-24T12:06:51.272042Z","shell.execute_reply":"2024-01-24T12:06:51.283624Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"cutmix.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n\ndef cutmix(data_f, data_b):\n    lam = np.random.beta(1., 1.)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(data_f.size(), lam)\n    data_b[:, :, bbx1:bbx2, bby1:bby2] = data_f[:, :, bbx1:bbx2, bby1:bby2]\n    lam = 1-((bbx2 - bbx1) * (bby2 - bby1) / (data_f.size()[2] * data_f.size()[3]))\n    \n    return data_b, torch.tensor(lam)\n\n# def cutmix(data_aug, data, label, param, percent=1.0):\n    # data = data_aug\n    # sample_num = int(len(param)*percent)\n    # argsort = torch.argsort(param,descending=True)\n    # param /= torch.max(param)\n    \n    # candidate = argsort[:sample_num]\n    \n    # data_f = data[candidate]\n    # label_f = label[candidate]\n    # param_f = param[candidate]\n    \n    # back_perm = candidate[torch.randperm(len(candidate))]\n    # data_b = data[back_perm]\n    # label_b = label[back_perm]\n    # param_b = param[back_perm]\n    \n    # # lam = torch.exp(param_f) / (torch.exp(param_f)+torch.exp(param_b))\n    # lam = torch.tensor(np.random.beta(1.,1.,(sample_num,)))\n    \n    # size = data.size()\n    # W = size[2]\n    # H = size[3]\n    # cut_rat = torch.sqrt(1. - lam)\n    # cut_w = (cut_rat * W).int()\n    # cut_h = (cut_rat * H).int()\n\n    # # uniform\n    # cx = torch.randint(0,W,(len(candidate),))\n    # cy = torch.randint(0,H,(len(candidate),))\n\n    # bbx1 = torch.clip(cx - cut_w // 2, 0, W)\n    # bby1 = torch.clip(cy - cut_h // 2, 0, H)\n    # bbx2 = torch.clip(cx + cut_w // 2, 0, W)\n    # bby2 = torch.clip(cy + cut_h // 2, 0, H)\n    \n    # for idx in range(len(data_b)):\n    #     data_b[idx, :, bbx1[idx]:bbx2[idx], bby1[idx]:bby2[idx]] = data_f[idx, :, bbx1[idx]:bbx2[idx], bby1[idx]:bby2[idx]]\n    # data_aug[candidate] = data_b\n    \n    # label[candidate] = label_b\n    # label_aug = torch.zeros(len(label),dtype=int)\n    # label_aug[candidate] = label_f.cpu()\n\n    # ret_lbd = torch.ones(len(label))\n    # ret_lbd[candidate] -= ((bbx2 - bbx1) * (bby2 - bby1) / (W*H))\n\n    # return data_aug, label, label_aug, ret_lbd","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.287669Z","iopub.execute_input":"2024-01-24T12:06:51.288107Z","iopub.status.idle":"2024-01-24T12:06:51.299699Z","shell.execute_reply.started":"2024-01-24T12:06:51.288082Z","shell.execute_reply":"2024-01-24T12:06:51.298928Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"basetrain","metadata":{}},{"cell_type":"code","source":"\nfrom __future__ import print_function\n\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n#from aug.cutmix import *\n\n#from utils.accuracy import AverageMeter\n#from utils.common import Bar\n\nimport copy, time\n\n#from datasets.cifar100 import test_CIFAR100\nimport random\n\n\n\ndef update_score_base(loader, model, n_samples_per_class, posthoc_la, num_test, accept_rate):\n    model.eval()\n    \n    if posthoc_la:\n        dist = torch.tensor(n_samples_per_class)\n        prob = dist / dist.sum()\n    \n    curr_state = loader.dataset.curr_state\n    max_state = torch.max(curr_state).int() + 1\n    \n    with torch.no_grad():\n        n = num_test\n        pos, state = [], []\n        ''' \n        for s in range(max_state):\n            entire_pos = torch.arange(len(loader.dataset.targets))\n            _pos = random.choices(entire_pos.tolist(), k = n * (s+1)) \n            pos +=  _pos\n            state += [s] * len(_pos)\n        tmp_dataest = test_CIFAR100(pos,  state, loader.dataset)\n        tmp_loader = torch.utils.data.DataLoader(tmp_dataest, batch_size = 128,             \n                                                 shuffle=False, num_workers = 8)\n        \n        '''\n        n = num_test\n        pos, state = [], []\n        for cidx in range(len(n_samples_per_class)):\n            class_pos = torch.where(torch.tensor(loader.dataset.targets) == cidx)[0]\n            max_state = loader.dataset.curr_state[class_pos[0]].int() \n            for s in range(max_state+1):\n                _pos = random.choices(class_pos.tolist(), k = n * (s+1))\n                pos += _pos \n                state += [s] * len(_pos)\n \n        tmp_dataset = test_CIFAR100(pos, state, loader.dataset)\n        tmp_loader = torch.utils.data.DataLoader(tmp_dataset, batch_size = 128, shuffle=False, num_workers=8)\n        \n\n        for batch_idx, data_tuple in enumerate(tmp_loader):\n            data = data_tuple[0].cuda()\n            label = data_tuple[1]\n            idx = data_tuple[2]\n            state = data_tuple[3]\n\n            logit = model(data, output_type = None).cpu()\n\n            if posthoc_la:\n                logit = logit.cpu() - torch.log(prob.view(1, -1).expand(logit.shape[0],-1))\n\n            correct = (logit.max(dim=1)[1] == label).int().detach().cpu()\n            loader.dataset.update_scores(correct,idx, state)\n\n    \n    \n    # loader.dataset.update()\n    correct_sum_per_class = torch.zeros(len(n_samples_per_class))\n    trial_sum_per_class = torch.zeros(len(n_samples_per_class))\n    \n    for cidx in range(len(n_samples_per_class)):\n        class_pos = torch.where(torch.tensor(loader.dataset.targets) == cidx)[0]\n        \n        correct_sum_row = torch.sum(loader.dataset.score_tmp[class_pos], dim=0)\n        trial_sum_row = torch.sum(loader.dataset.num_test[class_pos], dim=0)\n\n\n        ratio = correct_sum_row / trial_sum_row \n        idx = loader.dataset.curr_state[class_pos][0].int() + 1\n        condition = torch.sum((ratio[:idx] > accept_rate)) == idx \n        \n        # if correct_sum == trial_sum:\n        # if float(correct_sum) >= float(trial_sum * 0.6):\n        if condition:\n            loader.dataset.curr_state[class_pos] += 1\n        else:\n            loader.dataset.curr_state[class_pos] -= 1\n    '''\n    \n\n    all_indices = torch.arange(len(loader.dataset.targets))\n    correct_sum_all_classes = torch.sum(loader.dataset.score_tmp, dim=0)\n    trial_sum_all_classes = torch.sum(loader.dataset.num_test, dim=0)\n\n    ratio = correct_sum_all_classes / trial_sum_all_classes\n    idx = loader.dataset.curr_state[0].int() + 1\n    condition = torch.sum((ratio[:idx] > accept_rate)) == idx\n\n    if condition:\n            loader.dataset.curr_state[all_indices] += 1\n    else:\n            loader.dataset.curr_state[all_indices] -= 1\n    '''\n        \n    loader.dataset.curr_state = loader.dataset.curr_state.clamp(loader.dataset.min_state, loader.dataset.max_state-1)\n    loader.dataset.score_tmp *= 0\n    loader.dataset.num_test *= 0\n\n\n    # print(f'Max correct: {int(torch.max(correct_sum_per_class))} Max trial: {int(torch.max(trial_sum_per_class))}')\n    \n    # loader.dataset.update()\n    model.train()\n    \n    # Debug\n    curr_state = loader.dataset.curr_state\n    \n    label = loader.dataset.targets\n    print(f'Max state: {int(torch.max(curr_state))} // Min state: {int(torch.min(curr_state))}')\n\n    return curr_state, label\n\n\n# def update_score_base(loader, model, n_samples_per_class, posthoc_la):\n#     model.eval()\n    \n#     if posthoc_la:\n#         dist = torch.tensor(n_samples_per_class)\n#         prob = dist / dist.sum()\n    \n#     # curr_state= loader.dataset.curr_state\n#     # max_state = torch.max(curr_state).int() + 1\n    \n#     with torch.no_grad():\n#         # pos, state = [], []\n            \n#         # for s in range(max_state):\n#         #     _pos = torch.where(curr_state >= s)[0]\n#         #     pos_list = _pos.tolist() * (s+1) \n#         #     pos +=  pos_list\n#         #     state += [s] * len(pos_list)\n#         # tmp_dataest = test_CIFAR100(pos,  state, loader.dataset)\n#         # tmp_loader = torch.utils.data.DataLoader(tmp_dataest, batch_size = 128,             \n#         #                                         shuffle=False, num_workers = 8)\n        \n#         n = 10\n#         pos, state = [], []\n#         for cidx in range(len(n_samples_per_class)):\n#             class_pos = torch.where(torch.tensor(loader.dataset.targets) == cidx)[0]\n#             max_state = loader.dataset.curr_state[class_pos[0]].int() \n#             for s in range(max_state+1):\n#                 _pos = random.choices(class_pos.tolist(), k = n * (s+1))\n#                 pos += _pos \n#                 state += [s] * len(_pos)\n \n#         tmp_dataset = test_CIFAR100(pos, state, loader.dataset)\n#         tmp_loader = torch.utils.data.DataLoader(tmp_dataset, batch_size = 128, shuffle=False, num_workers=8)\n        \n\n#         for batch_idx, data_tuple in enumerate(tmp_loader):\n#             data = data_tuple[0].cuda()\n#             label = data_tuple[1]\n#             idx = data_tuple[2]\n\n#             logit = model(data, output_type = None).cpu()\n\n#             if posthoc_la:\n#                 logit = logit.cpu() - torch.log(prob.view(1, -1).expand(logit.shape[0],-1))\n\n#             correct = (logit.max(dim=1)[1] == label).int().detach().cpu()\n#             loader.dataset.update_scores(correct,idx)\n#     print(f'Max correct: {int(torch.max(loader.dataset.score_tmp))} Max trial: {int(torch.max(loader.dataset.num_test))}')\n    \n#     # loader.dataset.update()\n#     for cidx in range(len(n_samples_per_class)):\n#         class_pos = torch.where(torch.tensor(loader.dataset.targets) == cidx)[0]\n        \n#         correct_sum = torch.sum(loader.dataset.score_tmp[class_pos])\n#         trial_sum = torch.sum(loader.dataset.num_test[class_pos])\n\n#         # if correct_sum == trial_sum:\n#         if float(correct_sum) >= float(trial_sum * 0.8):\n#             loader.dataset.curr_state[class_pos] += 1\n#         else:\n#             loader.dataset.curr_state[class_pos] -= 1\n\n#     loader.dataset.curr_state = loader.dataset.curr_state.clamp(loader.dataset.min_state, loader.dataset.max_state-1)\n#     loader.dataset.score_tmp *= 0\n#     loader.dataset.num_test *= 0\n\n\n\n\n#     model.train()\n    \n#     # Debug\n#     curr_state = loader.dataset.curr_state\n#     label = loader.dataset.targets\n#     print(f'Max state: {int(torch.max(curr_state))} // Min state: {int(torch.min(curr_state))}')\n\n#     return curr_state, label\n\n\n\n\n\ndef train_base(args, trainloader, model, optimizer, criterion, epoch, weighted_trainloader, teacher = None):\n    model.train()\n    \n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    end = time.time()\n    \n    bar = Bar('Training', max=len(trainloader))\n\n    if args.cmo and 3 < epoch < (args.epochs - 3):\n        inverse_iter = iter(weighted_trainloader)\n\n        \n    for batch_idx, data_tuple in enumerate(trainloader):\n        inputs_b = data_tuple[0]\n        targets_b = data_tuple[1]\n        indexs = data_tuple[2]\n\n\n        # Measure data loading\n        data_time.update(time.time() - end)\n        batch_size = targets_b.size(0)\n        \n        if args.cmo and 3 < epoch < (args.epochs - 3):\n            try:\n                data_tuple_f = next(inverse_iter)\n            except:\n                inverse_iter = iter(weighted_trainloader)\n                data_tuple_f = next(inverse_iter)\n\n            inputs_f = data_tuple_f[0]\n            targets_f = data_tuple_f[1]\n            inputs_f = inputs_f[:len(inputs_b)]\n            targets_f = targets_f[:len(targets_b)]\n            inputs_f = inputs_f.cuda(non_blocking=True)\n            targets_f = targets_f.cuda(non_blocking=True)\n\n        inputs_b = inputs_b.cuda(non_blocking=True)\n        targets_b = targets_b.cuda(non_blocking=True)\n\n\n        r = np.random.rand(1)\n        if args.cmo and 3 < epoch < (args.epochs - 3) and r < 0.5:\n            inputs_b, lam = cutmix(inputs_f, inputs_b)\n            outputs = model(inputs_b, None)\n            loss = criterion(outputs, targets_b, epoch) * lam + criterion(outputs, targets_f, epoch) * (1.-lam)\n        else:\n            outputs = model(inputs_b, None)\n            loss = criterion(outputs, targets_b, epoch)\n        \n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # record\n        losses.update(loss.item(), targets_b.size(0))\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        # plot\n        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | ' \\\n                      'Loss: {loss:.4f}'.format(\n                    batch=batch_idx + 1,\n                    size=len(trainloader),\n                    data=data_time.avg,\n                    bt=batch_time.avg,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    )\n        bar.next()\n    bar.finish()\n    return losses.avg\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.300771Z","iopub.execute_input":"2024-01-24T12:06:51.301059Z","iopub.status.idle":"2024-01-24T12:06:51.333251Z","shell.execute_reply.started":"2024-01-24T12:06:51.301035Z","shell.execute_reply":"2024-01-24T12:06:51.332507Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"bcltrain","metadata":{}},{"cell_type":"code","source":"\n\nfrom __future__ import print_function\n\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n#from aug.cutmix import *\n\n#from utils.accuracy import AverageMeter\n#from utils.common import Bar\n\nimport copy, time\nimport random\n\n#from datasets.cifar100 import test_CIFAR100\n\n\n\ndef update_score_bcl(loader, model, n_samples_per_class, posthoc_la, num_test, accept_rate):\n    model.eval()\n    \n    if posthoc_la:\n        dist = torch.tensor(n_samples_per_class)\n        prob = dist / dist.sum()\n    \n    # curr_state = loader.dataset.curr_state\n    # max_state = torch.max(curr_state).int() + 1\n    \n    with torch.no_grad():\n        # pos, state = [], []\n            \n        # for s in range(max_state):\n        #     _pos = torch.where(curr_state >= s)[0]\n        #     pos_list = _pos.tolist() * (s+1) \n        #     pos +=  pos_list\n        #     state += [s] * len(pos_list)\n        # tmp_dataest = test_CIFAR100(pos,  state, loader.dataset)\n        # tmp_loader = torch.utils.data.DataLoader(tmp_dataest, batch_size = 128,             \n        #                                         shuffle=False, num_workers = 8)\n        \n        n = num_test\n        pos, state = [], []\n        for cidx in range(len(n_samples_per_class)):\n            class_pos = torch.where(torch.tensor(loader.dataset.targets) == cidx)[0]\n            max_state = loader.dataset.curr_state[class_pos[0]].int() \n            for s in range(max_state+1):\n                _pos = random.choices(class_pos.tolist(), k = n * (s+1))\n                pos += _pos \n                state += [s] * len(_pos)\n \n        \n        tmp_dataset = test_CIFAR100(pos, state, loader.dataset)\n        tmp_loader = torch.utils.data.DataLoader(tmp_dataset, batch_size = 128, shuffle=False, num_workers=8)\n\n        for batch_idx, data_tuple in enumerate(tmp_loader):\n            data = data_tuple[0].cuda()\n            label = data_tuple[1]\n            idx = data_tuple[2]\n            state = data_tuple[3]\n\n            _, logit, _ = model(data)\n            \n            if posthoc_la:\n                logit = logit.cpu() - torch.log(prob.view(1, -1).expand(logit.shape[0],-1))\n\n            correct = (logit.cpu().max(dim=1)[1] == label).int().detach().cpu()\n            loader.dataset.update_scores(correct,idx, state)\n\n\n            \n    \n    # loader.dataset.update()\n    correct_sum_per_class = torch.zeros(len(n_samples_per_class))\n    trial_sum_per_class = torch.zeros(len(n_samples_per_class))\n    for cidx in range(len(n_samples_per_class)):\n        class_pos = torch.where(torch.tensor(loader.dataset.targets) == cidx)[0]\n        \n        correct_sum_row = torch.sum(loader.dataset.score_tmp[class_pos], dim=0)\n        trial_sum_row = torch.sum(loader.dataset.num_test[class_pos], dim=0)\n\n        ratio = correct_sum_row / trial_sum_row \n        idx = loader.dataset.curr_state[class_pos][0].int() + 1\n        condition = torch.sum((ratio[:idx] > accept_rate)) == idx\n        \n        # if correct_sum == trial_sum:\n        # if float(correct_sum) >= float(trial_sum * 0.6):\n        if condition:\n            loader.dataset.curr_state[class_pos] += 1\n        else:\n            loader.dataset.curr_state[class_pos] -= 1\n    \n        \n\n    loader.dataset.curr_state = loader.dataset.curr_state.clamp(loader.dataset.min_state, loader.dataset.max_state-1)\n    loader.dataset.score_tmp *= 0\n    loader.dataset.num_test *= 0\n\n    # print(f'Max correct: {int(torch.max(loader.dataset.score_tmp))} Max trial: {int(torch.max(loader.dataset.num_test))}')\n    # print(f'Max correct: {int(torch.max(correct_sum_per_class))} Max trial: {int(torch.max(trial_sum_per_class))}')\n    \n    \n    # loader.dataset.update()\n    model.train()\n    \n    # Debug\n    curr_state = loader.dataset.curr_state\n    label = loader.dataset.targets\n    print(f'Max state: {int(torch.max(curr_state))} // Min state: {int(torch.min(curr_state))}')\n\n    return curr_state, label\n\n\n\n\n\ndef train_bcl(args, trainloader, model, optimizer, criterion, epoch, weighted_trainloader, teacher = None):\n    model.train()\n    \n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    end = time.time()\n    \n    bar = Bar('Training', max=len(trainloader))\n        \n    for batch_idx, data_tuple in enumerate(trainloader):\n        inputs_b = data_tuple[0]\n        targets_b = data_tuple[1]\n        indexs = data_tuple[2]\n\n        # Measure data loading\n        data_time.update(time.time() - end)\n        batch_size = targets_b.size(0)\n        \n        if args.cmo:\n            raise \"BCL not implemented for CMO...\"\n        else:\n            inputs_b = torch.cat([inputs_b[0], inputs_b[1], inputs_b[2]], dim=0).cuda()\n            batch_size = targets_b.shape[0]\n            targets_b = targets_b.cuda()\n            feat_mlp, logits, centers = model(inputs_b)\n            centers = centers[:args.num_class]\n            _, f2, f3 = torch.split(feat_mlp, [batch_size, batch_size, batch_size], dim=0)\n            features = torch.cat([f2.unsqueeze(1), f3.unsqueeze(1)], dim=1)\n            logits, _, __ = torch.split(logits, [batch_size, batch_size, batch_size], dim=0)\n            loss = criterion(centers, logits, features, targets_b)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # record\n        losses.update(loss.item(), targets_b.size(0))\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        # plot\n        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | ' \\\n                      'Loss: {loss:.4f}'.format(\n                    batch=batch_idx + 1,\n                    size=len(trainloader),\n                    data=data_time.avg,\n                    bt=batch_time.avg,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    )\n        bar.next()\n    bar.finish()\n    return losses.avg\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.334548Z","iopub.execute_input":"2024-01-24T12:06:51.334954Z","iopub.status.idle":"2024-01-24T12:06:51.359978Z","shell.execute_reply.started":"2024-01-24T12:06:51.334928Z","shell.execute_reply":"2024-01-24T12:06:51.359143Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"ncltrain","metadata":{}},{"cell_type":"code","source":"\n#from utils.accuracy import AverageMeter\nimport torch\nimport time\n#from utils.common import Bar, adjust_learning_rate\n\nimport copy\n\n#from datasets.cifar100 import test_CIFAR100\nimport random\n\ndef update_score_ncl(loader, model, n_samples_per_class, posthoc_la, num_test, accept_rate):\n    model = model['model']\n    model.eval()\n     \n    if posthoc_la:\n        dist = torch.tensor(n_samples_per_class)\n        prob = dist / dist.sum()\n    \n    \n    # curr_state = loader.dataset.curr_state\n    # max_state = torch.max(curr_state).int() + 1\n    \n    with torch.no_grad():\n        # pos, state = [], []\n            \n        # for s in range(max_state):\n        #     _pos = torch.where(curr_state >= s)[0]\n        #     pos_list = _pos.tolist() * (s+1) \n        #     pos +=  pos_list\n        #     state += [s] * len(pos_list)\n        # tmp_dataest = test_CIFAR100(pos,  state, loader.dataset)\n        # tmp_loader = torch.utils.data.DataLoader(tmp_dataest, batch_size = 128,   shuffle=False, num_workers = 8, drop_last=True)\n        \n        n = num_test\n        pos, state = [], []\n        for cidx in range(len(n_samples_per_class)):\n            class_pos = torch.where(torch.tensor(loader.dataset.targets) == cidx)[0]\n            max_state = loader.dataset.curr_state[class_pos[0]].int() \n            for s in range(max_state+1):\n                _pos = random.choices(class_pos.tolist(), k = n * (s+1))\n                pos += _pos \n                state += [s] * len(_pos)\n \n        tmp_dataset = test_CIFAR100(pos, state, loader.dataset)\n        tmp_loader = torch.utils.data.DataLoader(tmp_dataset, batch_size = 128, shuffle=False, num_workers=8)\n        \n\n        for batch_idx, data_tuple in enumerate(tmp_loader):\n            data = data_tuple[0].cuda()\n            label = data_tuple[1]\n            idx = data_tuple[2]\n            state = data_tuple[3]\n\n            data_list = [data for i in range(model.network_num)]\n\n            feature = model((data_list,data_list), label=label, feature_flag=True)\n            output_ce, output, output_MA = model(feature, classifier_flag=True)\n            logit = torch.mean(torch.stack(output_ce), dim=0).cpu()\n\n            if posthoc_la:\n                logit = logit.cpu() - torch.log(prob.view(1, -1).expand(logit.shape[0],-1)).cuda()\n\n            correct = (logit.max(dim=1)[1] == label).int().detach().cpu()\n            loader.dataset.update_scores(correct,idx, state)\n    \n    \n \n    \n    \n    # loader.dataset.update()\n    correct_sum_per_class = torch.zeros(len(n_samples_per_class))\n    trial_sum_per_class = torch.zeros(len(n_samples_per_class))\n    for cidx in range(len(n_samples_per_class)):\n        class_pos = torch.where(torch.tensor(loader.dataset.targets) == cidx)[0]\n        \n        correct_sum_row = torch.sum(loader.dataset.score_tmp[class_pos], dim=0)\n        trial_sum_row = torch.sum(loader.dataset.num_test[class_pos], dim=0)\n\n\n        ratio = correct_sum_row / trial_sum_row \n        idx = loader.dataset.curr_state[class_pos][0].int() + 1\n        condition = torch.sum((ratio[:idx] > accept_rate)) == idx \n        \n        # if correct_sum == trial_sum:\n        # if float(correct_sum) >= float(trial_sum * 0.6):\n        if condition:\n            loader.dataset.curr_state[class_pos] += 1\n        else:\n            loader.dataset.curr_state[class_pos] -= 1\n    \n        \n\n    loader.dataset.curr_state = loader.dataset.curr_state.clamp(loader.dataset.min_state, loader.dataset.max_state-1)\n    loader.dataset.score_tmp *= 0\n    loader.dataset.num_test *= 0\n\n    \n    # print(f'Max correct: {int(torch.max(loader.dataset.score_tmp))} Max trial: {int(torch.max(loader.dataset.num_test))}')\n    \n    # loader.dataset.update()\n    model.train()\n    \n    # Debug\n    curr_state = loader.dataset.curr_state\n    label = loader.dataset.targets\n    print(f'Max state: {int(torch.max(curr_state))} // Min state: {int(torch.min(curr_state))}')\n\n    return curr_state, label\n\n\ndef train_ncl(args, trainloader, model, optimizer, criterion, epoch, weighted_trainloader, teacher=None):\n    combiner = model['comb']\n    model = model['model']\n    network_num = 3\n\n    model.train()\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    end = time.time()\n\n    bar = Bar('Training', max=len(trainloader))\n    \n    for batch_idx, data_tuple in enumerate(trainloader):\n        inputs = data_tuple[0]\n        targets = data_tuple[1]\n        indexs = data_tuple[2]\n\n        # Measure data loading\n        data_time.update(time.time() - end)\n        batch_size = targets.size(0)\n\n        if args.cmo:\n            raise \"NCL not implemented for CMO...\"\n        else:\n            image_list = [inputs] * network_num\n            label_list = [targets] * network_num\n            indexs_list = [indexs] * network_num\n\n            loss = combiner.forward(model, criterion, image_list, label_list)\n\n            if args.dataset in ['cifar100', 'places']:\n                alpha = 0.999\n                for net_id in range(network_num):\n                    net = ['backbone', 'module']\n                    for name in net:\n                        for ema_param, param in zip(eval('model.' + name + '_MA').parameters(),\n                                                    eval('model.' + name).parameters()):\n                            ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n\n        # record\n        losses.update(loss.data.item(), targets.size(0))\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        # plot\n        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | ' \\\n                      'Loss: {loss:.4f}'.format(\n                    batch=batch_idx + 1,\n                    size=len(trainloader),\n                    data=data_time.avg,\n                    bt=batch_time.avg,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    )\n        bar.next()\n    bar.finish()\n    return losses.avg\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.361190Z","iopub.execute_input":"2024-01-24T12:06:51.361441Z","iopub.status.idle":"2024-01-24T12:06:51.388019Z","shell.execute_reply.started":"2024-01-24T12:06:51.361418Z","shell.execute_reply":"2024-01-24T12:06:51.387320Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"ridetrain","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function\n\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n#from aug.cutmix import *\n\n#from utils.accuracy import AverageMeter\n#from utils.common import Bar, adjust_learning_rate\n\nimport copy\n\n#from datasets.cifar100 import test_CIFAR100\nimport random\n\ndef update_score_ride(loader, model, n_samples_per_class, posthoc_la, num_test, accept_rate):\n    model.eval()\n    \n    if posthoc_la:\n        dist = torch.tensor(n_samples_per_class)\n        prob = dist / dist.sum()\n    \n    curr_state = loader.dataset.curr_state\n    max_state = torch.max(curr_state).int() + 1\n    \n    with torch.no_grad():\n        n = num_test\n        pos, state = [], []\n            \n\n    \n    with torch.no_grad():\n        pos, state = [], []\n            \n        # for s in range(max_state):\n        #     _pos = torch.where(curr_state >= s)[0]\n        #     pos_list = _pos.tolist() * (s+1) \n        #     pos +=  pos_list\n        #     state += [s] * len(pos_list)\n        # tmp_dataest = test_CIFAR100(pos,  state, loader.dataset)\n        # tmp_loader = torch.utils.data.DataLoader(tmp_dataest, batch_size = 128,             \n        #                                         shuffle=False, num_workers = 8)\n        \n        n = num_test\n        pos, state = [], []\n        \n        \n        for cidx in range(len(n_samples_per_class)):\n            class_pos = torch.where(torch.tensor(loader.dataset.targets) == cidx)[0]\n            max_state = loader.dataset.curr_state[class_pos[0]].int() \n            for s in range(max_state+1):\n                _pos = random.choices(class_pos.tolist(), k = n * (s+1))\n                pos += _pos \n                state += [s] * len(_pos)\n        '''\n        \n        for s in range(max_state):\n            entire_pos = torch.arange(len(loader.dataset.targets))\n            _pos = random.choices(entire_pos.tolist(), k = n * (s+1)) \n            pos +=  _pos\n            state += [s] * len(_pos)\n        '''\n\n        \n        tmp_dataset = test_CIFAR100(pos, state, loader.dataset)\n        tmp_loader = torch.utils.data.DataLoader(tmp_dataset, batch_size = 128, shuffle=False, num_workers=8)\n        \n\n        for batch_idx, data_tuple in enumerate(tmp_loader):\n            data = data_tuple[0].cuda()\n            label = data_tuple[1]\n            idx = data_tuple[2]\n            state = data_tuple[3]\n\n            # logit = model(data, output_type = None).cpu()\n            # if posthoc_la:\n            #     logit = logit - tau * torch.log(prob.view(1, -1).expand(logit.shape[0],-1))\n            # correct = (logit.max(dim=1)[1] == label).int().detach().cpu()\n\n            outputs = model(data, output_type='dict')\n            logit = outputs['logits'].cpu()\n\n            for cor_idx in range(logit.size(1)):\n                if cor_idx == 0:\n                    correct = (logit[:,cor_idx].max(dim=1)[1] == label).int().detach().cpu()\n                else:\n                    correct += (logit[:,cor_idx].max(dim=1)[1] == label).int().detach().cpu()\n            \n            correct = torch.floor(correct/logit.size(1))\n            loader.dataset.update_scores(correct,idx, state)\n    '''\n    all_indices = torch.arange(len(loader.dataset.targets))\n    correct_sum_all_classes = torch.sum(loader.dataset.score_tmp, dim=0)\n    trial_sum_all_classes = torch.sum(loader.dataset.num_test, dim=0)\n\n    ratio = correct_sum_all_classes / trial_sum_all_classes\n    idx = loader.dataset.curr_state[0].int() + 1\n    condition = torch.sum((ratio[:idx] > accept_rate)) == idx\n\n    if condition:\n            loader.dataset.curr_state[all_indices] += 1\n    else:\n            loader.dataset.curr_state[all_indices] -= 1 \n    \n    '''\n    # loader.dataset.update()\n    correct_sum_per_class = torch.zeros(len(n_samples_per_class))\n    trial_sum_per_class = torch.zeros(len(n_samples_per_class))\n    for cidx in range(len(n_samples_per_class)):\n        class_pos = torch.where(torch.tensor(loader.dataset.targets) == cidx)[0]\n        \n        correct_sum_row = torch.sum(loader.dataset.score_tmp[class_pos], dim=0)\n        trial_sum_row = torch.sum(loader.dataset.num_test[class_pos], dim=0)\n\n\n        ratio = correct_sum_row / trial_sum_row \n        idx = loader.dataset.curr_state[class_pos][0].int() + 1\n        condition = torch.sum((ratio[:idx] > accept_rate)) == idx \n        \n        # if correct_sum == trial_sum:\n        # if float(correct_sum) >= float(trial_sum * 0.6):\n        if condition:\n            loader.dataset.curr_state[class_pos] += 1\n        else:\n            loader.dataset.curr_state[class_pos] -= 1\n    \n        \n\n    loader.dataset.curr_state = loader.dataset.curr_state.clamp(loader.dataset.min_state, loader.dataset.max_state-1)\n    loader.dataset.score_tmp *= 0\n    loader.dataset.num_test *= 0\n\n    # print(f'Max correct: {int(torch.max(loader.dataset.score_tmp))} Max trial: {int(torch.max(loader.dataset.num_test))}')\n    \n\n    # loader.dataset.update()\n    model.train()\n    \n    # Debug\n    curr_state = loader.dataset.curr_state\n    label = loader.dataset.targets\n    print(f'Max state: {int(torch.max(curr_state))} // Min state: {int(torch.min(curr_state))}')\n\n    return curr_state, label\n\n\n\ndef ride_loss_wrap(criterion, student, teacher, target, extra_info):\n    if teacher == None:\n        return criterion(output_logits = student['output'], target = target, extra_info = extra_info)\n    else:\n        return criterion(student = student['output'], target = target, teacher = teacher, extra_info = extra_info)\n\ndef train_ride(args, trainloader, model, optimizer, criterion, epoch, weighted_trainloader, teacher):\n    \"\"\"\n    Training logic for an epoch\n    \n    :param epoch: Integer, current training epoch.\n    :return: A log that contains average loss and metric in this epoch.\n    \"\"\"\n    model.train()\n    \n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    end = time.time()\n    \n    if hasattr(criterion, \"_hook_before_epoch\"):\n        criterion._hook_before_epoch(epoch)\n        \n    bar = Bar('Training', max=len(trainloader))\n\n\n    if args.cmo and 3 < epoch < (args.epochs-3):\n        inverse_iter = iter(weighted_trainloader)\n\n    for batch_idx, data_tuple in enumerate(trainloader):\n        inputs_b = data_tuple[0]\n        targets_b = data_tuple[1]\n        indexs = data_tuple[2]\n        \n        # Measure data loading\n        data_time.update(time.time() - end)\n        batch_size = targets_b.size(0)\n        \n        if args.cmo and 3 < epoch < (args.epochs-3):\n            try:\n                data_tuple_f = next(inverse_iter)\n            except:\n                inverse_iter = iter(weighted_trainloader)\n                data_tuple_f = next(inverse_iter)\n                \n            inputs_f = data_tuple_f[0]\n            targets_f = data_tuple_f[1]\n            inputs_f = inputs_f[:len(inputs_b)]\n            targets_f = targets_f[:len(targets_b)]\n            inputs_f = inputs_f.cuda(non_blocking=True)\n            targets_f = targets_f.cuda(non_blocking=True)\n\n\n        inputs_b = inputs_b.cuda(non_blocking=True)\n        targets_b = targets_b.cuda(non_blocking=True)\n\n        r = np.random.rand(1)\n        if args.cmo and 3 < epoch < (args.epochs - 3) and r < 0.5:\n            inputs_b, lam = cutmix(inputs_f, inputs_b)\n            outputs =  model(inputs_b)\n            extra_info = {}\n            # logits = outputs[\"logits\"]\n            # extra_info.update({\"logits\" : logits.transpose(0,1)})\n            # loss = criterion(output_logits = outputs['output'], target = targets_b, extra_info = extra_info) * lam + criterion(output_logits = outputs['output'], target = targets_f, extra_info = extra_info) * (1.-lam)\n            if teacher == None:\n                teacher_outputs = None\n            else:\n                teacher_outputs = teacher(inputs_b)['output']\n            \n            extra_info.update({\"logits\" : outputs['logits'].transpose(0,1)})\n                \n            loss = ride_loss_wrap(criterion, outputs, teacher_outputs, targets_b, extra_info) * lam + ride_loss_wrap(criterion, outputs, teacher_outputs, targets_f, extra_info) * (1.-lam)\n            \n            \n        else:\n            extra_info = {}\n            outputs = model(inputs_b)\n            # logits = outputs[\"logits\"]\n            # extra_info.update({\"logits\": logits.transpose(0, 1)})\n            # loss = criterion(output_logits=outputs['output'], target=targets_b, extra_info=extra_info)\n            \n            if teacher == None:\n                teacher_outputs = None\n            else:\n                teacher_outputs = teacher(inputs_b)['output']\n            \n            extra_info.update({\"logits\" : outputs['logits'].transpose(0,1)})\n            loss = ride_loss_wrap(criterion, outputs, teacher_outputs, targets_b, extra_info)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # record\n        losses.update(loss.item(), targets_b.size(0))\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        # plot\n        bar.suffix = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | ' \\\n                      'Loss: {loss:.4f}'.format(\n                    batch=batch_idx + 1,\n                    size=len(trainloader),\n                    data=data_time.avg,\n                    bt=batch_time.avg,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    )\n        \n        bar.next()\n    bar.finish()\n    return losses.avg\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.389290Z","iopub.execute_input":"2024-01-24T12:06:51.389828Z","iopub.status.idle":"2024-01-24T12:06:51.425496Z","shell.execute_reply.started":"2024-01-24T12:06:51.389794Z","shell.execute_reply":"2024-01-24T12:06:51.424732Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"train.py","metadata":{}},{"cell_type":"code","source":"#from train.train_fn.base import train_base, update_score_base\n#from train.train_fn.ride import train_ride, update_score_ride\n#from train.train_fn.ncl import train_ncl, update_score_ncl\n#from train.train_fn.bcl import train_bcl, update_score_bcl\n\ndef get_train_fn(args):\n    if args.loss_fn == 'ride':\n        return train_ride\n    elif args.loss_fn == 'ncl':\n        return train_ncl\n    elif args.loss_fn == 'bcl':\n        return train_bcl\n    else:\n        return train_base\n\n        \n        \ndef get_update_score_fn(args):\n    if args.loss_fn == 'ride':\n        return update_score_ride\n    elif args.loss_fn == 'ncl':\n        return update_score_ncl\n    elif args.loss_fn == 'bcl':\n        return update_score_bcl\n    else:\n        return update_score_base\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.426948Z","iopub.execute_input":"2024-01-24T12:06:51.427276Z","iopub.status.idle":"2024-01-24T12:06:51.439555Z","shell.execute_reply.started":"2024-01-24T12:06:51.427246Z","shell.execute_reply":"2024-01-24T12:06:51.438876Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"validate.py","metadata":{}},{"cell_type":"code","source":"#from utils.accuracy import AverageMeter, accuracy\nfrom scipy import optimize\n#from utils.common import Bar\nimport torch\nimport numpy as np\nimport time\n\ndef get_valid_fn(args):\n    if args.loss_fn == 'ncl':\n        return valid_ncl\n    elif args.loss_fn == 'bcl':\n        return valid_bcl\n    else:\n        return valid_normal\n\n\ndef valid_ncl(args, valloader, model, criterion, per_class_num, num_class=10, mode='Test Stats'):\n    combiner = model['comb']\n    model = model['model']\n    network_num = 3\n    model.eval()\n    network_num = 3\n    cnt_all = 0\n    every_network_result = [0 for _ in range(network_num)]\n\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n\n    end = time.time()\n    bar = Bar(f'{mode}', max=len(valloader))\n    \n    classwise_correct = torch.zeros(num_class)\n    classwise_num = torch.zeros(num_class)\n    section_acc = torch.zeros(3)\n    \n    \n    with torch.no_grad():\n        for batch_idx, data_tuple in enumerate(valloader):\n            image = data_tuple[0]\n            label = data_tuple[1]\n            indexs = data_tuple[2]\n\n            image, label = image.cuda(), label.cuda()\n            image_list = [image for i in range(network_num)]\n\n            if args.dataset in ['cifar100', 'places']:\n                feature = model((image_list,image_list), label=label, feature_flag=True)\n                output_ce, output, output_MA = model(feature, classifier_flag=True)\n            else:\n                feature = model(image_list, label=label, feature_flag=True)\n                output_ce = model(feature, classifier_flag=True)\n\n\n            \n            for j, logit in enumerate(output_ce):\n                every_network_result[j] += torch.sum(torch.argmax(logit, dim=1).cpu() == label.cpu())\n\n            average_result = torch.mean(torch.stack(output_ce), dim=0)\n            loss = criterion(average_result, label)\n\n            prec1, prec5 = accuracy(average_result.cpu(), label.cpu(), topk=(1,5))\n            losses.update(loss.data.item(), image.size(0))\n            top1.update(prec1.item(), image.size(0))\n            top5.update(prec5.item(), image.size(0))\n\n            # classwise prediction\n            pred_label = average_result.max(1)[1]\n            pred_mask = (label == pred_label).float()\n            for i in range(num_class):\n                class_mask = (label == i).float()\n                classwise_correct[i] += (class_mask * pred_mask).sum().detach().cpu()\n                classwise_num[i] += class_mask.sum().detach().cpu()\n                \n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n                        \n            # plot progress\n            bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | ' \\\n                          'Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n                        batch=batch_idx + 1,\n                        size=len(valloader),\n                        data=data_time.avg,\n                        bt=batch_time.avg,\n                        total=bar.elapsed_td,\n                        eta=bar.eta_td,\n                        loss=losses.avg,\n                        top1=top1.avg,\n                        top5=top5.avg,\n                        )\n            bar.next()\n        bar.finish()\n        \n    # Major, Neutral, Minor\n    classwise_acc = (classwise_correct / classwise_num)\n    \n    per_class_num = torch.tensor(per_class_num)\n    many_pos = torch.where(per_class_num > 100)\n    med_pos = torch.where((per_class_num <= 100) & (per_class_num >=20))\n    few_pos = torch.where(per_class_num < 20)\n    section_acc[0] = classwise_acc[many_pos].mean()\n    section_acc[1] = classwise_acc[med_pos].mean()\n    section_acc[2] = classwise_acc[few_pos].mean()\n    \n    return (losses.avg, top1.avg,  section_acc)\n\ndef valid_normal(args, valloader, model, criterion, per_class_num, num_class=10, mode='Test Stats', trainloader = None):\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    \n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    bar = Bar(f'{mode}', max=len(valloader))\n    \n    classwise_correct = torch.zeros(num_class)\n    classwise_num = torch.zeros(num_class)\n    section_acc = torch.zeros(3)\n    \n    all_preds = np.zeros(len(valloader.dataset))\n    with torch.no_grad():\n        for batch_idx, data_tuple in enumerate(valloader):\n            inputs = data_tuple[0].cuda(non_blocking=True)\n            targets = data_tuple[1].cuda(non_blocking=True)\n            indexs = data_tuple[2]\n            \n            # measure data loading time\n            data_time.update(time.time() - end)\n            \n            # compute output\n            outputs = model(inputs, None)\n            loss = criterion(outputs, targets)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(outputs, targets, topk=(1,5))\n            losses.update(loss.item(), inputs.size(0))\n            top1.update(prec1.item(), inputs.size(0))\n            top5.update(prec5.item(), inputs.size(0))\n            \n            # classwise prediction\n            pred_label = outputs.max(1)[1]\n            all_preds[indexs] = pred_label.cpu().numpy()\n                \n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n                        \n            # plot progress\n            bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | ' \\\n                          'Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n                        batch=batch_idx + 1,\n                        size=len(valloader),\n                        data=data_time.avg,\n                        bt=batch_time.avg,\n                        total=bar.elapsed_td,\n                        eta=bar.eta_td,\n                        loss=losses.avg,\n                        top1=top1.avg,\n                        top5=top5.avg,\n                        )\n            bar.next()\n        bar.finish()\n        # Major, Neutral, Minor\n        \n        all_targets = np.array(valloader.dataset.targets)\n        pred_mask = (all_targets == all_preds).astype(float)\n        for i in range(num_class):\n            class_mask = np.where(all_targets == i)[0].reshape(-1)\n            classwise_correct[i] += pred_mask[class_mask].sum()\n            classwise_num[i] += len(class_mask)\n            \n        classwise_acc = (classwise_correct / classwise_num)\n        \n        per_class_num = torch.tensor(per_class_num)\n        many_pos = torch.where(per_class_num > 100)\n        med_pos = torch.where((per_class_num <= 100) & (per_class_num >=20))\n        few_pos = torch.where(per_class_num < 20)\n        section_acc[0] = classwise_acc[many_pos].mean()\n        section_acc[1] = classwise_acc[med_pos].mean()\n        section_acc[2] = classwise_acc[few_pos].mean()\n\n    return (losses.avg, top1.avg,  section_acc)\n\n\ndef valid_bcl(args, valloader, model, criterion, per_class_num, num_class=10, mode='Test Stats', trainloader = None):\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    \n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    bar = Bar(f'{mode}', max=len(valloader))\n    \n    classwise_correct = torch.zeros(num_class)\n    classwise_num = torch.zeros(num_class)\n    section_acc = torch.zeros(3)\n    \n    all_preds = np.zeros(len(valloader.dataset))\n    with torch.no_grad():\n        for batch_idx, data_tuple in enumerate(valloader):\n            inputs = data_tuple[0].cuda(non_blocking=True)\n            targets = data_tuple[1].cuda(non_blocking=True)\n            indexs = data_tuple[2]\n            \n            # measure data loading time\n            data_time.update(time.time() - end)\n            \n            # compute output\n            _, outputs, _ = model(inputs)\n            loss = criterion(outputs, targets)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(outputs, targets, topk=(1,5))\n            losses.update(loss.item(), inputs.size(0))\n            top1.update(prec1.item(), inputs.size(0))\n            top5.update(prec5.item(), inputs.size(0))\n            \n            # classwise prediction\n            pred_label = outputs.max(1)[1]\n            all_preds[indexs] = pred_label.cpu().numpy()\n                \n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n                        \n            # plot progress\n            bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | ' \\\n                          'Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n                        batch=batch_idx + 1,\n                        size=len(valloader),\n                        data=data_time.avg,\n                        bt=batch_time.avg,\n                        total=bar.elapsed_td,\n                        eta=bar.eta_td,\n                        loss=losses.avg,\n                        top1=top1.avg,\n                        top5=top5.avg,\n                        )\n            bar.next()\n        bar.finish()\n        # Major, Neutral, Minor\n        \n        all_targets = np.array(valloader.dataset.targets)\n        pred_mask = (all_targets == all_preds).astype(float)\n        for i in range(num_class):\n            class_mask = np.where(all_targets == i)[0].reshape(-1)\n            classwise_correct[i] += pred_mask[class_mask].sum()\n            classwise_num[i] += len(class_mask)\n            \n        classwise_acc = (classwise_correct / classwise_num)\n        \n        per_class_num = torch.tensor(per_class_num)\n        many_pos = torch.where(per_class_num > 100)\n        med_pos = torch.where((per_class_num <= 100) & (per_class_num >=20))\n        few_pos = torch.where(per_class_num < 20)\n        section_acc[0] = classwise_acc[many_pos].mean()\n        section_acc[1] = classwise_acc[med_pos].mean()\n        section_acc[2] = classwise_acc[few_pos].mean()\n\n    return (losses.avg, top1.avg,  section_acc)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.440848Z","iopub.execute_input":"2024-01-24T12:06:51.441466Z","iopub.status.idle":"2024-01-24T12:06:51.651406Z","shell.execute_reply.started":"2024-01-24T12:06:51.441433Z","shell.execute_reply":"2024-01-24T12:06:51.650500Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"resnetbcl","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.nn import Parameter\n\n\ndef _weights_init(m):\n    classname = m.__class__.__name__\n    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n        init.kaiming_normal_(m.weight)\n\nclass NormedLinear(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(NormedLinear, self).__init__()\n        self.weight = Parameter(torch.Tensor(in_features, out_features))\n        self.weight.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)\n        self.apply(_weights_init)\n\n    def forward(self, x):\n        out = F.normalize(x, dim=1).mm(F.normalize(self.weight, dim=0))\n        return out\n\nclass LambdaLayer(nn.Module):\n\n    def __init__(self, lambd):\n        super(LambdaLayer, self).__init__()\n        self.lambd = lambd\n\n    def forward(self, x):\n        return self.lambd(x)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, option='A'):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            if option == 'A':\n                \"\"\"\n                For CIFAR10 ResNet paper uses option A.\n                \"\"\"\n                self.shortcut = LambdaLayer(lambda x:\n                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n            elif option == 'B':\n                self.shortcut = nn.Sequential(\n                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                     nn.BatchNorm2d(self.expansion * planes)\n                )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet_s(nn.Module):\n\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet_s, self).__init__()\n        self.in_planes = 16\n\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.avg_pool2d(out, out.size()[3])\n        out = out.view(out.size(0), -1)\n        return out\n\nclass bcl_model(nn.Module):\n    def __init__(self, num_classes=100, use_norm=False):\n        super(bcl_model, self).__init__()\n        self.encoder = ResNet_s(BasicBlock, [5,5,5], num_classes)\n        dim_in = 64 #2048\n        mid_dim = 512 #2048\n        feat_dim = 128 #1024\n        self.use_norm = use_norm\n        self.head = nn.Sequential(nn.Linear(dim_in, mid_dim), nn.BatchNorm1d(mid_dim), nn.ReLU(inplace=True), nn.Linear(mid_dim, feat_dim))\n        \n        if self.use_norm:\n            self.fc = NormedLinear(dim_in, num_classes)\n        else:\n            self.fc = nn.Linear(dim_in, num_classes)\n        self.head_fc = nn.Sequential(nn.Linear(dim_in, mid_dim), nn.BatchNorm1d(mid_dim), nn.ReLU(inplace=True), nn.Linear(mid_dim, feat_dim))\n\n        self.apply(_weights_init)\n\n\n    def forward(self, x):\n        feat = self.encoder(x)\n        feat_mlp = F.normalize(self.head(feat), dim=1)\n        logits = self.fc(feat)\n        if self.use_norm:\n            centers_logits = F.normalize(self.head_fc(self.fc.weight.T), dim=1)\n        else:\n            centers_logits = F.normalize(self.head_fc(self.fc.weight), dim=1)\n        return feat_mlp, logits, centers_logits\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.652782Z","iopub.execute_input":"2024-01-24T12:06:51.653087Z","iopub.status.idle":"2024-01-24T12:06:51.802860Z","shell.execute_reply.started":"2024-01-24T12:06:51.653052Z","shell.execute_reply":"2024-01-24T12:06:51.801913Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"resnetncl","metadata":{}},{"cell_type":"code","source":"\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\n\nimport numpy as np\nimport cv2\nimport os\nimport copy\nimport math\nfrom torch.nn.parameter import Parameter\n\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(\n            inplanes, planes, kernel_size=3, padding=1, bias=False, stride=stride\n        )\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, padding=1, bias=False, stride=1\n        )\n        self.bn2 = nn.BatchNorm2d(planes)\n        # self.downsample = downsample\n        if stride != 1 or self.expansion * planes != inplanes:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(\n                    inplanes,\n                    self.expansion * planes,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(self.expansion * planes),\n            )\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass BottleNeck(nn.Module):\n\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super(BottleNeck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu1 = nn.ReLU(True)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.relu2 = nn.ReLU(True)\n        self.conv3 = nn.Conv2d(\n            planes, planes * self.expansion, kernel_size=1, bias=False\n        )\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        if stride != 1 or self.expansion * planes != inplanes:\n            self.downsample = nn.Sequential(\n                nn.Conv2d(\n                    inplanes,\n                    self.expansion * planes,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(self.expansion * planes),\n            )\n        else:\n            self.downsample = None\n        self.relu = nn.ReLU(True)\n\n    def forward(self, x):\n        out = self.relu1(self.bn1(self.conv1(x)))\n\n        out = self.relu2(self.bn2(self.conv2(out)))\n\n        out = self.bn3(self.conv3(out))\n\n        if self.downsample != None:\n            residual = self.downsample(x)\n        else:\n            residual = x\n        out = out + residual\n        out = self.relu(out)\n        return out\n\n##kaiming init missing!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nclass ResNet(nn.Module):\n    def __init__(\n        self,\n        args,\n        block_type,\n        num_blocks,\n        last_layer_stride=2,\n    ):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.block = block_type\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(True)\n        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(num_blocks[0], 64)\n        self.layer2 = self._make_layer(\n            num_blocks[1], 128, stride=2\n        )\n        self.layer3 = self._make_layer(\n            num_blocks[2], 256, stride=2\n        )\n        self.layer4 = self._make_layer(\n            num_blocks[3],\n            512,\n            stride=last_layer_stride,\n        )\n\n    def load_model(self, pretrain):\n        print(\"Loading Backbone pretrain model from {}......\".format(pretrain))\n        model_dict = self.state_dict()\n        pretrain_dict = torch.load(pretrain)\n        pretrain_dict = pretrain_dict[\"state_dict\"] if \"state_dict\" in pretrain_dict else pretrain_dict\n        from collections import OrderedDict\n\n        new_dict = OrderedDict()\n        for k, v in pretrain_dict.items():\n            if k.startswith(\"module\"):\n                k = k[7:]\n            if \"fc\" not in k and \"classifier\" not in k:\n                k = k.replace(\"backbone.\", \"\")\n                new_dict[k] = v\n\n        model_dict.update(new_dict)\n        self.load_state_dict(model_dict)\n        print(\"Backbone model has been loaded......\")\n\n    def _make_layer(self, num_block, planes, stride=1):\n        strides = [stride] + [1] * (num_block - 1)\n        layers = []\n        for now_stride in strides:\n            layers.append(\n                self.block(\n                    self.inplanes, planes, stride=now_stride\n                )\n            )\n            self.inplanes = planes * self.block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x, **kwargs):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.pool(out)\n\n        out = self.layer1(out)\n        if 'layer' in kwargs and kwargs['layer'] == 'layer1':\n            out = kwargs['coef']*out + (1-kwargs['coef'])*out[kwargs['index']]\n        out = self.layer2(out)\n        if 'layer' in kwargs and kwargs['layer'] == 'layer2':\n            out = kwargs['coef']*out+(1-kwargs['coef'])*out[kwargs['index']]\n        out = self.layer3(out)\n        if 'layer' in kwargs and kwargs['layer'] == 'layer3':\n            out = kwargs['coef']*out+(1-kwargs['coef'])*out[kwargs['index']]\n        out = self.layer4(out)\n        if 'layer' in kwargs and kwargs['layer'] == 'layer4':\n            out = kwargs['coef']*out+(1-kwargs['coef'])*out[kwargs['index']]\n        return out\n\ndef res50(args,last_layer_stride=2):\n    return ResNet(args,BottleNeck,[3, 4, 6, 3],last_layer_stride=last_layer_stride)\n    \n\ndef res152(args,last_layer_stride=2):\n    return ResNet(args,BottleNeck,[3, 8, 36, 3],last_layer_stride=last_layer_stride)\n    \n\n\n\n\ndef _weights_init(m):\n    classname = m.__class__.__name__\n    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n        init.kaiming_normal_(m.weight)\n\n\nclass LambdaLayer(nn.Module):\n    def __init__(self, lambd):\n        super(LambdaLayer, self).__init__()\n        self.lambd = lambd\n\n    def forward(self, x):\n        return self.lambd(x)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, option=\"A\"):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n        )\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n        )\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            if option == \"A\":\n                \"\"\"\n                For CIFAR10 ResNet paper uses option A.\n                \"\"\"\n                self.shortcut = LambdaLayer(\n                    lambda x: F.pad(\n                        x[:, :, ::2, ::2],\n                        (0, 0, 0, 0, planes // 4, planes // 4),\n                        \"constant\",\n                        0,\n                    )\n                )\n            elif option == \"B\":\n                self.shortcut = nn.Sequential(\n                    nn.Conv2d(\n                        in_planes,\n                        self.expansion * planes,\n                        kernel_size=1,\n                        stride=stride,\n                        bias=False,\n                    ),\n                    nn.BatchNorm2d(self.expansion * planes),\n                )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet_Cifar(nn.Module):\n    def __init__(self, block, num_blocks):\n        super(ResNet_Cifar, self).__init__()\n        self.in_planes = 16\n\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n        self.apply(_weights_init)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n\n        return nn.Sequential(*layers)\n\n    def load_model(self, pretrain):\n        print(\"Loading Backbone pretrain model from {}......\".format(pretrain))\n        model_dict = self.state_dict()\n        pretrain_dict = torch.load(pretrain)\n        pretrain_dict = pretrain_dict[\"state_dict\"] if \"state_dict\" in pretrain_dict else pretrain_dict\n        from collections import OrderedDict\n\n        new_dict = OrderedDict()\n        for k, v in pretrain_dict.items():\n            if k.startswith(\"module\"):\n                k = k[7:]\n            if \"last_linear\" not in k and \"classifier\" not in k and \"linear\" not in k and \"fd\" not in k:\n                k = k.replace(\"backbone.\", \"\")\n                k = k.replace(\"fr\", \"layer3.4\")\n                new_dict[k] = v\n        model_dict.update(new_dict)\n        self.load_state_dict(model_dict)\n        print(\"Backbone model has been loaded......\")\n\n    def forward(self, x, **kwargs):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        if 'layer' in kwargs and kwargs['layer'] == 'layer1':\n            out = kwargs['coef']*out + (1-kwargs['coef'])*out[kwargs['index']]\n        out = self.layer2(out)\n        if 'layer' in kwargs and kwargs['layer'] == 'layer2':\n            out = kwargs['coef']*out+(1-kwargs['coef'])*out[kwargs['index']]\n        out = self.layer3(out)\n        if 'layer' in kwargs and kwargs['layer'] == 'layer3':\n            out = kwargs['coef']*out+(1-kwargs['coef'])*out[kwargs['index']]\n        return out\n\ndef res32_cifar(args,last_layer_stride):\n    return ResNet_Cifar(BasicBlock, [5, 5, 5])\n    \n\n\n\ndef ncl_model(args, num_class_list):\n    if args.dataset in ['cifar100', 'places']:\n        model = multi_Network_MOCO(args, mode=\"train\", num_classes=args.num_class).cuda()\n        comb = Combiner(args, num_class_list)\n    else:\n        model = multi_Network(args, mode=\"train\", num_classes=args.num_class).cuda()\n        comb = Combiner(args, num_class_list)\n    return {'comb': comb, 'model': model}\n\nclass Combiner:\n    def __init__(self, args, num_class_list=None):\n        self.args = args\n\n        if self.args.dataset in ['cifar100', 'places']:\n            self.type = 'multi_network_default_CON'\n        else:\n            self.type = 'multi_network_default'\n        \n        self.num_class_list = torch.FloatTensor(num_class_list)\n        self.epoch_number = self.args.epochs\n        self.initilize_all_parameters()\n\n    def initilize_all_parameters(self):\n\n        if self.args.dataset == 'cifar100':\n            self.show_step = 100\n            self.CON_ratio = 1.0    \n            self.distributed = False\n        elif self.args.dataset == 'places':\n            self.show_step = 200\n            self.CON_ratio = 1.0    \n            self.distributed = True\n        elif self.args.dataset == 'imgnet':\n            self.show_step = 200\n            self.CON_ratio = 0.0\n            self.distributed = True\n        elif self.args.dataset == 'inat':\n            self.show_step = 500\n            self.CON_ratio = 0.0\n            self.distributed = True\n\n    def update(self, epoch):\n        self.epoch = epoch\n\n\n    def forward(self, model, criterion, image, label):\n        return eval(\"self.{}\".format(self.type))(model, criterion, image, label)\n\n\n    def multi_network_default(self, model, criterion, image, label):\n\n        for i in range(len(image)):\n            image[i], label[i] = image[i].cuda(), label[i].cuda()\n\n\n        feature = model(image, feature_flag=True, label=label)\n        output = model(feature, classifier_flag=True)\n\n        loss = criterion(output, label)\n\n        average_result = torch.mean(torch.stack(output), dim=0)\n        \n        return loss\n\n    def multi_network_default_CON(self, model, criterion, image, label):\n\n        image_p = []\n        image_k = []\n        for i in range(len(image)):\n            image_p.append(image[i][0].cuda())\n            image_k.append(image[i][1].cuda())\n            label[i] = label[i].cuda()\n\n        # shuffle BN\n        if self.distributed:\n            image_k, idx_unshuffle = shuffle_BN_DDP(image_k)\n            pass\n        else:\n            image_k, idx_unshuffle = shuffle_BN(image_k)\n\n\n        feature = model((image_p, image_k), feature_flag=True, label=label)\n        output_ce, output_p, output_k = model(feature, classifier_flag=True)\n\n        # unshuffle\n        if self.distributed:\n            output_k = unshuffle_BN_DDP(output_k, idx_unshuffle)\n        else:\n            output_k = unshuffle_BN(output_k, idx_unshuffle)\n\n        loss_ce = criterion(output_ce, label, feature=feature, classifier=model.classifier)\n\n        average_result = torch.mean(torch.stack(output_ce), dim=0)\n        \n        # contrastive_loss\n        loss_CON = 0\n        for i, (q, k) in enumerate(zip(output_p, output_k)):\n            q = F.normalize(q, dim=1)\n            k = F.normalize(k, dim=1)\n            # compute logits\n            # Einstein sum is more intuitive\n            # positive logits: Nx1\n            l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)\n            # negative logits: NxK\n            l_neg = torch.einsum('nc,ck->nk', [q, model.MOCO[i].queue.clone().detach()])\n\n            # logits: Nx(1+K)\n            logits = torch.cat([l_pos, l_neg], dim=1)\n\n            # apply temperature\n            logits /= model.MOCO[i].T\n\n            # labels: positive key indicators\n            labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda()\n\n            # dequeue and enqueue\n            if self.distributed:\n                model.MOCO[i]._dequeue_and_enqueue_DDP(k)\n            else:\n                model.MOCO[i]._dequeue_and_enqueue(k)\n\n\n            loss_CON += F.cross_entropy(logits, labels)\n\n        loss = loss_ce + loss_CON * self.CON_ratio\n\n        return loss\n\n\n\nclass FCNorm(nn.Module):\n    def __init__(self, num_features, num_classes):\n        super(FCNorm, self).__init__()\n        self.weight = nn.Parameter(torch.FloatTensor(num_classes, num_features))\n        self.weight.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)\n\n    def forward(self, x):\n        out = F.linear(F.normalize(x), F.normalize(self.weight))\n        return out\n\n\nclass GAP(nn.Module):\n    \"\"\"Global Average pooling\n        Widely used in ResNet, Inception, DenseNet, etc.\n     \"\"\"\n\n    def __init__(self):\n        super(GAP, self).__init__()\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n\n    def forward(self, x):\n        x = self.avgpool(x)\n        #         x = x.view(x.shape[0], -1)\n        return x\n\nclass Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def forward(self, x):\n        return x\n\n\n@torch.no_grad()\ndef concat_all_gather(tensor):\n    \"\"\"\n    Performs all_gather operation on the provided tensors.\n    *** Warning ***: torch.distributed.all_gather has no gradient.\n    \"\"\"\n    #with torch.no_grad():\n    tensors_gather = [torch.ones_like(tensor)\n        for _ in range(torch.distributed.get_world_size())]\n    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n\n    output = torch.cat(tensors_gather, dim=0)\n    return output\n\n@torch.no_grad()\ndef shuffle_BN(image):\n    #with torch.no_grad():\n    batch_size = image[0].shape[0]\n    idx_shuffle = torch.randperm(batch_size).cuda()\n    for i in range(len(image)):\n        image[i] = image[i][idx_shuffle]\n    idx_unshuffle = torch.argsort(idx_shuffle)\n    return image, idx_unshuffle\n\n@torch.no_grad()\ndef shuffle_BN_DDP(x):\n    \"\"\"\n    Batch shuffle, for making use of BatchNorm.\n    *** Only support DistributedDataParallel (DDP) model. ***\n    \"\"\"\n    # gather from all gpus\n\n    #with torch.no_grad():\n    shuffle_list = []\n    idx_shuffle = 0\n    for i in range(len(x)):\n        batch_size_this = x[i].shape[0]\n        x_gather = concat_all_gather(x[i])\n        batch_size_all = x_gather.shape[0]\n\n        num_gpus = batch_size_all // batch_size_this\n\n        # random shuffle index\n        if i == 0:\n            idx_shuffle = torch.randperm(batch_size_all).cuda()\n            # index for restoring\n            idx_unshuffle = torch.argsort(idx_shuffle)\n\n        # broadcast to all gpus\n        torch.distributed.broadcast(idx_shuffle, src=0)\n\n\n\n        # shuffled index for this gpu\n        gpu_idx = torch.distributed.get_rank()\n        idx_this = idx_shuffle.view(num_gpus, -1)[gpu_idx]\n        shuffle_list.append(x_gather[idx_this])\n\n    return shuffle_list, idx_unshuffle\n\n@torch.no_grad()\ndef unshuffle_BN(x, idx_unshuffle):\n    #with torch.no_grad():\n    for i in range(len(x)):\n        x[i] = x[i][idx_unshuffle]\n    return x\n\n@torch.no_grad()\ndef unshuffle_BN_DDP(x, idx_unshuffle):\n    \"\"\"\n    Undo batch shuffle.\n    *** Only support DistributedDataParallel (DDP) model. ***\n    \"\"\"\n    # gather from all gpus\n   # with torch.no_grad():\n    unshuffle_list = []\n    for i in range(len(x)):\n        batch_size_this = x[i].shape[0]\n        x_gather = concat_all_gather(x[i])\n        batch_size_all = x_gather.shape[0]\n\n        num_gpus = batch_size_all // batch_size_this\n\n        # restored index for this gpu\n        gpu_idx = torch.distributed.get_rank()\n        idx_this = idx_unshuffle.view(num_gpus, -1)[gpu_idx]\n        unshuffle_list.append(x_gather[idx_this])\n\n    return unshuffle_list\n\nclass MoCo(nn.Module):\n    \"\"\"\n    Build a MoCo model with: a query encoder, a key encoder, and a queue\n    https://arxiv.org/abs/1911.05722\n    \"\"\"\n    def __init__(self, dim=128, K=65536, m=0.999, T=0.07):\n        \"\"\"\n        dim: feature dimension (default: 128)\n        K: queue size; number of negative keys (default: 65536)\n        m: moco momentum of updating key encoder (default: 0.999)\n        T: softmax temperature (default: 0.07)\n        \"\"\"\n        super(MoCo, self).__init__()\n\n        self.K = K\n        self.m = m\n        self.T = T\n\n        # create the queue\n        self.register_buffer(\"queue\", torch.randn(dim, K))\n        self.queue = nn.functional.normalize(self.queue, dim=0)\n\n        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n\n\n\n    @torch.no_grad()\n    def _dequeue_and_enqueue_DDP(self, keys):\n        # gather keys before updating queue\n        keys = concat_all_gather(keys)\n\n        batch_size = keys.shape[0]\n\n        ptr = int(self.queue_ptr)\n\n        assert self.K % batch_size == 0  # for simplicity\n\n        # replace the keys at ptr (dequeue and enqueue)\n        self.queue[:, ptr:ptr + batch_size] = keys.T\n        ptr = (ptr + batch_size) % self.K  # move pointer\n\n        self.queue_ptr[0] = ptr\n\n    @torch.no_grad()\n    def _dequeue_and_enqueue(self, keys, **kwargs):\n\n        batch_size = keys.shape[0]\n\n        ptr = int(self.queue_ptr)\n\n        assert self.K % batch_size == 0  # for simplicity\n\n        # replace the keys at ptr (dequeue and enqueue)\n        self.queue[:, ptr:ptr + batch_size] = keys.T\n\n        ptr = (ptr + batch_size) % self.K  # move pointer\n\n        self.queue_ptr[0] = ptr\n\nclass Cos_Classifier(nn.Module):\n    \"\"\" plain cosine classifier \"\"\"\n\n    def __init__(self, num_classes=10, in_dim=640, scale=16, bias=False):\n        super(Cos_Classifier, self).__init__()\n        self.scale = scale\n        self.weight = Parameter(torch.Tensor(num_classes, in_dim).cuda())\n        self.bias = Parameter(torch.Tensor(num_classes).cuda(), requires_grad=bias)\n        self.init_weights()\n\n    def init_weights(self):\n        self.bias.data.fill_(0.)\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, x, **kwargs):\n        ex = x / torch.norm(x.clone(), 2, 1, keepdim=True)\n        ew = self.weight / torch.norm(self.weight, 2, 1, keepdim=True)\n        out = torch.mm(ex, self.scale * ew.t()) + self.bias\n        return out\n\nclass multi_Network(nn.Module):\n    def __init__(self, args, mode=\"train\", num_classes=1000):\n        super(multi_Network, self).__init__()\n        \n        self.num_classes = num_classes\n        self.args = args\n        self.network_num = 3\n        \n        \n        if self.args.dataset == 'cifar100':\n            self.args.net_type = 'res32_cifar'\n            self.args.cf = 'FC'\n            self.args.cos_scale = 16\n        elif self.args.dataset == 'imgnet':\n            self.args.net_type = 'res50'\n            self.args.cf = 'COS'\n            self.args.cos_scale = 16\n        elif self.args.dataset == 'inat':\n            self.args.net_type = 'res50'\n            self.args.cf = 'COS'\n            self.args.cos_scale = 32\n            \n\n        self.backbone = nn.ModuleList(\n            eval(self.args.net_type)(\n                self.args,\n                last_layer_stride=2,\n            ) for i in range(self.network_num))\n\n        self.module = nn.ModuleList(\n            self._get_module()\n            for i in range(self.network_num))\n\n        \n        self.classifier = nn.ModuleList(\n            self._get_multi_classifer(True, self.cf)\n            for i in range(self.network_num))\n\n    def forward(self, input, **kwargs):\n\n        if \"feature_flag\" in kwargs:\n            return self.extract_feature(input, **kwargs)\n        elif \"classifier_flag\" in kwargs:\n            return self.get_logits(input, **kwargs)\n\n        logits = []\n        for i in range(self.network_num):\n            x = (self.backbone[i])(input[i], **kwargs)\n            x = (self.module[i])(x)\n            x = x.view(x.shape[0], -1)\n            self.feat.append(copy.deepcopy(x))\n            x = (self.classifier[i])(x)\n            logits.append(x)\n\n        return logits\n\n    def extract_feature(self, input, **kwargs):\n\n        feature = []\n        for i in range(self.network_num):\n            x = (self.backbone[i])(input[i])\n            x = (self.module[i])(x)\n            x = x.view(x.shape[0], -1)\n            feature.append(x)\n\n        return feature\n\n    def get_logits(self, input, **kwargs):\n\n        logits = []\n        for i in range(self.network_num):\n            x = input[i]\n            x = (self.classifier[i])(x)\n            logits.append(x)\n\n        return logits\n\n    def extract_feature_maps(self, x):\n        x = self.backbone(x)\n        return x\n\n    def freeze_multi_backbone(self):\n        print(\"Freezing backbone .......\")\n        for p in self.backbone.parameters():\n            p.requires_grad = False\n\n    def load_backbone_model(self, backbone_path=\"\"):\n        self.backbone.load_model(backbone_path)\n        print(\"Backbone model has been loaded...\")\n\n    def load_model(self, model_path, **kwargs):\n        pretrain_dict = torch.load(\n            model_path, map_location=\"cuda\"\n        )\n        pretrain_dict = pretrain_dict['state_dict'] if 'state_dict' in pretrain_dict else pretrain_dict\n        model_dict = self.state_dict()\n        from collections import OrderedDict\n        new_dict = OrderedDict()\n        for k, v in pretrain_dict.items():\n            if 'backbone_only' in kwargs.keys() and 'classifier' in k:\n                continue;\n            if k.startswith(\"module\"):\n                if k[7:] not in model_dict.keys():\n                    print('not load:{}'.format(k))\n                new_dict[k[7:]] = v\n            else:\n                new_dict[k] = v\n        model_dict.update(new_dict)\n        self.load_state_dict(model_dict)\n        print(\"All model has been loaded...\")\n\n    def get_feature_length(self):\n        if \"cifar\" in self.args.net_type:\n            num_features = 64\n        else:\n            num_features = 2048\n        return num_features\n\n    def _get_module(self):\n        module = GAP()\n        return module\n\n    def _get_multi_classifer(self, bias_flag, type):\n\n        num_features = self.get_feature_length()\n        if type == \"FCNorm\":\n            classifier = FCNorm(num_features, self.num_classes)\n        elif type == \"FC\":\n            classifier = nn.Linear(num_features, self.num_classes, bias=bias_flag)\n        elif type == 'cos':\n            classifier = Cos_Classifier(self.num_classes, num_features, scale=self.args.cos_scale, bias=bias_flag)\n        else:\n            raise NotImplementedError\n\n        return classifier\n\nclass multi_Network_MOCO(nn.Module):\n    def __init__(self, args, mode=\"train\", num_classes=1000):\n        super(multi_Network_MOCO, self).__init__()\n        \n        self.args = args\n        self.num_classes = num_classes\n        self.network_num = 3\n        \n        if self.args.dataset == 'cifar100':\n            self.args.net_type = 'res32_cifar'\n            self.args.cf = 'FC'\n            self.args.scf = 'mlp'\n            self.args.cos_scale = 16\n            self.args.moco_dim = 64\n            self.args.mlp_dim = self.args.moco_dim\n            self.args.moco_k = 1024\n            self.args.moco_t = 0.2\n        \n        elif self.args.dataset == 'imgnet':\n            self.args.net_type = 'res50'\n            self.args.cf = 'COS'\n            self.args.cos_scale = 16\n\n        elif self.args.dataset == 'inat':\n            self.args.net_type = 'res50'\n            self.args.cf = 'COS'\n            self.args.cos_scale = 32\n\n        self.MOCO = nn.ModuleList(\n            MoCo(dim=self.args.moco_dim, K=self.args.moco_k, T=self.args.moco_t)\n            for i in range(self.network_num))\n\n\n        self.backbone = nn.ModuleList(\n            eval(self.args.net_type)(\n                self.args,\n                last_layer_stride=2,\n            ) for i in range(self.network_num))\n\n\n        self.module = nn.ModuleList(\n            self._get_module()\n            for i in range(self.network_num))\n\n        \n        self.classifier = nn.ModuleList(\n            self._get_multi_classifer(True, self.args.scf)\n            for i in range(self.network_num))\n        self.feat = []\n\n        self.backbone_MA = nn.ModuleList(\n            eval(self.args.net_type)(\n                self.args,\n                last_layer_stride=2,\n            ) for i in range(self.network_num))\n\n        for i in range(self.network_num):\n            for param in self.backbone_MA[i].parameters():\n                param.detach_()\n\n        self.module_MA = nn.ModuleList(\n            self._get_module()\n            for i in range(self.network_num))\n        for i in range(self.network_num):\n            for param in self.module_MA[i].parameters():\n                param.detach_()\n\n        \n        self.classifier_MA = nn.ModuleList(\n            self._get_multi_classifer(True, self.args.scf)\n            for i in range(self.network_num))\n        for i in range(self.network_num):\n            for param in self.classifier_MA[i].parameters():\n                param.detach_()\n        self.feat_MA = []\n\n        if self.args.cf == 'FC':\n            self.classifier_ce = nn.ModuleList(\n                nn.Linear(self.get_feature_length(), self.num_classes, True)\n                for i in range(self.network_num))\n        elif self.args.cf == 'cos':\n            self.classifier_ce = nn.ModuleList(\n                Cos_Classifier(self.num_classes, in_dim=self.get_feature_length(), scale=self.args.cos_scale, bias=True)\n                for i in range(self.network_num))\n\n    def forward(self, input, **kwargs):\n\n\n        if \"feature_flag\" in kwargs:\n            return self.extract_feature(input, **kwargs)\n        elif \"classifier_flag\" in kwargs:\n            return self.get_logits(input, **kwargs)\n\n        logits = []\n        logits_ce = []\n        for i in range(self.network_num):\n            x = (self.backbone[i])(input[i], **kwargs)\n            x = (self.module[i])(x)\n            feature = x.view(x.shape[0], -1)\n            self.feat.append(copy.deepcopy(feature))\n            \n            output = (self.classifier[i])(feature)\n            logits.append(output)\n\n            output_ce = (self.classifier_ce[i])(feature)\n            logits_ce.append(output_ce)\n\n        logits_MA = []\n        for i in range(self.network_num):\n            x = (self.backbone_MA[i])(input[i], **kwargs)\n            x = (self.module_MA[i])(x)\n            x = x.view(x.shape[0], -1)\n            self.feat_MA.append(copy.deepcopy(x))\n            x = (self.classifier_MA[i])(x)\n            logits_MA.append(x)\n\n        return logits_ce, logits, logits_MA\n\n    def extract_feature(self, input_all, **kwargs):\n\n        input, input_MA = input_all\n\n        feature = []\n        for i in range(self.network_num):\n            x = (self.backbone[i])(input[i], label=kwargs['label'][i])\n            x = (self.module[i])(x)\n            x = x.view(x.shape[0], -1)\n            feature.append(x)\n\n        feature_MA = []\n        for i in range(self.network_num):\n            x = (self.backbone_MA[i])(input_MA[i], label=kwargs['label'][i])\n            x = (self.module_MA[i])(x)\n            x = x.view(x.shape[0], -1)\n            feature_MA.append(x)\n        return feature, feature_MA\n\n    def get_logits(self, input_all, **kwargs):\n\n        input, input_MA = input_all\n        logits = []\n        logits_ce = []\n        for i in range(self.network_num):\n            feature = input[i]\n            \n            output = (self.classifier[i])(feature)\n            logits.append(output)\n\n            output_ce = (self.classifier_ce[i])(feature)\n            logits_ce.append(output_ce)\n\n        logits_MA = []\n        for i in range(self.network_num):\n            x = input_MA[i]\n            x = (self.classifier_MA[i])(x)\n            logits_MA.append(x)\n\n        return logits_ce, logits, logits_MA\n\n    def extract_feature_maps(self, x):\n        x = self.backbone(x)\n        return x\n\n    def freeze_multi_backbone(self):\n        print(\"Freezing backbone .......\")\n        for p in self.backbone.parameters():\n            p.requires_grad = False\n\n    def load_backbone_model(self, backbone_path=\"\"):\n        self.backbone.load_model(backbone_path)\n        print(\"Backbone model has been loaded...\")\n\n    def load_model(self, model_path, **kwargs):\n        pretrain_dict = torch.load(\n            model_path, map_location=\"cuda\"\n        )\n        pretrain_dict = pretrain_dict['state_dict'] if 'state_dict' in pretrain_dict else pretrain_dict\n        model_dict = self.state_dict()\n        from collections import OrderedDict\n        new_dict = OrderedDict()\n        for k, v in pretrain_dict.items():\n            if 'backbone_only' in kwargs.keys() and 'classifier' in k:\n                continue;\n            if k.startswith(\"module\"):\n                if k[7:] not in model_dict.keys():\n                    print('not load:{}'.format(k))\n                    continue\n                new_dict[k[7:]] = v\n            else:\n                new_dict[k] = v\n        model_dict.update(new_dict)\n        self.load_state_dict(model_dict)\n        print(\"All model has been loaded...\")\n\n    def get_feature_length(self):\n        if \"cifar\" in self.args.net_type:\n            num_features = 64\n        else:\n            num_features = 2048\n        return num_features\n\n    def _get_module(self):\n        module = GAP()\n        return module\n\n    def _get_multi_classifer(self, bias_flag, type):\n\n        num_features = self.get_feature_length()\n        if type == \"FCNorm\":\n            classifier = FCNorm(num_features, self.args.mlp_dim)\n        elif type == \"FC\":\n            classifier = nn.Linear(num_features, self.args.mlp_dim, bias=bias_flag)\n        elif type == \"mlp\":\n            classifier = nn.Sequential(nn.Linear(num_features, num_features, bias=bias_flag), \\\n                                       nn.ReLU(), \\\n                                       nn.Linear(num_features, self.args.mlp_dim, bias=bias_flag))\n        elif type == 'cos':\n            classifier = Cos_Classifier(self.args.mlp_dim, num_features, scale=self.args.cos_scale, bias=bias_flag)\n        else:\n            raise NotImplementedError\n\n        return classifier\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.804188Z","iopub.execute_input":"2024-01-24T12:06:51.804471Z","iopub.status.idle":"2024-01-24T12:06:51.985361Z","shell.execute_reply.started":"2024-01-24T12:06:51.804447Z","shell.execute_reply":"2024-01-24T12:06:51.984468Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"resnetride","metadata":{}},{"cell_type":"code","source":"import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.nn import Parameter\n\nimport random\n\n__all__ = ['resnet32_ride']\n\ndef _weights_init(m):\n    classname = m.__class__.__name__\n    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n        init.kaiming_normal_(m.weight)\n\nclass NormedLinear(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(NormedLinear, self).__init__()\n        self.weight = nn.Parameter(torch.Tensor(in_features, out_features))\n        self.weight.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)\n\n    def forward(self, x):\n        out = F.normalize(x, dim=1).mm(F.normalize(self.weight, dim=0))\n        return out\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, in_planes, planes, stride=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        \n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                     nn.BatchNorm2d(self.expansion * planes)\n                )\n            \n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n    \nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion *\n                               planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n    \nclass ResNet_s(nn.Module):\n    def __init__(self, block, num_blocks, num_experts, num_classes=10, \n                 reduce_dimension=False, layer2_output_dim=None, \n                 layer3_output_dim=None, use_norm=False, use_experts=None, s=30):\n        super(ResNet_s, self).__init__()\n        \n        self.in_planes = 16\n        self.num_experts = num_experts\n        \n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n        self.in_planes = self.next_in_planes\n        \n        if layer2_output_dim is None:\n            if reduce_dimension:\n                layer2_output_dim = 24\n            else:\n                layer2_output_dim = 32\n                \n        if layer3_output_dim is None:\n            if reduce_dimension:\n                layer3_output_dim = 48\n            else:\n                layer3_output_dim = 64\n                \n        self.layer2s = nn.ModuleList([self._make_layer(block, layer2_output_dim, num_blocks[1], stride=2) for _ in range(num_experts)])\n        self.in_planes = self.next_in_planes\n        self.layer3s = nn.ModuleList([self._make_layer(block, layer3_output_dim, num_blocks[2], stride=2) for _ in range(num_experts)])\n        self.in_planes = self.next_in_planes\n        \n        if use_norm:\n            self.linears = nn.ModuleList([NormedLinear(layer3_output_dim, num_classes) for _ in range(num_experts)])\n        else:\n            self.linears = nn.ModuleList([nn.Linear(layer3_output_dim, num_classes) for _ in range(num_experts)])\n            s = 1\n            \n        if use_experts is None:\n            self.use_experts = list(range(num_experts))\n        elif use_experts == \"rand\":\n            self.use_experts = None\n        else:\n            self.use_experts = [int(item) for item in use_experts.split(\",\")]\n            \n        self.s = s\n        self.apply(_weights_init)\n        \n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        self.next_in_planes = self.in_planes\n        for stride in strides:\n            layers.append(block(self.next_in_planes, planes, stride))\n            self.next_in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n    \n    def _hook_before_iter(self):\n        assert self.training, \"_hook_before_iter should be called at training time only, after train() is called\"\n        count = 0\n        for module in self.modules():\n            if isinstance(module, nn.BatchNorm2d):\n                if module.weight.requires_grad == False:\n                    module.eval()\n                    count += 1\n                    \n        if count > 0:\n            print(\"Warning: detected at least one frozen BN, set them to eval state. Count:\", count)\n            \n    def _separate_part(self, x, ind):\n        out = x\n        out = (self.layer2s[ind])(out)\n        out = (self.layer3s[ind])(out)\n        self.feat_before_GAP.append(out)\n        out = F.avg_pool2d(out, out.size()[3])\n        out = out.view(out.size(0), -1)\n        self.feat.append(out)\n        out = (self.linears[ind])(out)\n        out = out * self.s\n        return out\n    \n    def forward(self, x, output_type = 'dict'):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        \n        outs = []\n        self.feat = []\n        self.logits = outs\n        self.feat_before_GAP = []\n        \n        if self.use_experts is None:\n            use_experts = random.sample(range(self.num_experts), self.num_experts - 1)\n        else:\n            use_experts = self.use_experts\n            \n        for ind in use_experts:\n            outs.append(self._separate_part(out, ind))\n        final_out = torch.stack(outs, dim=1).mean(dim=1)\n\n        if output_type == 'dict':\n            return {\"output\": final_out, \"logits\": torch.stack(outs, dim=1)}\n        else:\n            return final_out\n        \ndef resnet32_ride(num_class, use_norm=True, num_experts=3):\n    return ResNet_s(BasicBlock, [5,5,5], num_experts, num_classes=num_class, use_norm=use_norm, reduce_dimension=True)\n\ndef test(net):\n    import numpy as np\n    total_params = 0\n\n    for x in filter(lambda p: p.requires_grad, net.parameters()):\n        total_params += np.prod(x.data.numpy().shape)\n    print(\"Total number of params\", total_params)\n    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n    \nif __name__ == \"__main__\":\n    for net_name in __all__:\n        if net_name.startswith(\"resnet\"):\n            print(net_name)\n            test(globals()[net_name](2))\n            print()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:51.990965Z","iopub.execute_input":"2024-01-24T12:06:51.991293Z","iopub.status.idle":"2024-01-24T12:06:52.083675Z","shell.execute_reply.started":"2024-01-24T12:06:51.991265Z","shell.execute_reply":"2024-01-24T12:06:52.082756Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"resnet32_ride\nTotal number of params 774784\nTotal layers 80\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"resnet","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.nn import Parameter\n\n__all__ = ['resnet32', 'NormedLinear']\n\ndef _weights_init(m):\n    classname = m.__class__.__name__\n    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n        init.kaiming_normal_(m.weight)\n\nclass NormedLinear(nn.Module):\n\n    def __init__(self, in_features, out_features):\n        super(NormedLinear, self).__init__()\n        self.weight = Parameter(torch.Tensor(in_features, out_features))\n        self.weight.data.uniform_(-1, 1).renorm_(2, 1, 1e-5).mul_(1e5)\n\n    def forward(self, x):\n        out = F.normalize(x, dim=1).mm(F.normalize(self.weight, dim=0))\n        return out\n\nclass LambdaLayer(nn.Module):\n\n    def __init__(self, lambd):\n        super(LambdaLayer, self).__init__()\n        self.lambd = lambd\n\n    def forward(self, x):\n        return self.lambd(x)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1, option='A'):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            if option == 'A':\n                \"\"\"\n                For CIFAR10 ResNet paper uses option A.\n                \"\"\"\n                self.shortcut = LambdaLayer(lambda x:\n                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n            elif option == 'B':\n                self.shortcut = nn.Sequential(\n                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n                     nn.BatchNorm2d(self.expansion * planes)\n                )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet_s(nn.Module):\n\n    def __init__(self, block, num_blocks, num_classes=10, use_norm=False):\n        super(ResNet_s, self).__init__()\n        self.in_planes = 16\n\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n        if use_norm:\n            self.linear = NormedLinear(64, num_classes)\n        else:\n            self.linear = nn.Linear(64, num_classes)\n        self.apply(_weights_init)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x, output_type='feat'):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.avg_pool2d(out, out.size()[3])\n        out1 = out.view(out.size(0), -1)\n        out = self.linear(out1)\n        if output_type == 'feat':\n            return out, out1\n        else:\n            return out\n\ndef resnet32(num_class, use_norm):\n    return ResNet_s(BasicBlock, [5,5,5], num_class, use_norm=use_norm)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:52.085052Z","iopub.execute_input":"2024-01-24T12:06:52.085404Z","iopub.status.idle":"2024-01-24T12:06:52.108293Z","shell.execute_reply.started":"2024-01-24T12:06:52.085370Z","shell.execute_reply":"2024-01-24T12:06:52.107419Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"net.py","metadata":{}},{"cell_type":"code","source":"import torch\nimport shutil\n#from models.resnet import *\n#from models.resnet_ride import *\n#from models.resnet_bcl import *\n#from models.resnet_ncl import *\n\nimport torch.nn as nn\nimport torchvision.models as models\n\ndef get_model(args, num_class_list):\n    if args.loss_fn in ['ride']:\n        model = resnet32_ride(args.num_class, num_experts=args.num_experts).cuda()\n        print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n    \n    elif args.loss_fn in ['ncl']:\n        model = ncl_model(args, num_class_list)\n        print('    Total params: %.2fM' % (sum(p.numel() for p in model['model'].parameters())/1000000.0))\n\n    elif args.loss_fn in ['bcl']:\n        model = bcl_model(args.num_class, use_norm=args.use_norm).cuda()\n        print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n\n    \n    else:\n        model = resnet32(args.num_class, use_norm= args.loss_fn == 'ldam_drw').cuda()\n        print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n    \n    \n    torch.backends.cudnn.benchmark = True\n    return model   \n    \n\n\ndef load_model(args):\n    if args.loss_fn == 'ride' and args.num_experts == 3 and args.ride_distill:\n        print(\"---- ride teacher load ----\")\n        filepath = os.path.join(args.out, 'checkpoint_teacher.pth.tar')\n        if os.path.isfile(filepath):\n            pass    \n        else:\n            shutil.copy2(os.path.join(args.out, 'checkpoint.pth.tar'), os.path.join(args.out, 'checkpoint_teacher.pth.tar'))\n        checkpoint = torch.load(filepath)\n        teacher = resnet32_ride(args.num_class, num_experts = 6).cuda()\n        teacher.load_state_dict(checkpoint['state_dict'])\n    else:\n        teacher = None\n    return teacher\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:52.109432Z","iopub.execute_input":"2024-01-24T12:06:52.109727Z","iopub.status.idle":"2024-01-24T12:06:52.123750Z","shell.execute_reply.started":"2024-01-24T12:06:52.109681Z","shell.execute_reply":"2024-01-24T12:06:52.122919Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"config","metadata":{}},{"cell_type":"code","source":"import argparse, torch, os, random\nimport numpy as np\n\ndef parse_args(run_type = 'terminal'):\n    parser = argparse.ArgumentParser(description='Python Training')\n    \n    # Optimization options\n    parser.add_argument('--network', default='resnet32', help='Network: resnet32')\n    parser.add_argument('--epochs', default=200, type=int, metavar='N', help='number of total epochs to run')\n    parser.add_argument('--batch-size', default=128, type=int, metavar='N', help='train batchsize')\n    parser.add_argument('--update-epoch', default=1, type=int, metavar='N', help='Update epoch')\n    parser.add_argument('--lr', '--learning-rate', default=0.1, type=float, metavar='LR', help='initial learning rate')\n    parser.add_argument('--lr_decay', default=0.01, type=float, help='learnign rate decay')\n    parser.add_argument('--momentum', default=0.9, type=float, help='SGD momentum')\n    parser.add_argument('--wd', default=2e-4, type=float, help='weight decay factor for optimizer')\n    parser.add_argument('--nesterov', action='store_true', help=\"Utilizing Nesterov\")\n    parser.add_argument('--scheduler', default='warmup', type=str, help='LR scheduler')\n    parser.add_argument('--warmup', default=5, type=int, help='Warmup epochs')\n        \n    parser.add_argument('--aug_prob', default=0.5, type=float, help='Augmentation Coin-tossing Probability')\n    parser.add_argument('--cutout', action='store_true', help='Utilizing Cutout')\n    parser.add_argument('--cmo', action='store_true', help='Utilizing CMO')\n    parser.add_argument('--posthoc_la', action='store_true', help='Posthoc LA for state update')\n    parser.add_argument('--cuda', action='store_true', help='Use CUDA')\n    parser.add_argument('--aug_type', default='none')\n    parser.add_argument('--sim_type', default='none')\n    parser.add_argument('--max_d', type=int, default=30, help='max_d')\n\n    parser.add_argument('--num_test', default=10, type=int, help='Curriculum Test')\n    parser.add_argument('--accept_rate', type=float, default=0.6, help='Increasing accept ratio')\n    parser.add_argument('--verbose', action='store_true', help='Debug on/off')\n    parser.add_argument('--use_norm', action='store_true', help='Utilize Normed Linear')\n    \n    # Checkpoints\n    parser.add_argument('--out', default='./results/', help='Directory to output the result')\n    parser.add_argument('--data_dir', default='~/dataset/')\n    \n    # Miscs\n    parser.add_argument('--workers', type=int, default=4, help='# workers')\n    parser.add_argument('--seed', type=str, default='None', help='manual seed')\n    parser.add_argument('--gpu', default=None, type=str, help='id(s) for CUDA_VISIBLE_DEVICES')\n    \n    # Dataset options\n    parser.add_argument('--dataset', default='cifar100', help='Dataset: cifar100')\n    parser.add_argument('--num_max', type=int, default=500, help='Number of samples in the maximal class')\n    parser.add_argument('--imb_ratio', type=int, default=100, help='Imbalance ratio for data')\n    \n    # Method options\n    parser.add_argument('--loss_fn', type=str, default='ce', help='Loss function for training')\n    parser.add_argument('--num_experts', type=int, default=3, help='Number of experts for RIDE')\n    parser.add_argument('--ride_distill', action='store_true', help='Use RIDEWithDistill Loss')\n    \n    if run_type == 'terminal':\n        args = parser.parse_args()\n    elif run_type =='jupyter':\n        args = parser.parse_args(args=[])\n        \n    args.out = f'{args.out}{args.dataset}/{args.loss_fn}@N_{args.num_max}_ir_{args.imb_ratio}/'\n    \n    if args.gpu:\n        os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n    return args\n\n\ndef reproducibility(seed):\n    if seed == 'None':\n        return\n    else:\n        seed = int(seed)\n        torch.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        np.random.seed(seed)\n        random.seed(seed)\n\ndef dataset_argument(args):\n    if args.dataset == 'cifar100':\n        args.num_class = 100\n    else:\n        args.num_class = 10\n\n    return args\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:52.124922Z","iopub.execute_input":"2024-01-24T12:06:52.125237Z","iopub.status.idle":"2024-01-24T12:06:52.144608Z","shell.execute_reply.started":"2024-01-24T12:06:52.125212Z","shell.execute_reply":"2024-01-24T12:06:52.143661Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"logger","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nfrom datetime import datetime\nimport os\nimport torch as t\n\nimport pandas as pd\n\nclass logger:\n    def __init__(self, args):\n            \n        self.logger = logging.getLogger('Evaluation')\n        self.logger.setLevel(logging.INFO)\n        self.args = args\n        \n        formatter = logging.Formatter('%(message)s')\n        \n        strm_handler = logging.StreamHandler()\n        strm_handler.setFormatter(formatter)\n        \n        now = datetime.now()\n        time = f'{now.hour}:{now.minute}:{now.second}-{now.year}-{now.month}-{now.day}'\n        os.makedirs(f'{args.out}',exist_ok=True)\n        file_handler = logging.FileHandler(f'{args.out}/{time.replace(\":\", \"-\")}.txt')\n\n\n        file_handler.setFormatter(formatter)\n                        \n        self.logger.addHandler(strm_handler)\n        self.logger.addHandler(file_handler)\n\n        message = f'---{args.dataset}---'\n        self(message, level=1)\n        self.arg_logging(args)\n\n    def __call__(self,message, level):\n        if level == 1:\n            prefix = '--->' \n        else:\n            prefix = '  '*level + '>'\n        \n        self.logger.info(f'{prefix} {message}')\n\n\n    def arg_logging(self, argument):\n        self('Argument', level=1)\n        arg_dict = vars(argument)\n        for key in arg_dict.keys():\n            if key == 'logger':\n                pass\n            else:\n                self(f'{key:12s}: {arg_dict[key]}', level=2)\n\n    def map_save(self, map):\n        map_df = pd.DataFrame(map)\n        map_df.to_csv(f'{self.args.out}/curriculum.csv',encoding='utf-8')","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:52.145945Z","iopub.execute_input":"2024-01-24T12:06:52.146303Z","iopub.status.idle":"2024-01-24T12:06:52.159476Z","shell.execute_reply.started":"2024-01-24T12:06:52.146278Z","shell.execute_reply":"2024-01-24T12:06:52.158552Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"plot","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch as t\nimport numpy as np\nimport os\nimport pandas as pd\n\nsns.set_palette(\"bright\")\nsns.set_style(\"darkgrid\")\n\ndef plot_score_epoch(curr_state, label, epoch, maps, out, name='heat'):\n    label = t.tensor(label)\n    \n    num_samples_per_class = t.sum(t.nn.functional.one_hot(label, num_classes=len(t.unique(label))), dim=0)\n    num_samples_sort = t.argsort(num_samples_per_class)\n    \n    for cidx in t.unique(label):\n        pos = t.where(cidx == label)\n        maps[epoch, cidx] = t.mean(curr_state[pos]).numpy()\n\n    # Transpose the matrix before plotting\n    transposed_maps = np.transpose(maps)\n\n    sns.heatmap(transposed_maps, cmap='YlGnBu', vmin=0, vmax=10)\n    plt.xlabel('Epoch')\n    plt.ylabel('Class index')\n\n    # Flip the graph vertically before saving\n    plt.gca().invert_yaxis()\n\n    os.makedirs(f'{out}/score_epoch_plot/', exist_ok=True)\n    plt.savefig(f'{out}/score_epoch_plot/{name}.png')\n\n    plt.close()\n\n    return maps\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:52.160504Z","iopub.execute_input":"2024-01-24T12:06:52.160794Z","iopub.status.idle":"2024-01-24T12:06:52.434027Z","shell.execute_reply.started":"2024-01-24T12:06:52.160769Z","shell.execute_reply":"2024-01-24T12:06:52.433227Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(torch.__version__)\nprint(f\"CUDA version: {torch.version.cuda}\")\ntorch.has_mps","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:06:52.435089Z","iopub.execute_input":"2024-01-24T12:06:52.435369Z","iopub.status.idle":"2024-01-24T12:06:52.442852Z","shell.execute_reply.started":"2024-01-24T12:06:52.435344Z","shell.execute_reply":"2024-01-24T12:06:52.441944Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"2.0.0\nCUDA version: 11.8\n","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import losses\n\n#from datasets.cifar100 import *\n\n#from train.train import *\n#from train.validate import *\n\n#from models.net import *\n\n#from losses.loss import *\n\n#from utils.config import *\n#from utils.plot import *\n#from utils.common import make_imb_data, save_checkpoint, hms_string\n\n#from utils.logger import logger\n\n#args = parse_args()\n\nfrom argparse import Namespace\n\n# Replace the command below with your actual values\nargs=Namespace(network='resnet32', epochs=200, batch_size=128, update_epoch=1,\n               lr=0.1, lr_decay=0.01, momentum=0.9, wd=0.0002, nesterov=False,\n               scheduler='warmup', warmup=5, aug_prob=0.5, cutout=False, cmo=False,\n               posthoc_la=False, cuda=False, aug_type='none', sim_type='none', max_d=30,\n               num_test=10, accept_rate=0.6, verbose=True, use_norm=False,\n               out='/kaggle/working/log3',\n               data_dir='~/dataset/', workers=4, seed='None',\n               gpu='0', dataset='cifar100', num_max=500, imb_ratio=100,\n               loss_fn='ce', num_experts=3, ride_distill=False)\n\nreproducibility(args.seed)\nargs = dataset_argument(args)\nargs.logger = logger(args)\n\nbest_acc = 0 # best test accuracy\ncurr_state_ac=[]\nlabel_ac=[]\nvariance_list=[]\n\ndef main():\n    global best_acc,curr_state_ac,label_ac,variance_list\n\n    try:\n        assert args.num_max <= 50000. / args.num_class\n    except AssertionError:\n        args.num_max = int(50000 / args.num_class)\n    \n    print(f'==> Preparing imbalanced CIFAR-100')\n    # N_SAMPLES_PER_CLASS = make_imb_data(args.num_max, args.num_class, args.imb_ratio)\n    trainset, testset = get_cifar100(os.path.join(args.data_dir, 'cifar100/'), args)\n    N_SAMPLES_PER_CLASS = trainset.img_num_list\n        \n    trainloader = data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, drop_last= args.loss_fn == 'ncl', pin_memory=True, sampler=None)\n    testloader = data.DataLoader(testset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True) \n    \n    if args.cmo:\n        cls_num_list = N_SAMPLES_PER_CLASS\n        cls_weight = 1.0 / (np.array(cls_num_list))\n        cls_weight = cls_weight / np.sum(cls_weight) * len(cls_num_list)\n        labels = trainloader.dataset.targets\n        samples_weight = np.array([cls_weight[t] for t in labels])\n        samples_weight = torch.from_numpy(samples_weight)\n        samples_weight = samples_weight.double()\n        print(\"samples_weight\", samples_weight)\n        sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(labels), replacement=True)\n        weighted_trainloader = data.DataLoader(trainset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True, sampler=sampler)\n    else:\n        weighted_trainloader = None\n    \n\n    # Model\n    print (\"==> creating {}\".format(args.network))\n    model = get_model(args, N_SAMPLES_PER_CLASS)\n    train_criterion = get_loss(args, N_SAMPLES_PER_CLASS)\n    criterion = nn.CrossEntropyLoss() # For test, validation \n    optimizer = get_optimizer(args, model)\n    scheduler = get_scheduler(args,optimizer)\n\n    teacher = load_model(args)\n\n\n    train = get_train_fn(args)\n    validate = get_valid_fn(args)\n    update_score = get_update_score_fn(args)\n    \n    start_time = time.time()\n    \n    test_accs = []\n    for epoch in range(args.epochs):\n        \n        lr = adjust_learning_rate(optimizer, epoch, scheduler, args)\n        if args.cuda:\n            if epoch % args.update_epoch == 0:\n                curr_state_ac, label_ac = update_score(trainloader, model, N_SAMPLES_PER_CLASS, posthoc_la = args.posthoc_la, num_test = args.num_test, accept_rate = args.accept_rate)\n                print(curr_state_ac)\n                \n            if args.verbose:\n                if epoch == 0:\n                    maps = np.zeros((args.epochs,args.num_class))\n                maps = plot_score_epoch(curr_state_ac,label_ac, epoch, maps, args.out)\n        train_loss = train(args, trainloader, model, optimizer,train_criterion, epoch, weighted_trainloader, teacher) \n\n\n        test_loss, test_acc, test_cls = validate(args, testloader, model, criterion, N_SAMPLES_PER_CLASS,  num_class=args.num_class, mode='test Valid')\n\n        if best_acc <= test_acc:\n            best_acc = test_acc\n            many_best = test_cls[0]\n            med_best = test_cls[1]\n            few_best = test_cls[2]\n            # Save models\n            save_checkpoint({\n                'epoch': epoch + 1,\n                'state_dict': model['model'].state_dict() if args.loss_fn == 'ncl' else model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n            }, epoch + 1, args.out)\n        test_accs.append(test_acc)\n        \n        model_weights = model.state_dict()\n        classwise_l1_norms = {}\n        for class_label in range(args.num_class):\n            class_params = {\n            'linear.weight': model_weights['linear.weight'][class_label],\n            'linear.bias': model_weights['linear.bias'][class_label]\n            }\n            class_l1_norm = sum(torch.abs(param).sum() for param in class_params.values())\n            classwise_l1_norms[f'Class_{class_label}'] = class_l1_norm\n        l1_norm_values = [tensor.item() for tensor in classwise_l1_norms.values()]\n        variance_l1_norm = np.var(l1_norm_values)\n        variance_list.append(np.sqrt(variance_l1_norm))\n        print(np.sqrt(variance_l1_norm))\n        \n        args.logger(f'Epoch: [{epoch+1} | {args.epochs}]', level=1)\n        if args.cuda:\n            args.logger(f'Max_state: {int(torch.max(curr_state_ac))}, min_state: {int(torch.min(curr_state_ac))}', level=2)\n        args.logger(f'[Train]\\tLoss:\\t{train_loss:.4f}', level=2)\n        args.logger(f'[Test ]\\tLoss:\\t{test_loss:.4f}\\tAcc:\\t{test_acc:.4f}', level=2)\n        args.logger(f'[Stats]\\tMany:\\t{test_cls[0]:.4f}\\tMedium:\\t{test_cls[1]:.4f}\\tFew:\\t{test_cls[2]:.4f}', level=2)\n        args.logger(f'[Best ]\\tAcc:\\t{np.max(test_accs):.4f}\\tMany:\\t{100*many_best:.4f}\\tMedium:\\t{100*med_best:.4f}\\tFew:\\t{100*few_best:.4f}', level=2)\n        args.logger(f'[Param]\\tLR:\\t{lr:.8f}', level=2)\n    \n    end_time = time.time()\n\n    # Print the final results\n    args.logger(f'Final performance...', level=1)\n    args.logger(f'best bAcc (test):\\t{np.max(test_accs)}', level=2)\n    args.logger(f'best statistics:\\tMany:\\t{many_best}\\tMed:\\t{med_best}\\tFew:\\t{few_best}', level=2)\n    args.logger(f'Training Time: {hms_string(end_time - start_time)}', level=1)\n\n    \n    if args.verbose:\n        args.logger.map_save(maps)\n\nif __name__ == '__main__':\n    main()\n\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:46:24.649237Z","iopub.execute_input":"2024-01-24T12:46:24.649635Z","iopub.status.idle":"2024-01-24T13:00:51.154528Z","shell.execute_reply.started":"2024-01-24T12:46:24.649601Z","shell.execute_reply":"2024-01-24T13:00:51.153107Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"---> ---cifar100---\n---> ---cifar100---\n---> ---cifar100---\n---> Argument\n---> Argument\n---> Argument\n    > network     : resnet32\n    > network     : resnet32\n    > network     : resnet32\n    > epochs      : 200\n    > epochs      : 200\n    > epochs      : 200\n    > batch_size  : 128\n    > batch_size  : 128\n    > batch_size  : 128\n    > update_epoch: 1\n    > update_epoch: 1\n    > update_epoch: 1\n    > lr          : 0.1\n    > lr          : 0.1\n    > lr          : 0.1\n    > lr_decay    : 0.01\n    > lr_decay    : 0.01\n    > lr_decay    : 0.01\n    > momentum    : 0.9\n    > momentum    : 0.9\n    > momentum    : 0.9\n    > wd          : 0.0002\n    > wd          : 0.0002\n    > wd          : 0.0002\n    > nesterov    : False\n    > nesterov    : False\n    > nesterov    : False\n    > scheduler   : warmup\n    > scheduler   : warmup\n    > scheduler   : warmup\n    > warmup      : 5\n    > warmup      : 5\n    > warmup      : 5\n    > aug_prob    : 0.5\n    > aug_prob    : 0.5\n    > aug_prob    : 0.5\n    > cutout      : False\n    > cutout      : False\n    > cutout      : False\n    > cmo         : False\n    > cmo         : False\n    > cmo         : False\n    > posthoc_la  : False\n    > posthoc_la  : False\n    > posthoc_la  : False\n    > cuda        : False\n    > cuda        : False\n    > cuda        : False\n    > aug_type    : none\n    > aug_type    : none\n    > aug_type    : none\n    > sim_type    : none\n    > sim_type    : none\n    > sim_type    : none\n    > max_d       : 30\n    > max_d       : 30\n    > max_d       : 30\n    > num_test    : 10\n    > num_test    : 10\n    > num_test    : 10\n    > accept_rate : 0.6\n    > accept_rate : 0.6\n    > accept_rate : 0.6\n    > verbose     : True\n    > verbose     : True\n    > verbose     : True\n    > use_norm    : False\n    > use_norm    : False\n    > use_norm    : False\n    > out         : /kaggle/working/log3\n    > out         : /kaggle/working/log3\n    > out         : /kaggle/working/log3\n    > data_dir    : ~/dataset/\n    > data_dir    : ~/dataset/\n    > data_dir    : ~/dataset/\n    > workers     : 4\n    > workers     : 4\n    > workers     : 4\n    > seed        : None\n    > seed        : None\n    > seed        : None\n    > gpu         : 0\n    > gpu         : 0\n    > gpu         : 0\n    > dataset     : cifar100\n    > dataset     : cifar100\n    > dataset     : cifar100\n    > num_max     : 500\n    > num_max     : 500\n    > num_max     : 500\n    > imb_ratio   : 100\n    > imb_ratio   : 100\n    > imb_ratio   : 100\n    > loss_fn     : ce\n    > loss_fn     : ce\n    > loss_fn     : ce\n    > num_experts : 3\n    > num_experts : 3\n    > num_experts : 3\n    > ride_distill: False\n    > ride_distill: False\n    > ride_distill: False\n    > num_class   : 100\n    > num_class   : 100\n    > num_class   : 100\n","output_type":"stream"},{"name":"stdout","text":"==> Preparing imbalanced CIFAR-100\nFiles already downloaded and verified\nMagnitude set = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=torch.int32)\nOperation set = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=torch.int32)\nFiles already downloaded and verified\n#Train: 10847, #Test: 10000\n==> creating resnet32\n    Total params: 0.47M\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [1 | 200]\n---> Epoch: [1 | 200]\n---> Epoch: [1 | 200]\n    > [Train]\tLoss:\t4.1985\n    > [Train]\tLoss:\t4.1985\n    > [Train]\tLoss:\t4.1985\n    > [Test ]\tLoss:\t5.4472\tAcc:\t5.4600\n    > [Test ]\tLoss:\t5.4472\tAcc:\t5.4600\n    > [Test ]\tLoss:\t5.4472\tAcc:\t5.4600\n    > [Stats]\tMany:\t0.1560\tMedium:\t0.0000\tFew:\t0.0000\n    > [Stats]\tMany:\t0.1560\tMedium:\t0.0000\tFew:\t0.0000\n    > [Stats]\tMany:\t0.1560\tMedium:\t0.0000\tFew:\t0.0000\n    > [Best ]\tAcc:\t5.4600\tMany:\t15.6000\tMedium:\t0.0000\tFew:\t0.0000\n    > [Best ]\tAcc:\t5.4600\tMany:\t15.6000\tMedium:\t0.0000\tFew:\t0.0000\n    > [Best ]\tAcc:\t5.4600\tMany:\t15.6000\tMedium:\t0.0000\tFew:\t0.0000\n    > [Param]\tLR:\t0.02000000\n    > [Param]\tLR:\t0.02000000\n    > [Param]\tLR:\t0.02000000\n","output_type":"stream"},{"name":"stdout","text":"0.7206641586721775\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [2 | 200]\n---> Epoch: [2 | 200]\n---> Epoch: [2 | 200]\n    > [Train]\tLoss:\t3.6074\n    > [Train]\tLoss:\t3.6074\n    > [Train]\tLoss:\t3.6074\n    > [Test ]\tLoss:\t5.1811\tAcc:\t6.1200\n    > [Test ]\tLoss:\t5.1811\tAcc:\t6.1200\n    > [Test ]\tLoss:\t5.1811\tAcc:\t6.1200\n    > [Stats]\tMany:\t0.1731\tMedium:\t0.0017\tFew:\t0.0000\n    > [Stats]\tMany:\t0.1731\tMedium:\t0.0017\tFew:\t0.0000\n    > [Stats]\tMany:\t0.1731\tMedium:\t0.0017\tFew:\t0.0000\n    > [Best ]\tAcc:\t6.1200\tMany:\t17.3143\tMedium:\t0.1714\tFew:\t0.0000\n    > [Best ]\tAcc:\t6.1200\tMany:\t17.3143\tMedium:\t0.1714\tFew:\t0.0000\n    > [Best ]\tAcc:\t6.1200\tMany:\t17.3143\tMedium:\t0.1714\tFew:\t0.0000\n    > [Param]\tLR:\t0.04000000\n    > [Param]\tLR:\t0.04000000\n    > [Param]\tLR:\t0.04000000\n","output_type":"stream"},{"name":"stdout","text":"0.6908714917546606\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [3 | 200]\n---> Epoch: [3 | 200]\n---> Epoch: [3 | 200]\n    > [Train]\tLoss:\t3.3687\n    > [Train]\tLoss:\t3.3687\n    > [Train]\tLoss:\t3.3687\n    > [Test ]\tLoss:\t4.8979\tAcc:\t8.2700\n    > [Test ]\tLoss:\t4.8979\tAcc:\t8.2700\n    > [Test ]\tLoss:\t4.8979\tAcc:\t8.2700\n    > [Stats]\tMany:\t0.2189\tMedium:\t0.0174\tFew:\t0.0000\n    > [Stats]\tMany:\t0.2189\tMedium:\t0.0174\tFew:\t0.0000\n    > [Stats]\tMany:\t0.2189\tMedium:\t0.0174\tFew:\t0.0000\n    > [Best ]\tAcc:\t8.2700\tMany:\t21.8857\tMedium:\t1.7429\tFew:\t0.0000\n    > [Best ]\tAcc:\t8.2700\tMany:\t21.8857\tMedium:\t1.7429\tFew:\t0.0000\n    > [Best ]\tAcc:\t8.2700\tMany:\t21.8857\tMedium:\t1.7429\tFew:\t0.0000\n    > [Param]\tLR:\t0.06000000\n    > [Param]\tLR:\t0.06000000\n    > [Param]\tLR:\t0.06000000\n","output_type":"stream"},{"name":"stdout","text":"0.7639585779840647\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [4 | 200]\n---> Epoch: [4 | 200]\n---> Epoch: [4 | 200]\n    > [Train]\tLoss:\t3.1718\n    > [Train]\tLoss:\t3.1718\n    > [Train]\tLoss:\t3.1718\n    > [Test ]\tLoss:\t5.0153\tAcc:\t8.5700\n    > [Test ]\tLoss:\t5.0153\tAcc:\t8.5700\n    > [Test ]\tLoss:\t5.0153\tAcc:\t8.5700\n    > [Stats]\tMany:\t0.2194\tMedium:\t0.0254\tFew:\t0.0000\n    > [Stats]\tMany:\t0.2194\tMedium:\t0.0254\tFew:\t0.0000\n    > [Stats]\tMany:\t0.2194\tMedium:\t0.0254\tFew:\t0.0000\n    > [Best ]\tAcc:\t8.5700\tMany:\t21.9429\tMedium:\t2.5429\tFew:\t0.0000\n    > [Best ]\tAcc:\t8.5700\tMany:\t21.9429\tMedium:\t2.5429\tFew:\t0.0000\n    > [Best ]\tAcc:\t8.5700\tMany:\t21.9429\tMedium:\t2.5429\tFew:\t0.0000\n    > [Param]\tLR:\t0.08000000\n    > [Param]\tLR:\t0.08000000\n    > [Param]\tLR:\t0.08000000\n","output_type":"stream"},{"name":"stdout","text":"0.882950800236471\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [5 | 200]\n---> Epoch: [5 | 200]\n---> Epoch: [5 | 200]\n    > [Train]\tLoss:\t3.0202\n    > [Train]\tLoss:\t3.0202\n    > [Train]\tLoss:\t3.0202\n    > [Test ]\tLoss:\t4.5917\tAcc:\t10.5300\n    > [Test ]\tLoss:\t4.5917\tAcc:\t10.5300\n    > [Test ]\tLoss:\t4.5917\tAcc:\t10.5300\n    > [Stats]\tMany:\t0.2840\tMedium:\t0.0169\tFew:\t0.0000\n    > [Stats]\tMany:\t0.2840\tMedium:\t0.0169\tFew:\t0.0000\n    > [Stats]\tMany:\t0.2840\tMedium:\t0.0169\tFew:\t0.0000\n    > [Best ]\tAcc:\t10.5300\tMany:\t28.4000\tMedium:\t1.6857\tFew:\t0.0000\n    > [Best ]\tAcc:\t10.5300\tMany:\t28.4000\tMedium:\t1.6857\tFew:\t0.0000\n    > [Best ]\tAcc:\t10.5300\tMany:\t28.4000\tMedium:\t1.6857\tFew:\t0.0000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"1.102954229553409\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [6 | 200]\n---> Epoch: [6 | 200]\n---> Epoch: [6 | 200]\n    > [Train]\tLoss:\t2.8447\n    > [Train]\tLoss:\t2.8447\n    > [Train]\tLoss:\t2.8447\n    > [Test ]\tLoss:\t4.6828\tAcc:\t12.4500\n    > [Test ]\tLoss:\t4.6828\tAcc:\t12.4500\n    > [Test ]\tLoss:\t4.6828\tAcc:\t12.4500\n    > [Stats]\tMany:\t0.2960\tMedium:\t0.0597\tFew:\t0.0000\n    > [Stats]\tMany:\t0.2960\tMedium:\t0.0597\tFew:\t0.0000\n    > [Stats]\tMany:\t0.2960\tMedium:\t0.0597\tFew:\t0.0000\n    > [Best ]\tAcc:\t12.4500\tMany:\t29.6000\tMedium:\t5.9714\tFew:\t0.0000\n    > [Best ]\tAcc:\t12.4500\tMany:\t29.6000\tMedium:\t5.9714\tFew:\t0.0000\n    > [Best ]\tAcc:\t12.4500\tMany:\t29.6000\tMedium:\t5.9714\tFew:\t0.0000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"1.3442035474762943\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [7 | 200]\n---> Epoch: [7 | 200]\n---> Epoch: [7 | 200]\n    > [Train]\tLoss:\t2.6662\n    > [Train]\tLoss:\t2.6662\n    > [Train]\tLoss:\t2.6662\n    > [Test ]\tLoss:\t4.4646\tAcc:\t12.2500\n    > [Test ]\tLoss:\t4.4646\tAcc:\t12.2500\n    > [Test ]\tLoss:\t4.4646\tAcc:\t12.2500\n    > [Stats]\tMany:\t0.3146\tMedium:\t0.0354\tFew:\t0.0000\n    > [Stats]\tMany:\t0.3146\tMedium:\t0.0354\tFew:\t0.0000\n    > [Stats]\tMany:\t0.3146\tMedium:\t0.0354\tFew:\t0.0000\n    > [Best ]\tAcc:\t12.4500\tMany:\t29.6000\tMedium:\t5.9714\tFew:\t0.0000\n    > [Best ]\tAcc:\t12.4500\tMany:\t29.6000\tMedium:\t5.9714\tFew:\t0.0000\n    > [Best ]\tAcc:\t12.4500\tMany:\t29.6000\tMedium:\t5.9714\tFew:\t0.0000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"1.5875812493301715\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [8 | 200]\n---> Epoch: [8 | 200]\n---> Epoch: [8 | 200]\n    > [Train]\tLoss:\t2.5475\n    > [Train]\tLoss:\t2.5475\n    > [Train]\tLoss:\t2.5475\n    > [Test ]\tLoss:\t4.4271\tAcc:\t15.1800\n    > [Test ]\tLoss:\t4.4271\tAcc:\t15.1800\n    > [Test ]\tLoss:\t4.4271\tAcc:\t15.1800\n    > [Stats]\tMany:\t0.3837\tMedium:\t0.0497\tFew:\t0.0003\n    > [Stats]\tMany:\t0.3837\tMedium:\t0.0497\tFew:\t0.0003\n    > [Stats]\tMany:\t0.3837\tMedium:\t0.0497\tFew:\t0.0003\n    > [Best ]\tAcc:\t15.1800\tMany:\t38.3714\tMedium:\t4.9714\tFew:\t0.0333\n    > [Best ]\tAcc:\t15.1800\tMany:\t38.3714\tMedium:\t4.9714\tFew:\t0.0333\n    > [Best ]\tAcc:\t15.1800\tMany:\t38.3714\tMedium:\t4.9714\tFew:\t0.0333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"1.7908585590534924\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [9 | 200]\n---> Epoch: [9 | 200]\n---> Epoch: [9 | 200]\n    > [Train]\tLoss:\t2.4024\n    > [Train]\tLoss:\t2.4024\n    > [Train]\tLoss:\t2.4024\n    > [Test ]\tLoss:\t4.6761\tAcc:\t15.8500\n    > [Test ]\tLoss:\t4.6761\tAcc:\t15.8500\n    > [Test ]\tLoss:\t4.6761\tAcc:\t15.8500\n    > [Stats]\tMany:\t0.3640\tMedium:\t0.0871\tFew:\t0.0020\n    > [Stats]\tMany:\t0.3640\tMedium:\t0.0871\tFew:\t0.0020\n    > [Stats]\tMany:\t0.3640\tMedium:\t0.0871\tFew:\t0.0020\n    > [Best ]\tAcc:\t15.8500\tMany:\t36.4000\tMedium:\t8.7143\tFew:\t0.2000\n    > [Best ]\tAcc:\t15.8500\tMany:\t36.4000\tMedium:\t8.7143\tFew:\t0.2000\n    > [Best ]\tAcc:\t15.8500\tMany:\t36.4000\tMedium:\t8.7143\tFew:\t0.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.0174678675243842\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [10 | 200]\n---> Epoch: [10 | 200]\n---> Epoch: [10 | 200]\n    > [Train]\tLoss:\t2.2787\n    > [Train]\tLoss:\t2.2787\n    > [Train]\tLoss:\t2.2787\n    > [Test ]\tLoss:\t4.5519\tAcc:\t15.1500\n    > [Test ]\tLoss:\t4.5519\tAcc:\t15.1500\n    > [Test ]\tLoss:\t4.5519\tAcc:\t15.1500\n    > [Stats]\tMany:\t0.3589\tMedium:\t0.0737\tFew:\t0.0003\n    > [Stats]\tMany:\t0.3589\tMedium:\t0.0737\tFew:\t0.0003\n    > [Stats]\tMany:\t0.3589\tMedium:\t0.0737\tFew:\t0.0003\n    > [Best ]\tAcc:\t15.8500\tMany:\t36.4000\tMedium:\t8.7143\tFew:\t0.2000\n    > [Best ]\tAcc:\t15.8500\tMany:\t36.4000\tMedium:\t8.7143\tFew:\t0.2000\n    > [Best ]\tAcc:\t15.8500\tMany:\t36.4000\tMedium:\t8.7143\tFew:\t0.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.2054443577891045\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [11 | 200]\n---> Epoch: [11 | 200]\n---> Epoch: [11 | 200]\n    > [Train]\tLoss:\t2.1807\n    > [Train]\tLoss:\t2.1807\n    > [Train]\tLoss:\t2.1807\n    > [Test ]\tLoss:\t4.2996\tAcc:\t17.6500\n    > [Test ]\tLoss:\t4.2996\tAcc:\t17.6500\n    > [Test ]\tLoss:\t4.2996\tAcc:\t17.6500\n    > [Stats]\tMany:\t0.4057\tMedium:\t0.0937\tFew:\t0.0057\n    > [Stats]\tMany:\t0.4057\tMedium:\t0.0937\tFew:\t0.0057\n    > [Stats]\tMany:\t0.4057\tMedium:\t0.0937\tFew:\t0.0057\n    > [Best ]\tAcc:\t17.6500\tMany:\t40.5714\tMedium:\t9.3714\tFew:\t0.5667\n    > [Best ]\tAcc:\t17.6500\tMany:\t40.5714\tMedium:\t9.3714\tFew:\t0.5667\n    > [Best ]\tAcc:\t17.6500\tMany:\t40.5714\tMedium:\t9.3714\tFew:\t0.5667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.3803349688898376\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [12 | 200]\n---> Epoch: [12 | 200]\n---> Epoch: [12 | 200]\n    > [Train]\tLoss:\t2.0773\n    > [Train]\tLoss:\t2.0773\n    > [Train]\tLoss:\t2.0773\n    > [Test ]\tLoss:\t4.3496\tAcc:\t16.8500\n    > [Test ]\tLoss:\t4.3496\tAcc:\t16.8500\n    > [Test ]\tLoss:\t4.3496\tAcc:\t16.8500\n    > [Stats]\tMany:\t0.3886\tMedium:\t0.0909\tFew:\t0.0023\n    > [Stats]\tMany:\t0.3886\tMedium:\t0.0909\tFew:\t0.0023\n    > [Stats]\tMany:\t0.3886\tMedium:\t0.0909\tFew:\t0.0023\n    > [Best ]\tAcc:\t17.6500\tMany:\t40.5714\tMedium:\t9.3714\tFew:\t0.5667\n    > [Best ]\tAcc:\t17.6500\tMany:\t40.5714\tMedium:\t9.3714\tFew:\t0.5667\n    > [Best ]\tAcc:\t17.6500\tMany:\t40.5714\tMedium:\t9.3714\tFew:\t0.5667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.4914933012592524\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [13 | 200]\n---> Epoch: [13 | 200]\n---> Epoch: [13 | 200]\n    > [Train]\tLoss:\t1.9821\n    > [Train]\tLoss:\t1.9821\n    > [Train]\tLoss:\t1.9821\n    > [Test ]\tLoss:\t4.0622\tAcc:\t19.9300\n    > [Test ]\tLoss:\t4.0622\tAcc:\t19.9300\n    > [Test ]\tLoss:\t4.0622\tAcc:\t19.9300\n    > [Stats]\tMany:\t0.4477\tMedium:\t0.1171\tFew:\t0.0053\n    > [Stats]\tMany:\t0.4477\tMedium:\t0.1171\tFew:\t0.0053\n    > [Stats]\tMany:\t0.4477\tMedium:\t0.1171\tFew:\t0.0053\n    > [Best ]\tAcc:\t19.9300\tMany:\t44.7714\tMedium:\t11.7143\tFew:\t0.5333\n    > [Best ]\tAcc:\t19.9300\tMany:\t44.7714\tMedium:\t11.7143\tFew:\t0.5333\n    > [Best ]\tAcc:\t19.9300\tMany:\t44.7714\tMedium:\t11.7143\tFew:\t0.5333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.620005473768445\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [14 | 200]\n---> Epoch: [14 | 200]\n---> Epoch: [14 | 200]\n    > [Train]\tLoss:\t1.8742\n    > [Train]\tLoss:\t1.8742\n    > [Train]\tLoss:\t1.8742\n    > [Test ]\tLoss:\t4.3080\tAcc:\t19.4200\n    > [Test ]\tLoss:\t4.3080\tAcc:\t19.4200\n    > [Test ]\tLoss:\t4.3080\tAcc:\t19.4200\n    > [Stats]\tMany:\t0.4400\tMedium:\t0.1131\tFew:\t0.0020\n    > [Stats]\tMany:\t0.4400\tMedium:\t0.1131\tFew:\t0.0020\n    > [Stats]\tMany:\t0.4400\tMedium:\t0.1131\tFew:\t0.0020\n    > [Best ]\tAcc:\t19.9300\tMany:\t44.7714\tMedium:\t11.7143\tFew:\t0.5333\n    > [Best ]\tAcc:\t19.9300\tMany:\t44.7714\tMedium:\t11.7143\tFew:\t0.5333\n    > [Best ]\tAcc:\t19.9300\tMany:\t44.7714\tMedium:\t11.7143\tFew:\t0.5333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.763507438284746\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [15 | 200]\n---> Epoch: [15 | 200]\n---> Epoch: [15 | 200]\n    > [Train]\tLoss:\t1.8114\n    > [Train]\tLoss:\t1.8114\n    > [Train]\tLoss:\t1.8114\n    > [Test ]\tLoss:\t4.0993\tAcc:\t20.0500\n    > [Test ]\tLoss:\t4.0993\tAcc:\t20.0500\n    > [Test ]\tLoss:\t4.0993\tAcc:\t20.0500\n    > [Stats]\tMany:\t0.4149\tMedium:\t0.1569\tFew:\t0.0013\n    > [Stats]\tMany:\t0.4149\tMedium:\t0.1569\tFew:\t0.0013\n    > [Stats]\tMany:\t0.4149\tMedium:\t0.1569\tFew:\t0.0013\n    > [Best ]\tAcc:\t20.0500\tMany:\t41.4857\tMedium:\t15.6857\tFew:\t0.1333\n    > [Best ]\tAcc:\t20.0500\tMany:\t41.4857\tMedium:\t15.6857\tFew:\t0.1333\n    > [Best ]\tAcc:\t20.0500\tMany:\t41.4857\tMedium:\t15.6857\tFew:\t0.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.7921078828860595\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [16 | 200]\n---> Epoch: [16 | 200]\n---> Epoch: [16 | 200]\n    > [Train]\tLoss:\t1.7349\n    > [Train]\tLoss:\t1.7349\n    > [Train]\tLoss:\t1.7349\n    > [Test ]\tLoss:\t4.2175\tAcc:\t20.2300\n    > [Test ]\tLoss:\t4.2175\tAcc:\t20.2300\n    > [Test ]\tLoss:\t4.2175\tAcc:\t20.2300\n    > [Stats]\tMany:\t0.4494\tMedium:\t0.1254\tFew:\t0.0037\n    > [Stats]\tMany:\t0.4494\tMedium:\t0.1254\tFew:\t0.0037\n    > [Stats]\tMany:\t0.4494\tMedium:\t0.1254\tFew:\t0.0037\n    > [Best ]\tAcc:\t20.2300\tMany:\t44.9429\tMedium:\t12.5429\tFew:\t0.3667\n    > [Best ]\tAcc:\t20.2300\tMany:\t44.9429\tMedium:\t12.5429\tFew:\t0.3667\n    > [Best ]\tAcc:\t20.2300\tMany:\t44.9429\tMedium:\t12.5429\tFew:\t0.3667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.9137387889045048\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [17 | 200]\n---> Epoch: [17 | 200]\n---> Epoch: [17 | 200]\n    > [Train]\tLoss:\t1.6735\n    > [Train]\tLoss:\t1.6735\n    > [Train]\tLoss:\t1.6735\n    > [Test ]\tLoss:\t4.3207\tAcc:\t18.9100\n    > [Test ]\tLoss:\t4.3207\tAcc:\t18.9100\n    > [Test ]\tLoss:\t4.3207\tAcc:\t18.9100\n    > [Stats]\tMany:\t0.3843\tMedium:\t0.1500\tFew:\t0.0070\n    > [Stats]\tMany:\t0.3843\tMedium:\t0.1500\tFew:\t0.0070\n    > [Stats]\tMany:\t0.3843\tMedium:\t0.1500\tFew:\t0.0070\n    > [Best ]\tAcc:\t20.2300\tMany:\t44.9429\tMedium:\t12.5429\tFew:\t0.3667\n    > [Best ]\tAcc:\t20.2300\tMany:\t44.9429\tMedium:\t12.5429\tFew:\t0.3667\n    > [Best ]\tAcc:\t20.2300\tMany:\t44.9429\tMedium:\t12.5429\tFew:\t0.3667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.9898086741237373\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [18 | 200]\n---> Epoch: [18 | 200]\n---> Epoch: [18 | 200]\n    > [Train]\tLoss:\t1.6241\n    > [Train]\tLoss:\t1.6241\n    > [Train]\tLoss:\t1.6241\n    > [Test ]\tLoss:\t4.0849\tAcc:\t21.8800\n    > [Test ]\tLoss:\t4.0849\tAcc:\t21.8800\n    > [Test ]\tLoss:\t4.0849\tAcc:\t21.8800\n    > [Stats]\tMany:\t0.4674\tMedium:\t0.1546\tFew:\t0.0037\n    > [Stats]\tMany:\t0.4674\tMedium:\t0.1546\tFew:\t0.0037\n    > [Stats]\tMany:\t0.4674\tMedium:\t0.1546\tFew:\t0.0037\n    > [Best ]\tAcc:\t21.8800\tMany:\t46.7429\tMedium:\t15.4571\tFew:\t0.3667\n    > [Best ]\tAcc:\t21.8800\tMany:\t46.7429\tMedium:\t15.4571\tFew:\t0.3667\n    > [Best ]\tAcc:\t21.8800\tMany:\t46.7429\tMedium:\t15.4571\tFew:\t0.3667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.0668657325821815\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [19 | 200]\n---> Epoch: [19 | 200]\n---> Epoch: [19 | 200]\n    > [Train]\tLoss:\t1.5754\n    > [Train]\tLoss:\t1.5754\n    > [Train]\tLoss:\t1.5754\n    > [Test ]\tLoss:\t3.8471\tAcc:\t23.5800\n    > [Test ]\tLoss:\t3.8471\tAcc:\t23.5800\n    > [Test ]\tLoss:\t3.8471\tAcc:\t23.5800\n    > [Stats]\tMany:\t0.4814\tMedium:\t0.1869\tFew:\t0.0063\n    > [Stats]\tMany:\t0.4814\tMedium:\t0.1869\tFew:\t0.0063\n    > [Stats]\tMany:\t0.4814\tMedium:\t0.1869\tFew:\t0.0063\n    > [Best ]\tAcc:\t23.5800\tMany:\t48.1429\tMedium:\t18.6857\tFew:\t0.6333\n    > [Best ]\tAcc:\t23.5800\tMany:\t48.1429\tMedium:\t18.6857\tFew:\t0.6333\n    > [Best ]\tAcc:\t23.5800\tMany:\t48.1429\tMedium:\t18.6857\tFew:\t0.6333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.1264808905804085\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [20 | 200]\n---> Epoch: [20 | 200]\n---> Epoch: [20 | 200]\n    > [Train]\tLoss:\t1.5084\n    > [Train]\tLoss:\t1.5084\n    > [Train]\tLoss:\t1.5084\n    > [Test ]\tLoss:\t3.7721\tAcc:\t24.0800\n    > [Test ]\tLoss:\t3.7721\tAcc:\t24.0800\n    > [Test ]\tLoss:\t3.7721\tAcc:\t24.0800\n    > [Stats]\tMany:\t0.4931\tMedium:\t0.1829\tFew:\t0.0140\n    > [Stats]\tMany:\t0.4931\tMedium:\t0.1829\tFew:\t0.0140\n    > [Stats]\tMany:\t0.4931\tMedium:\t0.1829\tFew:\t0.0140\n    > [Best ]\tAcc:\t24.0800\tMany:\t49.3143\tMedium:\t18.2857\tFew:\t1.4000\n    > [Best ]\tAcc:\t24.0800\tMany:\t49.3143\tMedium:\t18.2857\tFew:\t1.4000\n    > [Best ]\tAcc:\t24.0800\tMany:\t49.3143\tMedium:\t18.2857\tFew:\t1.4000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.201317766251162\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [21 | 200]\n---> Epoch: [21 | 200]\n---> Epoch: [21 | 200]\n    > [Train]\tLoss:\t1.4905\n    > [Train]\tLoss:\t1.4905\n    > [Train]\tLoss:\t1.4905\n    > [Test ]\tLoss:\t3.6315\tAcc:\t25.8000\n    > [Test ]\tLoss:\t3.6315\tAcc:\t25.8000\n    > [Test ]\tLoss:\t3.6315\tAcc:\t25.8000\n    > [Stats]\tMany:\t0.5211\tMedium:\t0.2080\tFew:\t0.0093\n    > [Stats]\tMany:\t0.5211\tMedium:\t0.2080\tFew:\t0.0093\n    > [Stats]\tMany:\t0.5211\tMedium:\t0.2080\tFew:\t0.0093\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.260583052978525\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [22 | 200]\n---> Epoch: [22 | 200]\n---> Epoch: [22 | 200]\n    > [Train]\tLoss:\t1.4227\n    > [Train]\tLoss:\t1.4227\n    > [Train]\tLoss:\t1.4227\n    > [Test ]\tLoss:\t3.9834\tAcc:\t24.4400\n    > [Test ]\tLoss:\t3.9834\tAcc:\t24.4400\n    > [Test ]\tLoss:\t3.9834\tAcc:\t24.4400\n    > [Stats]\tMany:\t0.5117\tMedium:\t0.1769\tFew:\t0.0113\n    > [Stats]\tMany:\t0.5117\tMedium:\t0.1769\tFew:\t0.0113\n    > [Stats]\tMany:\t0.5117\tMedium:\t0.1769\tFew:\t0.0113\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.305204783667277\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [23 | 200]\n---> Epoch: [23 | 200]\n---> Epoch: [23 | 200]\n    > [Train]\tLoss:\t1.3994\n    > [Train]\tLoss:\t1.3994\n    > [Train]\tLoss:\t1.3994\n    > [Test ]\tLoss:\t3.8623\tAcc:\t25.0100\n    > [Test ]\tLoss:\t3.8623\tAcc:\t25.0100\n    > [Test ]\tLoss:\t3.8623\tAcc:\t25.0100\n    > [Stats]\tMany:\t0.5060\tMedium:\t0.1969\tFew:\t0.0137\n    > [Stats]\tMany:\t0.5060\tMedium:\t0.1969\tFew:\t0.0137\n    > [Stats]\tMany:\t0.5060\tMedium:\t0.1969\tFew:\t0.0137\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.3624607537631923\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [24 | 200]\n---> Epoch: [24 | 200]\n---> Epoch: [24 | 200]\n    > [Train]\tLoss:\t1.3447\n    > [Train]\tLoss:\t1.3447\n    > [Train]\tLoss:\t1.3447\n    > [Test ]\tLoss:\t3.8403\tAcc:\t25.5400\n    > [Test ]\tLoss:\t3.8403\tAcc:\t25.5400\n    > [Test ]\tLoss:\t3.8403\tAcc:\t25.5400\n    > [Stats]\tMany:\t0.5106\tMedium:\t0.2106\tFew:\t0.0100\n    > [Stats]\tMany:\t0.5106\tMedium:\t0.2106\tFew:\t0.0100\n    > [Stats]\tMany:\t0.5106\tMedium:\t0.2106\tFew:\t0.0100\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.3763333957226576\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [25 | 200]\n---> Epoch: [25 | 200]\n---> Epoch: [25 | 200]\n    > [Train]\tLoss:\t1.3268\n    > [Train]\tLoss:\t1.3268\n    > [Train]\tLoss:\t1.3268\n    > [Test ]\tLoss:\t4.1568\tAcc:\t23.7200\n    > [Test ]\tLoss:\t4.1568\tAcc:\t23.7200\n    > [Test ]\tLoss:\t4.1568\tAcc:\t23.7200\n    > [Stats]\tMany:\t0.4669\tMedium:\t0.2009\tFew:\t0.0117\n    > [Stats]\tMany:\t0.4669\tMedium:\t0.2009\tFew:\t0.0117\n    > [Stats]\tMany:\t0.4669\tMedium:\t0.2009\tFew:\t0.0117\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.423104870601601\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [26 | 200]\n---> Epoch: [26 | 200]\n---> Epoch: [26 | 200]\n    > [Train]\tLoss:\t1.2888\n    > [Train]\tLoss:\t1.2888\n    > [Train]\tLoss:\t1.2888\n    > [Test ]\tLoss:\t3.8502\tAcc:\t24.7700\n    > [Test ]\tLoss:\t3.8502\tAcc:\t24.7700\n    > [Test ]\tLoss:\t3.8502\tAcc:\t24.7700\n    > [Stats]\tMany:\t0.5097\tMedium:\t0.1803\tFew:\t0.0207\n    > [Stats]\tMany:\t0.5097\tMedium:\t0.1803\tFew:\t0.0207\n    > [Stats]\tMany:\t0.5097\tMedium:\t0.1803\tFew:\t0.0207\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.4391188735864624\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [27 | 200]\n---> Epoch: [27 | 200]\n---> Epoch: [27 | 200]\n    > [Train]\tLoss:\t1.2487\n    > [Train]\tLoss:\t1.2487\n    > [Train]\tLoss:\t1.2487\n    > [Test ]\tLoss:\t4.0260\tAcc:\t25.6200\n    > [Test ]\tLoss:\t4.0260\tAcc:\t25.6200\n    > [Test ]\tLoss:\t4.0260\tAcc:\t25.6200\n    > [Stats]\tMany:\t0.5137\tMedium:\t0.1906\tFew:\t0.0323\n    > [Stats]\tMany:\t0.5137\tMedium:\t0.1906\tFew:\t0.0323\n    > [Stats]\tMany:\t0.5137\tMedium:\t0.1906\tFew:\t0.0323\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Best ]\tAcc:\t25.8000\tMany:\t52.1143\tMedium:\t20.8000\tFew:\t0.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.4605108713528514\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [28 | 200]\n---> Epoch: [28 | 200]\n---> Epoch: [28 | 200]\n    > [Train]\tLoss:\t1.2238\n    > [Train]\tLoss:\t1.2238\n    > [Train]\tLoss:\t1.2238\n    > [Test ]\tLoss:\t3.8175\tAcc:\t26.0900\n    > [Test ]\tLoss:\t3.8175\tAcc:\t26.0900\n    > [Test ]\tLoss:\t3.8175\tAcc:\t26.0900\n    > [Stats]\tMany:\t0.5291\tMedium:\t0.1983\tFew:\t0.0210\n    > [Stats]\tMany:\t0.5291\tMedium:\t0.1983\tFew:\t0.0210\n    > [Stats]\tMany:\t0.5291\tMedium:\t0.1983\tFew:\t0.0210\n    > [Best ]\tAcc:\t26.0900\tMany:\t52.9143\tMedium:\t19.8286\tFew:\t2.1000\n    > [Best ]\tAcc:\t26.0900\tMany:\t52.9143\tMedium:\t19.8286\tFew:\t2.1000\n    > [Best ]\tAcc:\t26.0900\tMany:\t52.9143\tMedium:\t19.8286\tFew:\t2.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.4895289884008545\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [29 | 200]\n---> Epoch: [29 | 200]\n---> Epoch: [29 | 200]\n    > [Train]\tLoss:\t1.1814\n    > [Train]\tLoss:\t1.1814\n    > [Train]\tLoss:\t1.1814\n    > [Test ]\tLoss:\t3.7378\tAcc:\t26.9800\n    > [Test ]\tLoss:\t3.7378\tAcc:\t26.9800\n    > [Test ]\tLoss:\t3.7378\tAcc:\t26.9800\n    > [Stats]\tMany:\t0.5280\tMedium:\t0.2274\tFew:\t0.0180\n    > [Stats]\tMany:\t0.5280\tMedium:\t0.2274\tFew:\t0.0180\n    > [Stats]\tMany:\t0.5280\tMedium:\t0.2274\tFew:\t0.0180\n    > [Best ]\tAcc:\t26.9800\tMany:\t52.8000\tMedium:\t22.7429\tFew:\t1.8000\n    > [Best ]\tAcc:\t26.9800\tMany:\t52.8000\tMedium:\t22.7429\tFew:\t1.8000\n    > [Best ]\tAcc:\t26.9800\tMany:\t52.8000\tMedium:\t22.7429\tFew:\t1.8000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.5311426236772023\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [30 | 200]\n---> Epoch: [30 | 200]\n---> Epoch: [30 | 200]\n    > [Train]\tLoss:\t1.1713\n    > [Train]\tLoss:\t1.1713\n    > [Train]\tLoss:\t1.1713\n    > [Test ]\tLoss:\t3.7490\tAcc:\t26.5600\n    > [Test ]\tLoss:\t3.7490\tAcc:\t26.5600\n    > [Test ]\tLoss:\t3.7490\tAcc:\t26.5600\n    > [Stats]\tMany:\t0.5289\tMedium:\t0.2051\tFew:\t0.0290\n    > [Stats]\tMany:\t0.5289\tMedium:\t0.2051\tFew:\t0.0290\n    > [Stats]\tMany:\t0.5289\tMedium:\t0.2051\tFew:\t0.0290\n    > [Best ]\tAcc:\t26.9800\tMany:\t52.8000\tMedium:\t22.7429\tFew:\t1.8000\n    > [Best ]\tAcc:\t26.9800\tMany:\t52.8000\tMedium:\t22.7429\tFew:\t1.8000\n    > [Best ]\tAcc:\t26.9800\tMany:\t52.8000\tMedium:\t22.7429\tFew:\t1.8000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.5509998947610715\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [31 | 200]\n---> Epoch: [31 | 200]\n---> Epoch: [31 | 200]\n    > [Train]\tLoss:\t1.1157\n    > [Train]\tLoss:\t1.1157\n    > [Train]\tLoss:\t1.1157\n    > [Test ]\tLoss:\t3.5545\tAcc:\t28.6800\n    > [Test ]\tLoss:\t3.5545\tAcc:\t28.6800\n    > [Test ]\tLoss:\t3.5545\tAcc:\t28.6800\n    > [Stats]\tMany:\t0.5700\tMedium:\t0.2274\tFew:\t0.0257\n    > [Stats]\tMany:\t0.5700\tMedium:\t0.2274\tFew:\t0.0257\n    > [Stats]\tMany:\t0.5700\tMedium:\t0.2274\tFew:\t0.0257\n    > [Best ]\tAcc:\t28.6800\tMany:\t57.0000\tMedium:\t22.7429\tFew:\t2.5667\n    > [Best ]\tAcc:\t28.6800\tMany:\t57.0000\tMedium:\t22.7429\tFew:\t2.5667\n    > [Best ]\tAcc:\t28.6800\tMany:\t57.0000\tMedium:\t22.7429\tFew:\t2.5667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.5672974169616363\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [32 | 200]\n---> Epoch: [32 | 200]\n---> Epoch: [32 | 200]\n    > [Train]\tLoss:\t1.1249\n    > [Train]\tLoss:\t1.1249\n    > [Train]\tLoss:\t1.1249\n    > [Test ]\tLoss:\t4.0909\tAcc:\t26.0900\n    > [Test ]\tLoss:\t4.0909\tAcc:\t26.0900\n    > [Test ]\tLoss:\t4.0909\tAcc:\t26.0900\n    > [Stats]\tMany:\t0.5003\tMedium:\t0.2131\tFew:\t0.0373\n    > [Stats]\tMany:\t0.5003\tMedium:\t0.2131\tFew:\t0.0373\n    > [Stats]\tMany:\t0.5003\tMedium:\t0.2131\tFew:\t0.0373\n    > [Best ]\tAcc:\t28.6800\tMany:\t57.0000\tMedium:\t22.7429\tFew:\t2.5667\n    > [Best ]\tAcc:\t28.6800\tMany:\t57.0000\tMedium:\t22.7429\tFew:\t2.5667\n    > [Best ]\tAcc:\t28.6800\tMany:\t57.0000\tMedium:\t22.7429\tFew:\t2.5667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.565970712896374\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [33 | 200]\n---> Epoch: [33 | 200]\n---> Epoch: [33 | 200]\n    > [Train]\tLoss:\t1.0867\n    > [Train]\tLoss:\t1.0867\n    > [Train]\tLoss:\t1.0867\n    > [Test ]\tLoss:\t3.8063\tAcc:\t29.0600\n    > [Test ]\tLoss:\t3.8063\tAcc:\t29.0600\n    > [Test ]\tLoss:\t3.8063\tAcc:\t29.0600\n    > [Stats]\tMany:\t0.5320\tMedium:\t0.2623\tFew:\t0.0420\n    > [Stats]\tMany:\t0.5320\tMedium:\t0.2623\tFew:\t0.0420\n    > [Stats]\tMany:\t0.5320\tMedium:\t0.2623\tFew:\t0.0420\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.5987983362390956\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [34 | 200]\n---> Epoch: [34 | 200]\n---> Epoch: [34 | 200]\n    > [Train]\tLoss:\t1.0162\n    > [Train]\tLoss:\t1.0162\n    > [Train]\tLoss:\t1.0162\n    > [Test ]\tLoss:\t4.6826\tAcc:\t24.0400\n    > [Test ]\tLoss:\t4.6826\tAcc:\t24.0400\n    > [Test ]\tLoss:\t4.6826\tAcc:\t24.0400\n    > [Stats]\tMany:\t0.5046\tMedium:\t0.1697\tFew:\t0.0147\n    > [Stats]\tMany:\t0.5046\tMedium:\t0.1697\tFew:\t0.0147\n    > [Stats]\tMany:\t0.5046\tMedium:\t0.1697\tFew:\t0.0147\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.6298011905503027\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [35 | 200]\n---> Epoch: [35 | 200]\n---> Epoch: [35 | 200]\n    > [Train]\tLoss:\t1.0203\n    > [Train]\tLoss:\t1.0203\n    > [Train]\tLoss:\t1.0203\n    > [Test ]\tLoss:\t3.4720\tAcc:\t28.0100\n    > [Test ]\tLoss:\t3.4720\tAcc:\t28.0100\n    > [Test ]\tLoss:\t3.4720\tAcc:\t28.0100\n    > [Stats]\tMany:\t0.5337\tMedium:\t0.2426\tFew:\t0.0280\n    > [Stats]\tMany:\t0.5337\tMedium:\t0.2426\tFew:\t0.0280\n    > [Stats]\tMany:\t0.5337\tMedium:\t0.2426\tFew:\t0.0280\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.6442152732716186\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [36 | 200]\n---> Epoch: [36 | 200]\n---> Epoch: [36 | 200]\n    > [Train]\tLoss:\t0.9739\n    > [Train]\tLoss:\t0.9739\n    > [Train]\tLoss:\t0.9739\n    > [Test ]\tLoss:\t4.1050\tAcc:\t26.3800\n    > [Test ]\tLoss:\t4.1050\tAcc:\t26.3800\n    > [Test ]\tLoss:\t4.1050\tAcc:\t26.3800\n    > [Stats]\tMany:\t0.4940\tMedium:\t0.2423\tFew:\t0.0203\n    > [Stats]\tMany:\t0.4940\tMedium:\t0.2423\tFew:\t0.0203\n    > [Stats]\tMany:\t0.4940\tMedium:\t0.2423\tFew:\t0.0203\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.6412157191477506\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [37 | 200]\n---> Epoch: [37 | 200]\n---> Epoch: [37 | 200]\n    > [Train]\tLoss:\t0.9967\n    > [Train]\tLoss:\t0.9967\n    > [Train]\tLoss:\t0.9967\n    > [Test ]\tLoss:\t3.7419\tAcc:\t26.5700\n    > [Test ]\tLoss:\t3.7419\tAcc:\t26.5700\n    > [Test ]\tLoss:\t3.7419\tAcc:\t26.5700\n    > [Stats]\tMany:\t0.5334\tMedium:\t0.2049\tFew:\t0.0243\n    > [Stats]\tMany:\t0.5334\tMedium:\t0.2049\tFew:\t0.0243\n    > [Stats]\tMany:\t0.5334\tMedium:\t0.2049\tFew:\t0.0243\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.670726285435755\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [38 | 200]\n---> Epoch: [38 | 200]\n---> Epoch: [38 | 200]\n    > [Train]\tLoss:\t0.9508\n    > [Train]\tLoss:\t0.9508\n    > [Train]\tLoss:\t0.9508\n    > [Test ]\tLoss:\t4.5977\tAcc:\t25.1300\n    > [Test ]\tLoss:\t4.5977\tAcc:\t25.1300\n    > [Test ]\tLoss:\t4.5977\tAcc:\t25.1300\n    > [Stats]\tMany:\t0.4837\tMedium:\t0.2160\tFew:\t0.0213\n    > [Stats]\tMany:\t0.4837\tMedium:\t0.2160\tFew:\t0.0213\n    > [Stats]\tMany:\t0.4837\tMedium:\t0.2160\tFew:\t0.0213\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Best ]\tAcc:\t29.0600\tMany:\t53.2000\tMedium:\t26.2286\tFew:\t4.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7020446009863055\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [39 | 200]\n---> Epoch: [39 | 200]\n---> Epoch: [39 | 200]\n    > [Train]\tLoss:\t0.9540\n    > [Train]\tLoss:\t0.9540\n    > [Train]\tLoss:\t0.9540\n    > [Test ]\tLoss:\t3.7842\tAcc:\t29.4200\n    > [Test ]\tLoss:\t3.7842\tAcc:\t29.4200\n    > [Test ]\tLoss:\t3.7842\tAcc:\t29.4200\n    > [Stats]\tMany:\t0.5391\tMedium:\t0.2786\tFew:\t0.0267\n    > [Stats]\tMany:\t0.5391\tMedium:\t0.2786\tFew:\t0.0267\n    > [Stats]\tMany:\t0.5391\tMedium:\t0.2786\tFew:\t0.0267\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.68446535338106\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [40 | 200]\n---> Epoch: [40 | 200]\n---> Epoch: [40 | 200]\n    > [Train]\tLoss:\t0.9163\n    > [Train]\tLoss:\t0.9163\n    > [Train]\tLoss:\t0.9163\n    > [Test ]\tLoss:\t4.2700\tAcc:\t25.8800\n    > [Test ]\tLoss:\t4.2700\tAcc:\t25.8800\n    > [Test ]\tLoss:\t4.2700\tAcc:\t25.8800\n    > [Stats]\tMany:\t0.5011\tMedium:\t0.2103\tFew:\t0.0327\n    > [Stats]\tMany:\t0.5011\tMedium:\t0.2103\tFew:\t0.0327\n    > [Stats]\tMany:\t0.5011\tMedium:\t0.2103\tFew:\t0.0327\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7135086735208795\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [41 | 200]\n---> Epoch: [41 | 200]\n---> Epoch: [41 | 200]\n    > [Train]\tLoss:\t0.8733\n    > [Train]\tLoss:\t0.8733\n    > [Train]\tLoss:\t0.8733\n    > [Test ]\tLoss:\t4.3985\tAcc:\t28.1100\n    > [Test ]\tLoss:\t4.3985\tAcc:\t28.1100\n    > [Test ]\tLoss:\t4.3985\tAcc:\t28.1100\n    > [Stats]\tMany:\t0.5191\tMedium:\t0.2606\tFew:\t0.0273\n    > [Stats]\tMany:\t0.5191\tMedium:\t0.2606\tFew:\t0.0273\n    > [Stats]\tMany:\t0.5191\tMedium:\t0.2606\tFew:\t0.0273\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.700135248433819\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [42 | 200]\n---> Epoch: [42 | 200]\n---> Epoch: [42 | 200]\n    > [Train]\tLoss:\t0.8755\n    > [Train]\tLoss:\t0.8755\n    > [Train]\tLoss:\t0.8755\n    > [Test ]\tLoss:\t3.7910\tAcc:\t29.1400\n    > [Test ]\tLoss:\t3.7910\tAcc:\t29.1400\n    > [Test ]\tLoss:\t3.7910\tAcc:\t29.1400\n    > [Stats]\tMany:\t0.5337\tMedium:\t0.2657\tFew:\t0.0387\n    > [Stats]\tMany:\t0.5337\tMedium:\t0.2657\tFew:\t0.0387\n    > [Stats]\tMany:\t0.5337\tMedium:\t0.2657\tFew:\t0.0387\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.720932482000155\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [43 | 200]\n---> Epoch: [43 | 200]\n---> Epoch: [43 | 200]\n    > [Train]\tLoss:\t0.8641\n    > [Train]\tLoss:\t0.8641\n    > [Train]\tLoss:\t0.8641\n    > [Test ]\tLoss:\t3.9799\tAcc:\t28.8100\n    > [Test ]\tLoss:\t3.9799\tAcc:\t28.8100\n    > [Test ]\tLoss:\t3.9799\tAcc:\t28.8100\n    > [Stats]\tMany:\t0.5486\tMedium:\t0.2434\tFew:\t0.0363\n    > [Stats]\tMany:\t0.5486\tMedium:\t0.2434\tFew:\t0.0363\n    > [Stats]\tMany:\t0.5486\tMedium:\t0.2434\tFew:\t0.0363\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.716955812553133\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [44 | 200]\n---> Epoch: [44 | 200]\n---> Epoch: [44 | 200]\n    > [Train]\tLoss:\t0.8376\n    > [Train]\tLoss:\t0.8376\n    > [Train]\tLoss:\t0.8376\n    > [Test ]\tLoss:\t3.7294\tAcc:\t28.7700\n    > [Test ]\tLoss:\t3.7294\tAcc:\t28.7700\n    > [Test ]\tLoss:\t3.7294\tAcc:\t28.7700\n    > [Stats]\tMany:\t0.5337\tMedium:\t0.2586\tFew:\t0.0347\n    > [Stats]\tMany:\t0.5337\tMedium:\t0.2586\tFew:\t0.0347\n    > [Stats]\tMany:\t0.5337\tMedium:\t0.2586\tFew:\t0.0347\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7129997679340256\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [45 | 200]\n---> Epoch: [45 | 200]\n---> Epoch: [45 | 200]\n    > [Train]\tLoss:\t0.8339\n    > [Train]\tLoss:\t0.8339\n    > [Train]\tLoss:\t0.8339\n    > [Test ]\tLoss:\t4.1233\tAcc:\t28.1900\n    > [Test ]\tLoss:\t4.1233\tAcc:\t28.1900\n    > [Test ]\tLoss:\t4.1233\tAcc:\t28.1900\n    > [Stats]\tMany:\t0.5403\tMedium:\t0.2443\tFew:\t0.0243\n    > [Stats]\tMany:\t0.5403\tMedium:\t0.2443\tFew:\t0.0243\n    > [Stats]\tMany:\t0.5403\tMedium:\t0.2443\tFew:\t0.0243\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Best ]\tAcc:\t29.4200\tMany:\t53.9143\tMedium:\t27.8571\tFew:\t2.6667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.726978649649172\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [46 | 200]\n---> Epoch: [46 | 200]\n---> Epoch: [46 | 200]\n    > [Train]\tLoss:\t0.7868\n    > [Train]\tLoss:\t0.7868\n    > [Train]\tLoss:\t0.7868\n    > [Test ]\tLoss:\t3.7649\tAcc:\t30.2100\n    > [Test ]\tLoss:\t3.7649\tAcc:\t30.2100\n    > [Test ]\tLoss:\t3.7649\tAcc:\t30.2100\n    > [Stats]\tMany:\t0.5757\tMedium:\t0.2571\tFew:\t0.0353\n    > [Stats]\tMany:\t0.5757\tMedium:\t0.2571\tFew:\t0.0353\n    > [Stats]\tMany:\t0.5757\tMedium:\t0.2571\tFew:\t0.0353\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7611647360474576\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [47 | 200]\n---> Epoch: [47 | 200]\n---> Epoch: [47 | 200]\n    > [Train]\tLoss:\t0.8024\n    > [Train]\tLoss:\t0.8024\n    > [Train]\tLoss:\t0.8024\n    > [Test ]\tLoss:\t4.0390\tAcc:\t27.0700\n    > [Test ]\tLoss:\t4.0390\tAcc:\t27.0700\n    > [Test ]\tLoss:\t4.0390\tAcc:\t27.0700\n    > [Stats]\tMany:\t0.5194\tMedium:\t0.2317\tFew:\t0.0260\n    > [Stats]\tMany:\t0.5194\tMedium:\t0.2317\tFew:\t0.0260\n    > [Stats]\tMany:\t0.5194\tMedium:\t0.2317\tFew:\t0.0260\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7835408265048165\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [48 | 200]\n---> Epoch: [48 | 200]\n---> Epoch: [48 | 200]\n    > [Train]\tLoss:\t0.7895\n    > [Train]\tLoss:\t0.7895\n    > [Train]\tLoss:\t0.7895\n    > [Test ]\tLoss:\t4.6952\tAcc:\t27.3400\n    > [Test ]\tLoss:\t4.6952\tAcc:\t27.3400\n    > [Test ]\tLoss:\t4.6952\tAcc:\t27.3400\n    > [Stats]\tMany:\t0.5469\tMedium:\t0.2009\tFew:\t0.0390\n    > [Stats]\tMany:\t0.5469\tMedium:\t0.2009\tFew:\t0.0390\n    > [Stats]\tMany:\t0.5469\tMedium:\t0.2009\tFew:\t0.0390\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.802506372549201\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [49 | 200]\n---> Epoch: [49 | 200]\n---> Epoch: [49 | 200]\n    > [Train]\tLoss:\t0.7390\n    > [Train]\tLoss:\t0.7390\n    > [Train]\tLoss:\t0.7390\n    > [Test ]\tLoss:\t4.2242\tAcc:\t29.1300\n    > [Test ]\tLoss:\t4.2242\tAcc:\t29.1300\n    > [Test ]\tLoss:\t4.2242\tAcc:\t29.1300\n    > [Stats]\tMany:\t0.5743\tMedium:\t0.2331\tFew:\t0.0290\n    > [Stats]\tMany:\t0.5743\tMedium:\t0.2331\tFew:\t0.0290\n    > [Stats]\tMany:\t0.5743\tMedium:\t0.2331\tFew:\t0.0290\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.815011628574721\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [50 | 200]\n---> Epoch: [50 | 200]\n---> Epoch: [50 | 200]\n    > [Train]\tLoss:\t0.7312\n    > [Train]\tLoss:\t0.7312\n    > [Train]\tLoss:\t0.7312\n    > [Test ]\tLoss:\t4.1077\tAcc:\t27.6000\n    > [Test ]\tLoss:\t4.1077\tAcc:\t27.6000\n    > [Test ]\tLoss:\t4.1077\tAcc:\t27.6000\n    > [Stats]\tMany:\t0.5417\tMedium:\t0.2123\tFew:\t0.0403\n    > [Stats]\tMany:\t0.5417\tMedium:\t0.2123\tFew:\t0.0403\n    > [Stats]\tMany:\t0.5417\tMedium:\t0.2123\tFew:\t0.0403\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.8081328768725715\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [51 | 200]\n---> Epoch: [51 | 200]\n---> Epoch: [51 | 200]\n    > [Train]\tLoss:\t0.7267\n    > [Train]\tLoss:\t0.7267\n    > [Train]\tLoss:\t0.7267\n    > [Test ]\tLoss:\t4.1067\tAcc:\t29.5500\n    > [Test ]\tLoss:\t4.1067\tAcc:\t29.5500\n    > [Test ]\tLoss:\t4.1067\tAcc:\t29.5500\n    > [Stats]\tMany:\t0.5757\tMedium:\t0.2363\tFew:\t0.0377\n    > [Stats]\tMany:\t0.5757\tMedium:\t0.2363\tFew:\t0.0377\n    > [Stats]\tMany:\t0.5757\tMedium:\t0.2363\tFew:\t0.0377\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.753635376786268\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [52 | 200]\n---> Epoch: [52 | 200]\n---> Epoch: [52 | 200]\n    > [Train]\tLoss:\t0.7106\n    > [Train]\tLoss:\t0.7106\n    > [Train]\tLoss:\t0.7106\n    > [Test ]\tLoss:\t4.1001\tAcc:\t28.8600\n    > [Test ]\tLoss:\t4.1001\tAcc:\t28.8600\n    > [Test ]\tLoss:\t4.1001\tAcc:\t28.8600\n    > [Stats]\tMany:\t0.5171\tMedium:\t0.2651\tFew:\t0.0493\n    > [Stats]\tMany:\t0.5171\tMedium:\t0.2651\tFew:\t0.0493\n    > [Stats]\tMany:\t0.5171\tMedium:\t0.2651\tFew:\t0.0493\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.766116192067389\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [53 | 200]\n---> Epoch: [53 | 200]\n---> Epoch: [53 | 200]\n    > [Train]\tLoss:\t0.7311\n    > [Train]\tLoss:\t0.7311\n    > [Train]\tLoss:\t0.7311\n    > [Test ]\tLoss:\t4.2436\tAcc:\t27.7000\n    > [Test ]\tLoss:\t4.2436\tAcc:\t27.7000\n    > [Test ]\tLoss:\t4.2436\tAcc:\t27.7000\n    > [Stats]\tMany:\t0.4914\tMedium:\t0.2703\tFew:\t0.0347\n    > [Stats]\tMany:\t0.4914\tMedium:\t0.2703\tFew:\t0.0347\n    > [Stats]\tMany:\t0.4914\tMedium:\t0.2703\tFew:\t0.0347\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Best ]\tAcc:\t30.2100\tMany:\t57.5714\tMedium:\t25.7143\tFew:\t3.5333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.766257821126788\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [54 | 200]\n---> Epoch: [54 | 200]\n---> Epoch: [54 | 200]\n    > [Train]\tLoss:\t0.7142\n    > [Train]\tLoss:\t0.7142\n    > [Train]\tLoss:\t0.7142\n    > [Test ]\tLoss:\t3.9654\tAcc:\t31.0900\n    > [Test ]\tLoss:\t3.9654\tAcc:\t31.0900\n    > [Test ]\tLoss:\t3.9654\tAcc:\t31.0900\n    > [Stats]\tMany:\t0.5477\tMedium:\t0.3034\tFew:\t0.0433\n    > [Stats]\tMany:\t0.5477\tMedium:\t0.3034\tFew:\t0.0433\n    > [Stats]\tMany:\t0.5477\tMedium:\t0.3034\tFew:\t0.0433\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.747176735682484\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [55 | 200]\n---> Epoch: [55 | 200]\n---> Epoch: [55 | 200]\n    > [Train]\tLoss:\t0.6780\n    > [Train]\tLoss:\t0.6780\n    > [Train]\tLoss:\t0.6780\n    > [Test ]\tLoss:\t4.1164\tAcc:\t28.8700\n    > [Test ]\tLoss:\t4.1164\tAcc:\t28.8700\n    > [Test ]\tLoss:\t4.1164\tAcc:\t28.8700\n    > [Stats]\tMany:\t0.5374\tMedium:\t0.2589\tFew:\t0.0333\n    > [Stats]\tMany:\t0.5374\tMedium:\t0.2589\tFew:\t0.0333\n    > [Stats]\tMany:\t0.5374\tMedium:\t0.2589\tFew:\t0.0333\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7396291847148895\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [56 | 200]\n---> Epoch: [56 | 200]\n---> Epoch: [56 | 200]\n    > [Train]\tLoss:\t0.6444\n    > [Train]\tLoss:\t0.6444\n    > [Train]\tLoss:\t0.6444\n    > [Test ]\tLoss:\t4.5059\tAcc:\t29.2100\n    > [Test ]\tLoss:\t4.5059\tAcc:\t29.2100\n    > [Test ]\tLoss:\t4.5059\tAcc:\t29.2100\n    > [Stats]\tMany:\t0.5466\tMedium:\t0.2646\tFew:\t0.0273\n    > [Stats]\tMany:\t0.5466\tMedium:\t0.2646\tFew:\t0.0273\n    > [Stats]\tMany:\t0.5466\tMedium:\t0.2646\tFew:\t0.0273\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7465952977971964\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [57 | 200]\n---> Epoch: [57 | 200]\n---> Epoch: [57 | 200]\n    > [Train]\tLoss:\t0.6615\n    > [Train]\tLoss:\t0.6615\n    > [Train]\tLoss:\t0.6615\n    > [Test ]\tLoss:\t4.1143\tAcc:\t28.9500\n    > [Test ]\tLoss:\t4.1143\tAcc:\t28.9500\n    > [Test ]\tLoss:\t4.1143\tAcc:\t28.9500\n    > [Stats]\tMany:\t0.5183\tMedium:\t0.2654\tFew:\t0.0507\n    > [Stats]\tMany:\t0.5183\tMedium:\t0.2654\tFew:\t0.0507\n    > [Stats]\tMany:\t0.5183\tMedium:\t0.2654\tFew:\t0.0507\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.755300882000568\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [58 | 200]\n---> Epoch: [58 | 200]\n---> Epoch: [58 | 200]\n    > [Train]\tLoss:\t0.6813\n    > [Train]\tLoss:\t0.6813\n    > [Train]\tLoss:\t0.6813\n    > [Test ]\tLoss:\t4.0637\tAcc:\t30.9500\n    > [Test ]\tLoss:\t4.0637\tAcc:\t30.9500\n    > [Test ]\tLoss:\t4.0637\tAcc:\t30.9500\n    > [Stats]\tMany:\t0.5534\tMedium:\t0.2894\tFew:\t0.0483\n    > [Stats]\tMany:\t0.5534\tMedium:\t0.2894\tFew:\t0.0483\n    > [Stats]\tMany:\t0.5534\tMedium:\t0.2894\tFew:\t0.0483\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7492206456178563\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [59 | 200]\n---> Epoch: [59 | 200]\n---> Epoch: [59 | 200]\n    > [Train]\tLoss:\t0.6519\n    > [Train]\tLoss:\t0.6519\n    > [Train]\tLoss:\t0.6519\n    > [Test ]\tLoss:\t4.0570\tAcc:\t29.5400\n    > [Test ]\tLoss:\t4.0570\tAcc:\t29.5400\n    > [Test ]\tLoss:\t4.0570\tAcc:\t29.5400\n    > [Stats]\tMany:\t0.5569\tMedium:\t0.2480\tFew:\t0.0457\n    > [Stats]\tMany:\t0.5569\tMedium:\t0.2480\tFew:\t0.0457\n    > [Stats]\tMany:\t0.5569\tMedium:\t0.2480\tFew:\t0.0457\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Best ]\tAcc:\t31.0900\tMany:\t54.7714\tMedium:\t30.3429\tFew:\t4.3333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7747892318946903\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [60 | 200]\n---> Epoch: [60 | 200]\n---> Epoch: [60 | 200]\n    > [Train]\tLoss:\t0.5898\n    > [Train]\tLoss:\t0.5898\n    > [Train]\tLoss:\t0.5898\n    > [Test ]\tLoss:\t3.9622\tAcc:\t31.3100\n    > [Test ]\tLoss:\t3.9622\tAcc:\t31.3100\n    > [Test ]\tLoss:\t3.9622\tAcc:\t31.3100\n    > [Stats]\tMany:\t0.5614\tMedium:\t0.2869\tFew:\t0.0540\n    > [Stats]\tMany:\t0.5614\tMedium:\t0.2869\tFew:\t0.0540\n    > [Stats]\tMany:\t0.5614\tMedium:\t0.2869\tFew:\t0.0540\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7477557799504244\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [61 | 200]\n---> Epoch: [61 | 200]\n---> Epoch: [61 | 200]\n    > [Train]\tLoss:\t0.6185\n    > [Train]\tLoss:\t0.6185\n    > [Train]\tLoss:\t0.6185\n    > [Test ]\tLoss:\t4.1023\tAcc:\t28.0800\n    > [Test ]\tLoss:\t4.1023\tAcc:\t28.0800\n    > [Test ]\tLoss:\t4.1023\tAcc:\t28.0800\n    > [Stats]\tMany:\t0.5129\tMedium:\t0.2531\tFew:\t0.0423\n    > [Stats]\tMany:\t0.5129\tMedium:\t0.2531\tFew:\t0.0423\n    > [Stats]\tMany:\t0.5129\tMedium:\t0.2531\tFew:\t0.0423\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7419271280291717\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [62 | 200]\n---> Epoch: [62 | 200]\n---> Epoch: [62 | 200]\n    > [Train]\tLoss:\t0.5978\n    > [Train]\tLoss:\t0.5978\n    > [Train]\tLoss:\t0.5978\n    > [Test ]\tLoss:\t4.7380\tAcc:\t29.5400\n    > [Test ]\tLoss:\t4.7380\tAcc:\t29.5400\n    > [Test ]\tLoss:\t4.7380\tAcc:\t29.5400\n    > [Stats]\tMany:\t0.4940\tMedium:\t0.2951\tFew:\t0.0640\n    > [Stats]\tMany:\t0.4940\tMedium:\t0.2951\tFew:\t0.0640\n    > [Stats]\tMany:\t0.4940\tMedium:\t0.2951\tFew:\t0.0640\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7433758796681906\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [63 | 200]\n---> Epoch: [63 | 200]\n---> Epoch: [63 | 200]\n    > [Train]\tLoss:\t0.6070\n    > [Train]\tLoss:\t0.6070\n    > [Train]\tLoss:\t0.6070\n    > [Test ]\tLoss:\t4.5778\tAcc:\t28.0600\n    > [Test ]\tLoss:\t4.5778\tAcc:\t28.0600\n    > [Test ]\tLoss:\t4.5778\tAcc:\t28.0600\n    > [Stats]\tMany:\t0.5403\tMedium:\t0.2289\tFew:\t0.0380\n    > [Stats]\tMany:\t0.5403\tMedium:\t0.2289\tFew:\t0.0380\n    > [Stats]\tMany:\t0.5403\tMedium:\t0.2289\tFew:\t0.0380\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7515740082914375\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [64 | 200]\n---> Epoch: [64 | 200]\n---> Epoch: [64 | 200]\n    > [Train]\tLoss:\t0.5933\n    > [Train]\tLoss:\t0.5933\n    > [Train]\tLoss:\t0.5933\n    > [Test ]\tLoss:\t4.3499\tAcc:\t29.9300\n    > [Test ]\tLoss:\t4.3499\tAcc:\t29.9300\n    > [Test ]\tLoss:\t4.3499\tAcc:\t29.9300\n    > [Stats]\tMany:\t0.5597\tMedium:\t0.2523\tFew:\t0.0503\n    > [Stats]\tMany:\t0.5597\tMedium:\t0.2523\tFew:\t0.0503\n    > [Stats]\tMany:\t0.5597\tMedium:\t0.2523\tFew:\t0.0503\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.729787466450391\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [65 | 200]\n---> Epoch: [65 | 200]\n---> Epoch: [65 | 200]\n    > [Train]\tLoss:\t0.5696\n    > [Train]\tLoss:\t0.5696\n    > [Train]\tLoss:\t0.5696\n    > [Test ]\tLoss:\t4.6175\tAcc:\t29.3600\n    > [Test ]\tLoss:\t4.6175\tAcc:\t29.3600\n    > [Test ]\tLoss:\t4.6175\tAcc:\t29.3600\n    > [Stats]\tMany:\t0.5471\tMedium:\t0.2560\tFew:\t0.0417\n    > [Stats]\tMany:\t0.5471\tMedium:\t0.2560\tFew:\t0.0417\n    > [Stats]\tMany:\t0.5471\tMedium:\t0.2560\tFew:\t0.0417\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.725064076743056\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [66 | 200]\n---> Epoch: [66 | 200]\n---> Epoch: [66 | 200]\n    > [Train]\tLoss:\t0.5593\n    > [Train]\tLoss:\t0.5593\n    > [Train]\tLoss:\t0.5593\n    > [Test ]\tLoss:\t4.4248\tAcc:\t30.6300\n    > [Test ]\tLoss:\t4.4248\tAcc:\t30.6300\n    > [Test ]\tLoss:\t4.4248\tAcc:\t30.6300\n    > [Stats]\tMany:\t0.5597\tMedium:\t0.2746\tFew:\t0.0477\n    > [Stats]\tMany:\t0.5597\tMedium:\t0.2746\tFew:\t0.0477\n    > [Stats]\tMany:\t0.5597\tMedium:\t0.2746\tFew:\t0.0477\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.725345485553217\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [67 | 200]\n---> Epoch: [67 | 200]\n---> Epoch: [67 | 200]\n    > [Train]\tLoss:\t0.5903\n    > [Train]\tLoss:\t0.5903\n    > [Train]\tLoss:\t0.5903\n    > [Test ]\tLoss:\t4.3405\tAcc:\t28.0900\n    > [Test ]\tLoss:\t4.3405\tAcc:\t28.0900\n    > [Test ]\tLoss:\t4.3405\tAcc:\t28.0900\n    > [Stats]\tMany:\t0.4980\tMedium:\t0.2571\tFew:\t0.0553\n    > [Stats]\tMany:\t0.4980\tMedium:\t0.2571\tFew:\t0.0553\n    > [Stats]\tMany:\t0.4980\tMedium:\t0.2571\tFew:\t0.0553\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7122398055352663\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [68 | 200]\n---> Epoch: [68 | 200]\n---> Epoch: [68 | 200]\n    > [Train]\tLoss:\t0.5816\n    > [Train]\tLoss:\t0.5816\n    > [Train]\tLoss:\t0.5816\n    > [Test ]\tLoss:\t4.0474\tAcc:\t30.0500\n    > [Test ]\tLoss:\t4.0474\tAcc:\t30.0500\n    > [Test ]\tLoss:\t4.0474\tAcc:\t30.0500\n    > [Stats]\tMany:\t0.5494\tMedium:\t0.2677\tFew:\t0.0483\n    > [Stats]\tMany:\t0.5494\tMedium:\t0.2677\tFew:\t0.0483\n    > [Stats]\tMany:\t0.5494\tMedium:\t0.2677\tFew:\t0.0483\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Best ]\tAcc:\t31.3100\tMany:\t56.1429\tMedium:\t28.6857\tFew:\t5.4000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.678506785835707\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [69 | 200]\n---> Epoch: [69 | 200]\n---> Epoch: [69 | 200]\n    > [Train]\tLoss:\t0.5196\n    > [Train]\tLoss:\t0.5196\n    > [Train]\tLoss:\t0.5196\n    > [Test ]\tLoss:\t3.8808\tAcc:\t32.9800\n    > [Test ]\tLoss:\t3.8808\tAcc:\t32.9800\n    > [Test ]\tLoss:\t3.8808\tAcc:\t32.9800\n    > [Stats]\tMany:\t0.5420\tMedium:\t0.3477\tFew:\t0.0613\n    > [Stats]\tMany:\t0.5420\tMedium:\t0.3477\tFew:\t0.0613\n    > [Stats]\tMany:\t0.5420\tMedium:\t0.3477\tFew:\t0.0613\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.6791071911536424\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [70 | 200]\n---> Epoch: [70 | 200]\n---> Epoch: [70 | 200]\n    > [Train]\tLoss:\t0.5300\n    > [Train]\tLoss:\t0.5300\n    > [Train]\tLoss:\t0.5300\n    > [Test ]\tLoss:\t4.4392\tAcc:\t27.6500\n    > [Test ]\tLoss:\t4.4392\tAcc:\t27.6500\n    > [Test ]\tLoss:\t4.4392\tAcc:\t27.6500\n    > [Stats]\tMany:\t0.4886\tMedium:\t0.2691\tFew:\t0.0377\n    > [Stats]\tMany:\t0.4886\tMedium:\t0.2691\tFew:\t0.0377\n    > [Stats]\tMany:\t0.4886\tMedium:\t0.2691\tFew:\t0.0377\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7217450314150353\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [71 | 200]\n---> Epoch: [71 | 200]\n---> Epoch: [71 | 200]\n    > [Train]\tLoss:\t0.5203\n    > [Train]\tLoss:\t0.5203\n    > [Train]\tLoss:\t0.5203\n    > [Test ]\tLoss:\t4.0618\tAcc:\t31.7500\n    > [Test ]\tLoss:\t4.0618\tAcc:\t31.7500\n    > [Test ]\tLoss:\t4.0618\tAcc:\t31.7500\n    > [Stats]\tMany:\t0.5603\tMedium:\t0.2960\tFew:\t0.0593\n    > [Stats]\tMany:\t0.5603\tMedium:\t0.2960\tFew:\t0.0593\n    > [Stats]\tMany:\t0.5603\tMedium:\t0.2960\tFew:\t0.0593\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.7190714856542586\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [72 | 200]\n---> Epoch: [72 | 200]\n---> Epoch: [72 | 200]\n    > [Train]\tLoss:\t0.5248\n    > [Train]\tLoss:\t0.5248\n    > [Train]\tLoss:\t0.5248\n    > [Test ]\tLoss:\t4.1931\tAcc:\t30.8200\n    > [Test ]\tLoss:\t4.1931\tAcc:\t30.8200\n    > [Test ]\tLoss:\t4.1931\tAcc:\t30.8200\n    > [Stats]\tMany:\t0.5449\tMedium:\t0.2669\tFew:\t0.0803\n    > [Stats]\tMany:\t0.5449\tMedium:\t0.2669\tFew:\t0.0803\n    > [Stats]\tMany:\t0.5449\tMedium:\t0.2669\tFew:\t0.0803\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.6870472552230162\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [73 | 200]\n---> Epoch: [73 | 200]\n---> Epoch: [73 | 200]\n    > [Train]\tLoss:\t0.5148\n    > [Train]\tLoss:\t0.5148\n    > [Train]\tLoss:\t0.5148\n    > [Test ]\tLoss:\t4.6275\tAcc:\t28.6800\n    > [Test ]\tLoss:\t4.6275\tAcc:\t28.6800\n    > [Test ]\tLoss:\t4.6275\tAcc:\t28.6800\n    > [Stats]\tMany:\t0.5609\tMedium:\t0.2334\tFew:\t0.0293\n    > [Stats]\tMany:\t0.5609\tMedium:\t0.2334\tFew:\t0.0293\n    > [Stats]\tMany:\t0.5609\tMedium:\t0.2334\tFew:\t0.0293\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.702615090220856\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [74 | 200]\n---> Epoch: [74 | 200]\n---> Epoch: [74 | 200]\n    > [Train]\tLoss:\t0.5201\n    > [Train]\tLoss:\t0.5201\n    > [Train]\tLoss:\t0.5201\n    > [Test ]\tLoss:\t4.7895\tAcc:\t27.9800\n    > [Test ]\tLoss:\t4.7895\tAcc:\t27.9800\n    > [Test ]\tLoss:\t4.7895\tAcc:\t27.9800\n    > [Stats]\tMany:\t0.5069\tMedium:\t0.2574\tFew:\t0.0410\n    > [Stats]\tMany:\t0.5069\tMedium:\t0.2574\tFew:\t0.0410\n    > [Stats]\tMany:\t0.5069\tMedium:\t0.2574\tFew:\t0.0410\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.684585221088283\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [75 | 200]\n---> Epoch: [75 | 200]\n---> Epoch: [75 | 200]\n    > [Train]\tLoss:\t0.4965\n    > [Train]\tLoss:\t0.4965\n    > [Train]\tLoss:\t0.4965\n    > [Test ]\tLoss:\t4.2640\tAcc:\t31.5600\n    > [Test ]\tLoss:\t4.2640\tAcc:\t31.5600\n    > [Test ]\tLoss:\t4.2640\tAcc:\t31.5600\n    > [Stats]\tMany:\t0.5620\tMedium:\t0.2917\tFew:\t0.0560\n    > [Stats]\tMany:\t0.5620\tMedium:\t0.2917\tFew:\t0.0560\n    > [Stats]\tMany:\t0.5620\tMedium:\t0.2917\tFew:\t0.0560\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.67993275411912\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [76 | 200]\n---> Epoch: [76 | 200]\n---> Epoch: [76 | 200]\n    > [Train]\tLoss:\t0.5103\n    > [Train]\tLoss:\t0.5103\n    > [Train]\tLoss:\t0.5103\n    > [Test ]\tLoss:\t5.4514\tAcc:\t27.1100\n    > [Test ]\tLoss:\t5.4514\tAcc:\t27.1100\n    > [Test ]\tLoss:\t5.4514\tAcc:\t27.1100\n    > [Stats]\tMany:\t0.5211\tMedium:\t0.2200\tFew:\t0.0390\n    > [Stats]\tMany:\t0.5211\tMedium:\t0.2200\tFew:\t0.0390\n    > [Stats]\tMany:\t0.5211\tMedium:\t0.2200\tFew:\t0.0390\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.6665606053881077\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [77 | 200]\n---> Epoch: [77 | 200]\n---> Epoch: [77 | 200]\n    > [Train]\tLoss:\t0.5065\n    > [Train]\tLoss:\t0.5065\n    > [Train]\tLoss:\t0.5065\n    > [Test ]\tLoss:\t4.9754\tAcc:\t29.3200\n    > [Test ]\tLoss:\t4.9754\tAcc:\t29.3200\n    > [Test ]\tLoss:\t4.9754\tAcc:\t29.3200\n    > [Stats]\tMany:\t0.5020\tMedium:\t0.2983\tFew:\t0.0437\n    > [Stats]\tMany:\t0.5020\tMedium:\t0.2983\tFew:\t0.0437\n    > [Stats]\tMany:\t0.5020\tMedium:\t0.2983\tFew:\t0.0437\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.6819514306868335\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [78 | 200]\n---> Epoch: [78 | 200]\n---> Epoch: [78 | 200]\n    > [Train]\tLoss:\t0.5161\n    > [Train]\tLoss:\t0.5161\n    > [Train]\tLoss:\t0.5161\n    > [Test ]\tLoss:\t4.8527\tAcc:\t28.6700\n    > [Test ]\tLoss:\t4.8527\tAcc:\t28.6700\n    > [Test ]\tLoss:\t4.8527\tAcc:\t28.6700\n    > [Stats]\tMany:\t0.5417\tMedium:\t0.2446\tFew:\t0.0383\n    > [Stats]\tMany:\t0.5417\tMedium:\t0.2446\tFew:\t0.0383\n    > [Stats]\tMany:\t0.5417\tMedium:\t0.2446\tFew:\t0.0383\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.682435449385259\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [79 | 200]\n---> Epoch: [79 | 200]\n---> Epoch: [79 | 200]\n    > [Train]\tLoss:\t0.4754\n    > [Train]\tLoss:\t0.4754\n    > [Train]\tLoss:\t0.4754\n    > [Test ]\tLoss:\t5.0033\tAcc:\t28.6600\n    > [Test ]\tLoss:\t5.0033\tAcc:\t28.6600\n    > [Test ]\tLoss:\t5.0033\tAcc:\t28.6600\n    > [Stats]\tMany:\t0.5531\tMedium:\t0.2306\tFew:\t0.0410\n    > [Stats]\tMany:\t0.5531\tMedium:\t0.2306\tFew:\t0.0410\n    > [Stats]\tMany:\t0.5531\tMedium:\t0.2306\tFew:\t0.0410\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.6646350822501894\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [80 | 200]\n---> Epoch: [80 | 200]\n---> Epoch: [80 | 200]\n    > [Train]\tLoss:\t0.5168\n    > [Train]\tLoss:\t0.5168\n    > [Train]\tLoss:\t0.5168\n    > [Test ]\tLoss:\t4.3970\tAcc:\t29.7200\n    > [Test ]\tLoss:\t4.3970\tAcc:\t29.7200\n    > [Test ]\tLoss:\t4.3970\tAcc:\t29.7200\n    > [Stats]\tMany:\t0.5574\tMedium:\t0.2511\tFew:\t0.0473\n    > [Stats]\tMany:\t0.5574\tMedium:\t0.2511\tFew:\t0.0473\n    > [Stats]\tMany:\t0.5574\tMedium:\t0.2511\tFew:\t0.0473\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.674790652518728\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [81 | 200]\n---> Epoch: [81 | 200]\n---> Epoch: [81 | 200]\n    > [Train]\tLoss:\t0.4413\n    > [Train]\tLoss:\t0.4413\n    > [Train]\tLoss:\t0.4413\n    > [Test ]\tLoss:\t4.4590\tAcc:\t30.1000\n    > [Test ]\tLoss:\t4.4590\tAcc:\t30.1000\n    > [Test ]\tLoss:\t4.4590\tAcc:\t30.1000\n    > [Stats]\tMany:\t0.5263\tMedium:\t0.2840\tFew:\t0.0580\n    > [Stats]\tMany:\t0.5263\tMedium:\t0.2840\tFew:\t0.0580\n    > [Stats]\tMany:\t0.5263\tMedium:\t0.2840\tFew:\t0.0580\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.6663934322308527\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [82 | 200]\n---> Epoch: [82 | 200]\n---> Epoch: [82 | 200]\n    > [Train]\tLoss:\t0.4587\n    > [Train]\tLoss:\t0.4587\n    > [Train]\tLoss:\t0.4587\n    > [Test ]\tLoss:\t4.3678\tAcc:\t31.1200\n    > [Test ]\tLoss:\t4.3678\tAcc:\t31.1200\n    > [Test ]\tLoss:\t4.3678\tAcc:\t31.1200\n    > [Stats]\tMany:\t0.5643\tMedium:\t0.2797\tFew:\t0.0527\n    > [Stats]\tMany:\t0.5643\tMedium:\t0.2797\tFew:\t0.0527\n    > [Stats]\tMany:\t0.5643\tMedium:\t0.2797\tFew:\t0.0527\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.6618401478881575\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [83 | 200]\n---> Epoch: [83 | 200]\n---> Epoch: [83 | 200]\n    > [Train]\tLoss:\t0.4468\n    > [Train]\tLoss:\t0.4468\n    > [Train]\tLoss:\t0.4468\n    > [Test ]\tLoss:\t4.7525\tAcc:\t31.8200\n    > [Test ]\tLoss:\t4.7525\tAcc:\t31.8200\n    > [Test ]\tLoss:\t4.7525\tAcc:\t31.8200\n    > [Stats]\tMany:\t0.5869\tMedium:\t0.2737\tFew:\t0.0567\n    > [Stats]\tMany:\t0.5869\tMedium:\t0.2737\tFew:\t0.0567\n    > [Stats]\tMany:\t0.5869\tMedium:\t0.2737\tFew:\t0.0567\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.640628464840174\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [84 | 200]\n---> Epoch: [84 | 200]\n---> Epoch: [84 | 200]\n    > [Train]\tLoss:\t0.4768\n    > [Train]\tLoss:\t0.4768\n    > [Train]\tLoss:\t0.4768\n    > [Test ]\tLoss:\t4.2333\tAcc:\t31.0100\n    > [Test ]\tLoss:\t4.2333\tAcc:\t31.0100\n    > [Test ]\tLoss:\t4.2333\tAcc:\t31.0100\n    > [Stats]\tMany:\t0.5706\tMedium:\t0.2651\tFew:\t0.0587\n    > [Stats]\tMany:\t0.5706\tMedium:\t0.2651\tFew:\t0.0587\n    > [Stats]\tMany:\t0.5706\tMedium:\t0.2651\tFew:\t0.0587\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.63964028055979\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [85 | 200]\n---> Epoch: [85 | 200]\n---> Epoch: [85 | 200]\n    > [Train]\tLoss:\t0.4181\n    > [Train]\tLoss:\t0.4181\n    > [Train]\tLoss:\t0.4181\n    > [Test ]\tLoss:\t4.6097\tAcc:\t30.9800\n    > [Test ]\tLoss:\t4.6097\tAcc:\t30.9800\n    > [Test ]\tLoss:\t4.6097\tAcc:\t30.9800\n    > [Stats]\tMany:\t0.5746\tMedium:\t0.2663\tFew:\t0.0517\n    > [Stats]\tMany:\t0.5746\tMedium:\t0.2663\tFew:\t0.0517\n    > [Stats]\tMany:\t0.5746\tMedium:\t0.2663\tFew:\t0.0517\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.624826289434785\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [86 | 200]\n---> Epoch: [86 | 200]\n---> Epoch: [86 | 200]\n    > [Train]\tLoss:\t0.4433\n    > [Train]\tLoss:\t0.4433\n    > [Train]\tLoss:\t0.4433\n    > [Test ]\tLoss:\t4.1103\tAcc:\t31.8500\n    > [Test ]\tLoss:\t4.1103\tAcc:\t31.8500\n    > [Test ]\tLoss:\t4.1103\tAcc:\t31.8500\n    > [Stats]\tMany:\t0.5694\tMedium:\t0.2931\tFew:\t0.0553\n    > [Stats]\tMany:\t0.5694\tMedium:\t0.2931\tFew:\t0.0553\n    > [Stats]\tMany:\t0.5694\tMedium:\t0.2931\tFew:\t0.0553\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.6216404467820675\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [87 | 200]\n---> Epoch: [87 | 200]\n---> Epoch: [87 | 200]\n    > [Train]\tLoss:\t0.4669\n    > [Train]\tLoss:\t0.4669\n    > [Train]\tLoss:\t0.4669\n    > [Test ]\tLoss:\t4.2762\tAcc:\t31.8200\n    > [Test ]\tLoss:\t4.2762\tAcc:\t31.8200\n    > [Test ]\tLoss:\t4.2762\tAcc:\t31.8200\n    > [Stats]\tMany:\t0.5811\tMedium:\t0.2871\tFew:\t0.0477\n    > [Stats]\tMany:\t0.5811\tMedium:\t0.2871\tFew:\t0.0477\n    > [Stats]\tMany:\t0.5811\tMedium:\t0.2871\tFew:\t0.0477\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.6217400263819526\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [88 | 200]\n---> Epoch: [88 | 200]\n---> Epoch: [88 | 200]\n    > [Train]\tLoss:\t0.4554\n    > [Train]\tLoss:\t0.4554\n    > [Train]\tLoss:\t0.4554\n    > [Test ]\tLoss:\t5.4637\tAcc:\t26.4500\n    > [Test ]\tLoss:\t5.4637\tAcc:\t26.4500\n    > [Test ]\tLoss:\t5.4637\tAcc:\t26.4500\n    > [Stats]\tMany:\t0.4960\tMedium:\t0.2300\tFew:\t0.0347\n    > [Stats]\tMany:\t0.4960\tMedium:\t0.2300\tFew:\t0.0347\n    > [Stats]\tMany:\t0.4960\tMedium:\t0.2300\tFew:\t0.0347\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.607224397838301\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [89 | 200]\n---> Epoch: [89 | 200]\n---> Epoch: [89 | 200]\n    > [Train]\tLoss:\t0.4681\n    > [Train]\tLoss:\t0.4681\n    > [Train]\tLoss:\t0.4681\n    > [Test ]\tLoss:\t5.1356\tAcc:\t24.5600\n    > [Test ]\tLoss:\t5.1356\tAcc:\t24.5600\n    > [Test ]\tLoss:\t5.1356\tAcc:\t24.5600\n    > [Stats]\tMany:\t0.4500\tMedium:\t0.2149\tFew:\t0.0430\n    > [Stats]\tMany:\t0.4500\tMedium:\t0.2149\tFew:\t0.0430\n    > [Stats]\tMany:\t0.4500\tMedium:\t0.2149\tFew:\t0.0430\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.597703906327397\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [90 | 200]\n---> Epoch: [90 | 200]\n---> Epoch: [90 | 200]\n    > [Train]\tLoss:\t0.4380\n    > [Train]\tLoss:\t0.4380\n    > [Train]\tLoss:\t0.4380\n    > [Test ]\tLoss:\t4.4423\tAcc:\t30.8400\n    > [Test ]\tLoss:\t4.4423\tAcc:\t30.8400\n    > [Test ]\tLoss:\t4.4423\tAcc:\t30.8400\n    > [Stats]\tMany:\t0.5223\tMedium:\t0.2991\tFew:\t0.0697\n    > [Stats]\tMany:\t0.5223\tMedium:\t0.2991\tFew:\t0.0697\n    > [Stats]\tMany:\t0.5223\tMedium:\t0.2991\tFew:\t0.0697\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.5829744245481225\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [91 | 200]\n---> Epoch: [91 | 200]\n---> Epoch: [91 | 200]\n    > [Train]\tLoss:\t0.4447\n    > [Train]\tLoss:\t0.4447\n    > [Train]\tLoss:\t0.4447\n    > [Test ]\tLoss:\t4.1596\tAcc:\t31.9500\n    > [Test ]\tLoss:\t4.1596\tAcc:\t31.9500\n    > [Test ]\tLoss:\t4.1596\tAcc:\t31.9500\n    > [Stats]\tMany:\t0.5380\tMedium:\t0.3197\tFew:\t0.0643\n    > [Stats]\tMany:\t0.5380\tMedium:\t0.3197\tFew:\t0.0643\n    > [Stats]\tMany:\t0.5380\tMedium:\t0.3197\tFew:\t0.0643\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.56730318579851\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [92 | 200]\n---> Epoch: [92 | 200]\n---> Epoch: [92 | 200]\n    > [Train]\tLoss:\t0.4360\n    > [Train]\tLoss:\t0.4360\n    > [Train]\tLoss:\t0.4360\n    > [Test ]\tLoss:\t4.4962\tAcc:\t30.3500\n    > [Test ]\tLoss:\t4.4962\tAcc:\t30.3500\n    > [Test ]\tLoss:\t4.4962\tAcc:\t30.3500\n    > [Stats]\tMany:\t0.5546\tMedium:\t0.2711\tFew:\t0.0483\n    > [Stats]\tMany:\t0.5546\tMedium:\t0.2711\tFew:\t0.0483\n    > [Stats]\tMany:\t0.5546\tMedium:\t0.2711\tFew:\t0.0483\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.602694591883653\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [93 | 200]\n---> Epoch: [93 | 200]\n---> Epoch: [93 | 200]\n    > [Train]\tLoss:\t0.4304\n    > [Train]\tLoss:\t0.4304\n    > [Train]\tLoss:\t0.4304\n    > [Test ]\tLoss:\t4.7360\tAcc:\t29.7100\n    > [Test ]\tLoss:\t4.7360\tAcc:\t29.7100\n    > [Test ]\tLoss:\t4.7360\tAcc:\t29.7100\n    > [Stats]\tMany:\t0.5451\tMedium:\t0.2669\tFew:\t0.0430\n    > [Stats]\tMany:\t0.5451\tMedium:\t0.2669\tFew:\t0.0430\n    > [Stats]\tMany:\t0.5451\tMedium:\t0.2669\tFew:\t0.0430\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.589419963260789\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [94 | 200]\n---> Epoch: [94 | 200]\n---> Epoch: [94 | 200]\n    > [Train]\tLoss:\t0.4165\n    > [Train]\tLoss:\t0.4165\n    > [Train]\tLoss:\t0.4165\n    > [Test ]\tLoss:\t4.8352\tAcc:\t29.7400\n    > [Test ]\tLoss:\t4.8352\tAcc:\t29.7400\n    > [Test ]\tLoss:\t4.8352\tAcc:\t29.7400\n    > [Stats]\tMany:\t0.4986\tMedium:\t0.3146\tFew:\t0.0427\n    > [Stats]\tMany:\t0.4986\tMedium:\t0.3146\tFew:\t0.0427\n    > [Stats]\tMany:\t0.4986\tMedium:\t0.3146\tFew:\t0.0427\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.6015941180871303\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [95 | 200]\n---> Epoch: [95 | 200]\n---> Epoch: [95 | 200]\n    > [Train]\tLoss:\t0.3879\n    > [Train]\tLoss:\t0.3879\n    > [Train]\tLoss:\t0.3879\n    > [Test ]\tLoss:\t4.7612\tAcc:\t30.0100\n    > [Test ]\tLoss:\t4.7612\tAcc:\t30.0100\n    > [Test ]\tLoss:\t4.7612\tAcc:\t30.0100\n    > [Stats]\tMany:\t0.5186\tMedium:\t0.2840\tFew:\t0.0640\n    > [Stats]\tMany:\t0.5186\tMedium:\t0.2840\tFew:\t0.0640\n    > [Stats]\tMany:\t0.5186\tMedium:\t0.2840\tFew:\t0.0640\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.5885551523945134\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [96 | 200]\n---> Epoch: [96 | 200]\n---> Epoch: [96 | 200]\n    > [Train]\tLoss:\t0.4270\n    > [Train]\tLoss:\t0.4270\n    > [Train]\tLoss:\t0.4270\n    > [Test ]\tLoss:\t4.5987\tAcc:\t30.1200\n    > [Test ]\tLoss:\t4.5987\tAcc:\t30.1200\n    > [Test ]\tLoss:\t4.5987\tAcc:\t30.1200\n    > [Stats]\tMany:\t0.5186\tMedium:\t0.2860\tFew:\t0.0653\n    > [Stats]\tMany:\t0.5186\tMedium:\t0.2860\tFew:\t0.0653\n    > [Stats]\tMany:\t0.5186\tMedium:\t0.2860\tFew:\t0.0653\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.5742165088809337\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [97 | 200]\n---> Epoch: [97 | 200]\n---> Epoch: [97 | 200]\n    > [Train]\tLoss:\t0.4302\n    > [Train]\tLoss:\t0.4302\n    > [Train]\tLoss:\t0.4302\n    > [Test ]\tLoss:\t4.2355\tAcc:\t30.5500\n    > [Test ]\tLoss:\t4.2355\tAcc:\t30.5500\n    > [Test ]\tLoss:\t4.2355\tAcc:\t30.5500\n    > [Stats]\tMany:\t0.5566\tMedium:\t0.2734\tFew:\t0.0500\n    > [Stats]\tMany:\t0.5566\tMedium:\t0.2734\tFew:\t0.0500\n    > [Stats]\tMany:\t0.5566\tMedium:\t0.2734\tFew:\t0.0500\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.5583445761873085\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [98 | 200]\n---> Epoch: [98 | 200]\n---> Epoch: [98 | 200]\n    > [Train]\tLoss:\t0.3903\n    > [Train]\tLoss:\t0.3903\n    > [Train]\tLoss:\t0.3903\n    > [Test ]\tLoss:\t4.4365\tAcc:\t31.6900\n    > [Test ]\tLoss:\t4.4365\tAcc:\t31.6900\n    > [Test ]\tLoss:\t4.4365\tAcc:\t31.6900\n    > [Stats]\tMany:\t0.5780\tMedium:\t0.2811\tFew:\t0.0540\n    > [Stats]\tMany:\t0.5780\tMedium:\t0.2811\tFew:\t0.0540\n    > [Stats]\tMany:\t0.5780\tMedium:\t0.2811\tFew:\t0.0540\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.5429143419172635\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [99 | 200]\n---> Epoch: [99 | 200]\n---> Epoch: [99 | 200]\n    > [Train]\tLoss:\t0.3898\n    > [Train]\tLoss:\t0.3898\n    > [Train]\tLoss:\t0.3898\n    > [Test ]\tLoss:\t4.4040\tAcc:\t29.6800\n    > [Test ]\tLoss:\t4.4040\tAcc:\t29.6800\n    > [Test ]\tLoss:\t4.4040\tAcc:\t29.6800\n    > [Stats]\tMany:\t0.5426\tMedium:\t0.2600\tFew:\t0.0530\n    > [Stats]\tMany:\t0.5426\tMedium:\t0.2600\tFew:\t0.0530\n    > [Stats]\tMany:\t0.5426\tMedium:\t0.2600\tFew:\t0.0530\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.531953234083478\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [100 | 200]\n---> Epoch: [100 | 200]\n---> Epoch: [100 | 200]\n    > [Train]\tLoss:\t0.3877\n    > [Train]\tLoss:\t0.3877\n    > [Train]\tLoss:\t0.3877\n    > [Test ]\tLoss:\t4.7560\tAcc:\t28.6800\n    > [Test ]\tLoss:\t4.7560\tAcc:\t28.6800\n    > [Test ]\tLoss:\t4.7560\tAcc:\t28.6800\n    > [Stats]\tMany:\t0.5374\tMedium:\t0.2409\tFew:\t0.0480\n    > [Stats]\tMany:\t0.5374\tMedium:\t0.2409\tFew:\t0.0480\n    > [Stats]\tMany:\t0.5374\tMedium:\t0.2409\tFew:\t0.0480\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.5518758989398576\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [101 | 200]\n---> Epoch: [101 | 200]\n---> Epoch: [101 | 200]\n    > [Train]\tLoss:\t0.3925\n    > [Train]\tLoss:\t0.3925\n    > [Train]\tLoss:\t0.3925\n    > [Test ]\tLoss:\t4.9091\tAcc:\t30.0000\n    > [Test ]\tLoss:\t4.9091\tAcc:\t30.0000\n    > [Test ]\tLoss:\t4.9091\tAcc:\t30.0000\n    > [Stats]\tMany:\t0.5360\tMedium:\t0.2763\tFew:\t0.0523\n    > [Stats]\tMany:\t0.5360\tMedium:\t0.2763\tFew:\t0.0523\n    > [Stats]\tMany:\t0.5360\tMedium:\t0.2763\tFew:\t0.0523\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.5282255677284184\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [102 | 200]\n---> Epoch: [102 | 200]\n---> Epoch: [102 | 200]\n    > [Train]\tLoss:\t0.3951\n    > [Train]\tLoss:\t0.3951\n    > [Train]\tLoss:\t0.3951\n    > [Test ]\tLoss:\t4.2358\tAcc:\t32.6000\n    > [Test ]\tLoss:\t4.2358\tAcc:\t32.6000\n    > [Test ]\tLoss:\t4.2358\tAcc:\t32.6000\n    > [Stats]\tMany:\t0.5634\tMedium:\t0.3174\tFew:\t0.0590\n    > [Stats]\tMany:\t0.5634\tMedium:\t0.3174\tFew:\t0.0590\n    > [Stats]\tMany:\t0.5634\tMedium:\t0.3174\tFew:\t0.0590\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.499708769299523\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [103 | 200]\n---> Epoch: [103 | 200]\n---> Epoch: [103 | 200]\n    > [Train]\tLoss:\t0.3984\n    > [Train]\tLoss:\t0.3984\n    > [Train]\tLoss:\t0.3984\n    > [Test ]\tLoss:\t4.2448\tAcc:\t31.8300\n    > [Test ]\tLoss:\t4.2448\tAcc:\t31.8300\n    > [Test ]\tLoss:\t4.2448\tAcc:\t31.8300\n    > [Stats]\tMany:\t0.5423\tMedium:\t0.3129\tFew:\t0.0633\n    > [Stats]\tMany:\t0.5423\tMedium:\t0.3129\tFew:\t0.0633\n    > [Stats]\tMany:\t0.5423\tMedium:\t0.3129\tFew:\t0.0633\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.506097512179128\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [104 | 200]\n---> Epoch: [104 | 200]\n---> Epoch: [104 | 200]\n    > [Train]\tLoss:\t0.3972\n    > [Train]\tLoss:\t0.3972\n    > [Train]\tLoss:\t0.3972\n    > [Test ]\tLoss:\t4.7650\tAcc:\t30.0900\n    > [Test ]\tLoss:\t4.7650\tAcc:\t30.0900\n    > [Test ]\tLoss:\t4.7650\tAcc:\t30.0900\n    > [Stats]\tMany:\t0.5763\tMedium:\t0.2391\tFew:\t0.0517\n    > [Stats]\tMany:\t0.5763\tMedium:\t0.2391\tFew:\t0.0517\n    > [Stats]\tMany:\t0.5763\tMedium:\t0.2391\tFew:\t0.0517\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.512562720414096\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [105 | 200]\n---> Epoch: [105 | 200]\n---> Epoch: [105 | 200]\n    > [Train]\tLoss:\t0.4206\n    > [Train]\tLoss:\t0.4206\n    > [Train]\tLoss:\t0.4206\n    > [Test ]\tLoss:\t4.8372\tAcc:\t29.6800\n    > [Test ]\tLoss:\t4.8372\tAcc:\t29.6800\n    > [Test ]\tLoss:\t4.8372\tAcc:\t29.6800\n    > [Stats]\tMany:\t0.4840\tMedium:\t0.3091\tFew:\t0.0640\n    > [Stats]\tMany:\t0.4840\tMedium:\t0.3091\tFew:\t0.0640\n    > [Stats]\tMany:\t0.4840\tMedium:\t0.3091\tFew:\t0.0640\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.4816732004714557\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [106 | 200]\n---> Epoch: [106 | 200]\n---> Epoch: [106 | 200]\n    > [Train]\tLoss:\t0.3764\n    > [Train]\tLoss:\t0.3764\n    > [Train]\tLoss:\t0.3764\n    > [Test ]\tLoss:\t4.7631\tAcc:\t30.9500\n    > [Test ]\tLoss:\t4.7631\tAcc:\t30.9500\n    > [Test ]\tLoss:\t4.7631\tAcc:\t30.9500\n    > [Stats]\tMany:\t0.5383\tMedium:\t0.2900\tFew:\t0.0653\n    > [Stats]\tMany:\t0.5383\tMedium:\t0.2900\tFew:\t0.0653\n    > [Stats]\tMany:\t0.5383\tMedium:\t0.2900\tFew:\t0.0653\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.498489417683147\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [107 | 200]\n---> Epoch: [107 | 200]\n---> Epoch: [107 | 200]\n    > [Train]\tLoss:\t0.3713\n    > [Train]\tLoss:\t0.3713\n    > [Train]\tLoss:\t0.3713\n    > [Test ]\tLoss:\t4.7352\tAcc:\t30.9100\n    > [Test ]\tLoss:\t4.7352\tAcc:\t30.9100\n    > [Test ]\tLoss:\t4.7352\tAcc:\t30.9100\n    > [Stats]\tMany:\t0.5560\tMedium:\t0.2894\tFew:\t0.0440\n    > [Stats]\tMany:\t0.5560\tMedium:\t0.2894\tFew:\t0.0440\n    > [Stats]\tMany:\t0.5560\tMedium:\t0.2894\tFew:\t0.0440\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.5014855109366168\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [108 | 200]\n---> Epoch: [108 | 200]\n---> Epoch: [108 | 200]\n    > [Train]\tLoss:\t0.3851\n    > [Train]\tLoss:\t0.3851\n    > [Train]\tLoss:\t0.3851\n    > [Test ]\tLoss:\t4.4891\tAcc:\t32.7500\n    > [Test ]\tLoss:\t4.4891\tAcc:\t32.7500\n    > [Test ]\tLoss:\t4.4891\tAcc:\t32.7500\n    > [Stats]\tMany:\t0.5483\tMedium:\t0.3357\tFew:\t0.0603\n    > [Stats]\tMany:\t0.5483\tMedium:\t0.3357\tFew:\t0.0603\n    > [Stats]\tMany:\t0.5483\tMedium:\t0.3357\tFew:\t0.0603\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.46620011028114\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [109 | 200]\n---> Epoch: [109 | 200]\n---> Epoch: [109 | 200]\n    > [Train]\tLoss:\t0.3908\n    > [Train]\tLoss:\t0.3908\n    > [Train]\tLoss:\t0.3908\n    > [Test ]\tLoss:\t4.5968\tAcc:\t31.4400\n    > [Test ]\tLoss:\t4.5968\tAcc:\t31.4400\n    > [Test ]\tLoss:\t4.5968\tAcc:\t31.4400\n    > [Stats]\tMany:\t0.5737\tMedium:\t0.2894\tFew:\t0.0410\n    > [Stats]\tMany:\t0.5737\tMedium:\t0.2894\tFew:\t0.0410\n    > [Stats]\tMany:\t0.5737\tMedium:\t0.2894\tFew:\t0.0410\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.450689110433217\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [110 | 200]\n---> Epoch: [110 | 200]\n---> Epoch: [110 | 200]\n    > [Train]\tLoss:\t0.3729\n    > [Train]\tLoss:\t0.3729\n    > [Train]\tLoss:\t0.3729\n    > [Test ]\tLoss:\t4.3451\tAcc:\t32.1600\n    > [Test ]\tLoss:\t4.3451\tAcc:\t32.1600\n    > [Test ]\tLoss:\t4.3451\tAcc:\t32.1600\n    > [Stats]\tMany:\t0.5811\tMedium:\t0.3046\tFew:\t0.0387\n    > [Stats]\tMany:\t0.5811\tMedium:\t0.3046\tFew:\t0.0387\n    > [Stats]\tMany:\t0.5811\tMedium:\t0.3046\tFew:\t0.0387\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.431971858622021\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [111 | 200]\n---> Epoch: [111 | 200]\n---> Epoch: [111 | 200]\n    > [Train]\tLoss:\t0.3917\n    > [Train]\tLoss:\t0.3917\n    > [Train]\tLoss:\t0.3917\n    > [Test ]\tLoss:\t4.7892\tAcc:\t31.0000\n    > [Test ]\tLoss:\t4.7892\tAcc:\t31.0000\n    > [Test ]\tLoss:\t4.7892\tAcc:\t31.0000\n    > [Stats]\tMany:\t0.5480\tMedium:\t0.2917\tFew:\t0.0537\n    > [Stats]\tMany:\t0.5480\tMedium:\t0.2917\tFew:\t0.0537\n    > [Stats]\tMany:\t0.5480\tMedium:\t0.2917\tFew:\t0.0537\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.4427112599999283\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [112 | 200]\n---> Epoch: [112 | 200]\n---> Epoch: [112 | 200]\n    > [Train]\tLoss:\t0.3672\n    > [Train]\tLoss:\t0.3672\n    > [Train]\tLoss:\t0.3672\n    > [Test ]\tLoss:\t4.9048\tAcc:\t30.0300\n    > [Test ]\tLoss:\t4.9048\tAcc:\t30.0300\n    > [Test ]\tLoss:\t4.9048\tAcc:\t30.0300\n    > [Stats]\tMany:\t0.5449\tMedium:\t0.2737\tFew:\t0.0460\n    > [Stats]\tMany:\t0.5449\tMedium:\t0.2737\tFew:\t0.0460\n    > [Stats]\tMany:\t0.5449\tMedium:\t0.2737\tFew:\t0.0460\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.4222090029929535\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [113 | 200]\n---> Epoch: [113 | 200]\n---> Epoch: [113 | 200]\n    > [Train]\tLoss:\t0.3943\n    > [Train]\tLoss:\t0.3943\n    > [Train]\tLoss:\t0.3943\n    > [Test ]\tLoss:\t5.1231\tAcc:\t28.5100\n    > [Test ]\tLoss:\t5.1231\tAcc:\t28.5100\n    > [Test ]\tLoss:\t5.1231\tAcc:\t28.5100\n    > [Stats]\tMany:\t0.4974\tMedium:\t0.2654\tFew:\t0.0603\n    > [Stats]\tMany:\t0.4974\tMedium:\t0.2654\tFew:\t0.0603\n    > [Stats]\tMany:\t0.4974\tMedium:\t0.2654\tFew:\t0.0603\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.4162237249713074\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [114 | 200]\n---> Epoch: [114 | 200]\n---> Epoch: [114 | 200]\n    > [Train]\tLoss:\t0.3911\n    > [Train]\tLoss:\t0.3911\n    > [Train]\tLoss:\t0.3911\n    > [Test ]\tLoss:\t4.8632\tAcc:\t30.8500\n    > [Test ]\tLoss:\t4.8632\tAcc:\t30.8500\n    > [Test ]\tLoss:\t4.8632\tAcc:\t30.8500\n    > [Stats]\tMany:\t0.5589\tMedium:\t0.2766\tFew:\t0.0537\n    > [Stats]\tMany:\t0.5589\tMedium:\t0.2766\tFew:\t0.0537\n    > [Stats]\tMany:\t0.5589\tMedium:\t0.2766\tFew:\t0.0537\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.429861101370578\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [115 | 200]\n---> Epoch: [115 | 200]\n---> Epoch: [115 | 200]\n    > [Train]\tLoss:\t0.4082\n    > [Train]\tLoss:\t0.4082\n    > [Train]\tLoss:\t0.4082\n    > [Test ]\tLoss:\t4.7733\tAcc:\t29.4000\n    > [Test ]\tLoss:\t4.7733\tAcc:\t29.4000\n    > [Test ]\tLoss:\t4.7733\tAcc:\t29.4000\n    > [Stats]\tMany:\t0.5309\tMedium:\t0.2640\tFew:\t0.0527\n    > [Stats]\tMany:\t0.5309\tMedium:\t0.2640\tFew:\t0.0527\n    > [Stats]\tMany:\t0.5309\tMedium:\t0.2640\tFew:\t0.0527\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.436714360296111\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [116 | 200]\n---> Epoch: [116 | 200]\n---> Epoch: [116 | 200]\n    > [Train]\tLoss:\t0.3802\n    > [Train]\tLoss:\t0.3802\n    > [Train]\tLoss:\t0.3802\n    > [Test ]\tLoss:\t4.7909\tAcc:\t32.0600\n    > [Test ]\tLoss:\t4.7909\tAcc:\t32.0600\n    > [Test ]\tLoss:\t4.7909\tAcc:\t32.0600\n    > [Stats]\tMany:\t0.5637\tMedium:\t0.3154\tFew:\t0.0430\n    > [Stats]\tMany:\t0.5637\tMedium:\t0.3154\tFew:\t0.0430\n    > [Stats]\tMany:\t0.5637\tMedium:\t0.3154\tFew:\t0.0430\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.4354787620876386\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [117 | 200]\n---> Epoch: [117 | 200]\n---> Epoch: [117 | 200]\n    > [Train]\tLoss:\t0.3542\n    > [Train]\tLoss:\t0.3542\n    > [Train]\tLoss:\t0.3542\n    > [Test ]\tLoss:\t4.3599\tAcc:\t32.5000\n    > [Test ]\tLoss:\t4.3599\tAcc:\t32.5000\n    > [Test ]\tLoss:\t4.3599\tAcc:\t32.5000\n    > [Stats]\tMany:\t0.5469\tMedium:\t0.3137\tFew:\t0.0793\n    > [Stats]\tMany:\t0.5469\tMedium:\t0.3137\tFew:\t0.0793\n    > [Stats]\tMany:\t0.5469\tMedium:\t0.3137\tFew:\t0.0793\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.42659536614801\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [118 | 200]\n---> Epoch: [118 | 200]\n---> Epoch: [118 | 200]\n    > [Train]\tLoss:\t0.3506\n    > [Train]\tLoss:\t0.3506\n    > [Train]\tLoss:\t0.3506\n    > [Test ]\tLoss:\t4.7844\tAcc:\t31.7600\n    > [Test ]\tLoss:\t4.7844\tAcc:\t31.7600\n    > [Test ]\tLoss:\t4.7844\tAcc:\t31.7600\n    > [Stats]\tMany:\t0.5491\tMedium:\t0.3131\tFew:\t0.0527\n    > [Stats]\tMany:\t0.5491\tMedium:\t0.3131\tFew:\t0.0527\n    > [Stats]\tMany:\t0.5491\tMedium:\t0.3131\tFew:\t0.0527\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.4514857883628443\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [119 | 200]\n---> Epoch: [119 | 200]\n---> Epoch: [119 | 200]\n    > [Train]\tLoss:\t0.3326\n    > [Train]\tLoss:\t0.3326\n    > [Train]\tLoss:\t0.3326\n    > [Test ]\tLoss:\t4.3232\tAcc:\t31.0600\n    > [Test ]\tLoss:\t4.3232\tAcc:\t31.0600\n    > [Test ]\tLoss:\t4.3232\tAcc:\t31.0600\n    > [Stats]\tMany:\t0.5449\tMedium:\t0.2763\tFew:\t0.0773\n    > [Stats]\tMany:\t0.5449\tMedium:\t0.2763\tFew:\t0.0773\n    > [Stats]\tMany:\t0.5449\tMedium:\t0.2763\tFew:\t0.0773\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.4590375307151175\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [120 | 200]\n---> Epoch: [120 | 200]\n---> Epoch: [120 | 200]\n    > [Train]\tLoss:\t0.3339\n    > [Train]\tLoss:\t0.3339\n    > [Train]\tLoss:\t0.3339\n    > [Test ]\tLoss:\t4.6770\tAcc:\t32.7500\n    > [Test ]\tLoss:\t4.6770\tAcc:\t32.7500\n    > [Test ]\tLoss:\t4.6770\tAcc:\t32.7500\n    > [Stats]\tMany:\t0.5703\tMedium:\t0.3023\tFew:\t0.0737\n    > [Stats]\tMany:\t0.5703\tMedium:\t0.3023\tFew:\t0.0737\n    > [Stats]\tMany:\t0.5703\tMedium:\t0.3023\tFew:\t0.0737\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.4477647532378013\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [121 | 200]\n---> Epoch: [121 | 200]\n---> Epoch: [121 | 200]\n    > [Train]\tLoss:\t0.3757\n    > [Train]\tLoss:\t0.3757\n    > [Train]\tLoss:\t0.3757\n    > [Test ]\tLoss:\t4.7511\tAcc:\t30.7400\n    > [Test ]\tLoss:\t4.7511\tAcc:\t30.7400\n    > [Test ]\tLoss:\t4.7511\tAcc:\t30.7400\n    > [Stats]\tMany:\t0.5494\tMedium:\t0.2803\tFew:\t0.0567\n    > [Stats]\tMany:\t0.5494\tMedium:\t0.2803\tFew:\t0.0567\n    > [Stats]\tMany:\t0.5494\tMedium:\t0.2803\tFew:\t0.0567\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.433684805241373\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [122 | 200]\n---> Epoch: [122 | 200]\n---> Epoch: [122 | 200]\n    > [Train]\tLoss:\t0.3542\n    > [Train]\tLoss:\t0.3542\n    > [Train]\tLoss:\t0.3542\n    > [Test ]\tLoss:\t4.4763\tAcc:\t31.5500\n    > [Test ]\tLoss:\t4.4763\tAcc:\t31.5500\n    > [Test ]\tLoss:\t4.4763\tAcc:\t31.5500\n    > [Stats]\tMany:\t0.5471\tMedium:\t0.2954\tFew:\t0.0687\n    > [Stats]\tMany:\t0.5471\tMedium:\t0.2954\tFew:\t0.0687\n    > [Stats]\tMany:\t0.5471\tMedium:\t0.2954\tFew:\t0.0687\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.4051975218446624\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [123 | 200]\n---> Epoch: [123 | 200]\n---> Epoch: [123 | 200]\n    > [Train]\tLoss:\t0.3661\n    > [Train]\tLoss:\t0.3661\n    > [Train]\tLoss:\t0.3661\n    > [Test ]\tLoss:\t4.9391\tAcc:\t30.9000\n    > [Test ]\tLoss:\t4.9391\tAcc:\t30.9000\n    > [Test ]\tLoss:\t4.9391\tAcc:\t30.9000\n    > [Stats]\tMany:\t0.5257\tMedium:\t0.3080\tFew:\t0.0573\n    > [Stats]\tMany:\t0.5257\tMedium:\t0.3080\tFew:\t0.0573\n    > [Stats]\tMany:\t0.5257\tMedium:\t0.3080\tFew:\t0.0573\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.3875076383817633\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [124 | 200]\n---> Epoch: [124 | 200]\n---> Epoch: [124 | 200]\n    > [Train]\tLoss:\t0.3234\n    > [Train]\tLoss:\t0.3234\n    > [Train]\tLoss:\t0.3234\n    > [Test ]\tLoss:\t4.9061\tAcc:\t30.4700\n    > [Test ]\tLoss:\t4.9061\tAcc:\t30.4700\n    > [Test ]\tLoss:\t4.9061\tAcc:\t30.4700\n    > [Stats]\tMany:\t0.5851\tMedium:\t0.2486\tFew:\t0.0430\n    > [Stats]\tMany:\t0.5851\tMedium:\t0.2486\tFew:\t0.0430\n    > [Stats]\tMany:\t0.5851\tMedium:\t0.2486\tFew:\t0.0430\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.407903406715475\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [125 | 200]\n---> Epoch: [125 | 200]\n---> Epoch: [125 | 200]\n    > [Train]\tLoss:\t0.3417\n    > [Train]\tLoss:\t0.3417\n    > [Train]\tLoss:\t0.3417\n    > [Test ]\tLoss:\t5.2746\tAcc:\t29.8300\n    > [Test ]\tLoss:\t5.2746\tAcc:\t29.8300\n    > [Test ]\tLoss:\t5.2746\tAcc:\t29.8300\n    > [Stats]\tMany:\t0.5346\tMedium:\t0.2794\tFew:\t0.0447\n    > [Stats]\tMany:\t0.5346\tMedium:\t0.2794\tFew:\t0.0447\n    > [Stats]\tMany:\t0.5346\tMedium:\t0.2794\tFew:\t0.0447\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.393249962462379\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [126 | 200]\n---> Epoch: [126 | 200]\n---> Epoch: [126 | 200]\n    > [Train]\tLoss:\t0.3372\n    > [Train]\tLoss:\t0.3372\n    > [Train]\tLoss:\t0.3372\n    > [Test ]\tLoss:\t4.7826\tAcc:\t28.4700\n    > [Test ]\tLoss:\t4.7826\tAcc:\t28.4700\n    > [Test ]\tLoss:\t4.7826\tAcc:\t28.4700\n    > [Stats]\tMany:\t0.5189\tMedium:\t0.2480\tFew:\t0.0543\n    > [Stats]\tMany:\t0.5189\tMedium:\t0.2480\tFew:\t0.0543\n    > [Stats]\tMany:\t0.5189\tMedium:\t0.2480\tFew:\t0.0543\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.4005538853764694\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [127 | 200]\n---> Epoch: [127 | 200]\n---> Epoch: [127 | 200]\n    > [Train]\tLoss:\t0.3907\n    > [Train]\tLoss:\t0.3907\n    > [Train]\tLoss:\t0.3907\n    > [Test ]\tLoss:\t4.9733\tAcc:\t28.9800\n    > [Test ]\tLoss:\t4.9733\tAcc:\t28.9800\n    > [Test ]\tLoss:\t4.9733\tAcc:\t28.9800\n    > [Stats]\tMany:\t0.5217\tMedium:\t0.2666\tFew:\t0.0463\n    > [Stats]\tMany:\t0.5217\tMedium:\t0.2666\tFew:\t0.0463\n    > [Stats]\tMany:\t0.5217\tMedium:\t0.2666\tFew:\t0.0463\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.3864105263297977\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [128 | 200]\n---> Epoch: [128 | 200]\n---> Epoch: [128 | 200]\n    > [Train]\tLoss:\t0.3564\n    > [Train]\tLoss:\t0.3564\n    > [Train]\tLoss:\t0.3564\n    > [Test ]\tLoss:\t4.7245\tAcc:\t31.4600\n    > [Test ]\tLoss:\t4.7245\tAcc:\t31.4600\n    > [Test ]\tLoss:\t4.7245\tAcc:\t31.4600\n    > [Stats]\tMany:\t0.5546\tMedium:\t0.2986\tFew:\t0.0533\n    > [Stats]\tMany:\t0.5546\tMedium:\t0.2986\tFew:\t0.0533\n    > [Stats]\tMany:\t0.5546\tMedium:\t0.2986\tFew:\t0.0533\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.365118167963014\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [129 | 200]\n---> Epoch: [129 | 200]\n---> Epoch: [129 | 200]\n    > [Train]\tLoss:\t0.3402\n    > [Train]\tLoss:\t0.3402\n    > [Train]\tLoss:\t0.3402\n    > [Test ]\tLoss:\t4.8731\tAcc:\t31.7600\n    > [Test ]\tLoss:\t4.8731\tAcc:\t31.7600\n    > [Test ]\tLoss:\t4.8731\tAcc:\t31.7600\n    > [Stats]\tMany:\t0.5531\tMedium:\t0.2914\tFew:\t0.0733\n    > [Stats]\tMany:\t0.5531\tMedium:\t0.2914\tFew:\t0.0733\n    > [Stats]\tMany:\t0.5531\tMedium:\t0.2914\tFew:\t0.0733\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.3800131372583277\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [130 | 200]\n---> Epoch: [130 | 200]\n---> Epoch: [130 | 200]\n    > [Train]\tLoss:\t0.3320\n    > [Train]\tLoss:\t0.3320\n    > [Train]\tLoss:\t0.3320\n    > [Test ]\tLoss:\t5.3603\tAcc:\t27.6800\n    > [Test ]\tLoss:\t5.3603\tAcc:\t27.6800\n    > [Test ]\tLoss:\t5.3603\tAcc:\t27.6800\n    > [Stats]\tMany:\t0.5086\tMedium:\t0.2523\tFew:\t0.0350\n    > [Stats]\tMany:\t0.5086\tMedium:\t0.2523\tFew:\t0.0350\n    > [Stats]\tMany:\t0.5086\tMedium:\t0.2523\tFew:\t0.0350\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.3600699169255623\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [131 | 200]\n---> Epoch: [131 | 200]\n---> Epoch: [131 | 200]\n    > [Train]\tLoss:\t0.3044\n    > [Train]\tLoss:\t0.3044\n    > [Train]\tLoss:\t0.3044\n    > [Test ]\tLoss:\t4.8270\tAcc:\t31.1300\n    > [Test ]\tLoss:\t4.8270\tAcc:\t31.1300\n    > [Test ]\tLoss:\t4.8270\tAcc:\t31.1300\n    > [Stats]\tMany:\t0.5360\tMedium:\t0.3109\tFew:\t0.0497\n    > [Stats]\tMany:\t0.5360\tMedium:\t0.3109\tFew:\t0.0497\n    > [Stats]\tMany:\t0.5360\tMedium:\t0.3109\tFew:\t0.0497\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.3643867934720295\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [132 | 200]\n---> Epoch: [132 | 200]\n---> Epoch: [132 | 200]\n    > [Train]\tLoss:\t0.3361\n    > [Train]\tLoss:\t0.3361\n    > [Train]\tLoss:\t0.3361\n    > [Test ]\tLoss:\t4.8507\tAcc:\t31.7000\n    > [Test ]\tLoss:\t4.8507\tAcc:\t31.7000\n    > [Test ]\tLoss:\t4.8507\tAcc:\t31.7000\n    > [Stats]\tMany:\t0.5351\tMedium:\t0.3123\tFew:\t0.0680\n    > [Stats]\tMany:\t0.5351\tMedium:\t0.3123\tFew:\t0.0680\n    > [Stats]\tMany:\t0.5351\tMedium:\t0.3123\tFew:\t0.0680\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.330366194953882\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [133 | 200]\n---> Epoch: [133 | 200]\n---> Epoch: [133 | 200]\n    > [Train]\tLoss:\t0.3636\n    > [Train]\tLoss:\t0.3636\n    > [Train]\tLoss:\t0.3636\n    > [Test ]\tLoss:\t4.5166\tAcc:\t32.4800\n    > [Test ]\tLoss:\t4.5166\tAcc:\t32.4800\n    > [Test ]\tLoss:\t4.5166\tAcc:\t32.4800\n    > [Stats]\tMany:\t0.5611\tMedium:\t0.2989\tFew:\t0.0793\n    > [Stats]\tMany:\t0.5611\tMedium:\t0.2989\tFew:\t0.0793\n    > [Stats]\tMany:\t0.5611\tMedium:\t0.2989\tFew:\t0.0793\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.33271168874675\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [134 | 200]\n---> Epoch: [134 | 200]\n---> Epoch: [134 | 200]\n    > [Train]\tLoss:\t0.3853\n    > [Train]\tLoss:\t0.3853\n    > [Train]\tLoss:\t0.3853\n    > [Test ]\tLoss:\t5.8771\tAcc:\t24.1500\n    > [Test ]\tLoss:\t5.8771\tAcc:\t24.1500\n    > [Test ]\tLoss:\t5.8771\tAcc:\t24.1500\n    > [Stats]\tMany:\t0.4683\tMedium:\t0.1886\tFew:\t0.0387\n    > [Stats]\tMany:\t0.4683\tMedium:\t0.1886\tFew:\t0.0387\n    > [Stats]\tMany:\t0.4683\tMedium:\t0.1886\tFew:\t0.0387\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.327671308373242\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [135 | 200]\n---> Epoch: [135 | 200]\n---> Epoch: [135 | 200]\n    > [Train]\tLoss:\t0.3594\n    > [Train]\tLoss:\t0.3594\n    > [Train]\tLoss:\t0.3594\n    > [Test ]\tLoss:\t4.6402\tAcc:\t31.5900\n    > [Test ]\tLoss:\t4.6402\tAcc:\t31.5900\n    > [Test ]\tLoss:\t4.6402\tAcc:\t31.5900\n    > [Stats]\tMany:\t0.5517\tMedium:\t0.3006\tFew:\t0.0587\n    > [Stats]\tMany:\t0.5517\tMedium:\t0.3006\tFew:\t0.0587\n    > [Stats]\tMany:\t0.5517\tMedium:\t0.3006\tFew:\t0.0587\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.3080260968026374\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [136 | 200]\n---> Epoch: [136 | 200]\n---> Epoch: [136 | 200]\n    > [Train]\tLoss:\t0.3341\n    > [Train]\tLoss:\t0.3341\n    > [Train]\tLoss:\t0.3341\n    > [Test ]\tLoss:\t4.4439\tAcc:\t30.8200\n    > [Test ]\tLoss:\t4.4439\tAcc:\t30.8200\n    > [Test ]\tLoss:\t4.4439\tAcc:\t30.8200\n    > [Stats]\tMany:\t0.5580\tMedium:\t0.2649\tFew:\t0.0673\n    > [Stats]\tMany:\t0.5580\tMedium:\t0.2649\tFew:\t0.0673\n    > [Stats]\tMany:\t0.5580\tMedium:\t0.2649\tFew:\t0.0673\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.325642134977182\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [137 | 200]\n---> Epoch: [137 | 200]\n---> Epoch: [137 | 200]\n    > [Train]\tLoss:\t0.3297\n    > [Train]\tLoss:\t0.3297\n    > [Train]\tLoss:\t0.3297\n    > [Test ]\tLoss:\t5.0539\tAcc:\t30.1500\n    > [Test ]\tLoss:\t5.0539\tAcc:\t30.1500\n    > [Test ]\tLoss:\t5.0539\tAcc:\t30.1500\n    > [Stats]\tMany:\t0.5460\tMedium:\t0.2671\tFew:\t0.0563\n    > [Stats]\tMany:\t0.5460\tMedium:\t0.2671\tFew:\t0.0563\n    > [Stats]\tMany:\t0.5460\tMedium:\t0.2671\tFew:\t0.0563\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.3247150952350304\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [138 | 200]\n---> Epoch: [138 | 200]\n---> Epoch: [138 | 200]\n    > [Train]\tLoss:\t0.3293\n    > [Train]\tLoss:\t0.3293\n    > [Train]\tLoss:\t0.3293\n    > [Test ]\tLoss:\t4.7420\tAcc:\t30.0000\n    > [Test ]\tLoss:\t4.7420\tAcc:\t30.0000\n    > [Test ]\tLoss:\t4.7420\tAcc:\t30.0000\n    > [Stats]\tMany:\t0.5389\tMedium:\t0.2729\tFew:\t0.0530\n    > [Stats]\tMany:\t0.5389\tMedium:\t0.2729\tFew:\t0.0530\n    > [Stats]\tMany:\t0.5389\tMedium:\t0.2729\tFew:\t0.0530\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.3275020679802445\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [139 | 200]\n---> Epoch: [139 | 200]\n---> Epoch: [139 | 200]\n    > [Train]\tLoss:\t0.3320\n    > [Train]\tLoss:\t0.3320\n    > [Train]\tLoss:\t0.3320\n    > [Test ]\tLoss:\t4.9195\tAcc:\t28.9200\n    > [Test ]\tLoss:\t4.9195\tAcc:\t28.9200\n    > [Test ]\tLoss:\t4.9195\tAcc:\t28.9200\n    > [Stats]\tMany:\t0.5177\tMedium:\t0.2586\tFew:\t0.0583\n    > [Stats]\tMany:\t0.5177\tMedium:\t0.2586\tFew:\t0.0583\n    > [Stats]\tMany:\t0.5177\tMedium:\t0.2586\tFew:\t0.0583\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.322832855733423\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [140 | 200]\n---> Epoch: [140 | 200]\n---> Epoch: [140 | 200]\n    > [Train]\tLoss:\t0.3379\n    > [Train]\tLoss:\t0.3379\n    > [Train]\tLoss:\t0.3379\n    > [Test ]\tLoss:\t4.5550\tAcc:\t32.2100\n    > [Test ]\tLoss:\t4.5550\tAcc:\t32.2100\n    > [Test ]\tLoss:\t4.5550\tAcc:\t32.2100\n    > [Stats]\tMany:\t0.5709\tMedium:\t0.3131\tFew:\t0.0423\n    > [Stats]\tMany:\t0.5709\tMedium:\t0.3131\tFew:\t0.0423\n    > [Stats]\tMany:\t0.5709\tMedium:\t0.3131\tFew:\t0.0423\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.3055996752343106\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [141 | 200]\n---> Epoch: [141 | 200]\n---> Epoch: [141 | 200]\n    > [Train]\tLoss:\t0.3237\n    > [Train]\tLoss:\t0.3237\n    > [Train]\tLoss:\t0.3237\n    > [Test ]\tLoss:\t4.9094\tAcc:\t30.3700\n    > [Test ]\tLoss:\t4.9094\tAcc:\t30.3700\n    > [Test ]\tLoss:\t4.9094\tAcc:\t30.3700\n    > [Stats]\tMany:\t0.5686\tMedium:\t0.2477\tFew:\t0.0600\n    > [Stats]\tMany:\t0.5686\tMedium:\t0.2477\tFew:\t0.0600\n    > [Stats]\tMany:\t0.5686\tMedium:\t0.2477\tFew:\t0.0600\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.2904690379378416\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [142 | 200]\n---> Epoch: [142 | 200]\n---> Epoch: [142 | 200]\n    > [Train]\tLoss:\t0.3492\n    > [Train]\tLoss:\t0.3492\n    > [Train]\tLoss:\t0.3492\n    > [Test ]\tLoss:\t5.0516\tAcc:\t29.8400\n    > [Test ]\tLoss:\t5.0516\tAcc:\t29.8400\n    > [Test ]\tLoss:\t5.0516\tAcc:\t29.8400\n    > [Stats]\tMany:\t0.5294\tMedium:\t0.2680\tFew:\t0.0643\n    > [Stats]\tMany:\t0.5294\tMedium:\t0.2680\tFew:\t0.0643\n    > [Stats]\tMany:\t0.5294\tMedium:\t0.2680\tFew:\t0.0643\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.2764451817328033\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [143 | 200]\n---> Epoch: [143 | 200]\n---> Epoch: [143 | 200]\n    > [Train]\tLoss:\t0.3317\n    > [Train]\tLoss:\t0.3317\n    > [Train]\tLoss:\t0.3317\n    > [Test ]\tLoss:\t5.1723\tAcc:\t29.6700\n    > [Test ]\tLoss:\t5.1723\tAcc:\t29.6700\n    > [Test ]\tLoss:\t5.1723\tAcc:\t29.6700\n    > [Stats]\tMany:\t0.5517\tMedium:\t0.2431\tFew:\t0.0617\n    > [Stats]\tMany:\t0.5517\tMedium:\t0.2431\tFew:\t0.0617\n    > [Stats]\tMany:\t0.5517\tMedium:\t0.2431\tFew:\t0.0617\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.2552676610674425\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [144 | 200]\n---> Epoch: [144 | 200]\n---> Epoch: [144 | 200]\n    > [Train]\tLoss:\t0.3702\n    > [Train]\tLoss:\t0.3702\n    > [Train]\tLoss:\t0.3702\n    > [Test ]\tLoss:\t4.3511\tAcc:\t31.1200\n    > [Test ]\tLoss:\t4.3511\tAcc:\t31.1200\n    > [Test ]\tLoss:\t4.3511\tAcc:\t31.1200\n    > [Stats]\tMany:\t0.5140\tMedium:\t0.3231\tFew:\t0.0607\n    > [Stats]\tMany:\t0.5140\tMedium:\t0.3231\tFew:\t0.0607\n    > [Stats]\tMany:\t0.5140\tMedium:\t0.3231\tFew:\t0.0607\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.268707797234289\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [145 | 200]\n---> Epoch: [145 | 200]\n---> Epoch: [145 | 200]\n    > [Train]\tLoss:\t0.3290\n    > [Train]\tLoss:\t0.3290\n    > [Train]\tLoss:\t0.3290\n    > [Test ]\tLoss:\t4.8739\tAcc:\t31.7000\n    > [Test ]\tLoss:\t4.8739\tAcc:\t31.7000\n    > [Test ]\tLoss:\t4.8739\tAcc:\t31.7000\n    > [Stats]\tMany:\t0.5711\tMedium:\t0.2834\tFew:\t0.0597\n    > [Stats]\tMany:\t0.5711\tMedium:\t0.2834\tFew:\t0.0597\n    > [Stats]\tMany:\t0.5711\tMedium:\t0.2834\tFew:\t0.0597\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.269740951047754\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [146 | 200]\n---> Epoch: [146 | 200]\n---> Epoch: [146 | 200]\n    > [Train]\tLoss:\t0.3172\n    > [Train]\tLoss:\t0.3172\n    > [Train]\tLoss:\t0.3172\n    > [Test ]\tLoss:\t4.9182\tAcc:\t31.4800\n    > [Test ]\tLoss:\t4.9182\tAcc:\t31.4800\n    > [Test ]\tLoss:\t4.9182\tAcc:\t31.4800\n    > [Stats]\tMany:\t0.5780\tMedium:\t0.2666\tFew:\t0.0640\n    > [Stats]\tMany:\t0.5780\tMedium:\t0.2666\tFew:\t0.0640\n    > [Stats]\tMany:\t0.5780\tMedium:\t0.2666\tFew:\t0.0640\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.2727137811197746\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [147 | 200]\n---> Epoch: [147 | 200]\n---> Epoch: [147 | 200]\n    > [Train]\tLoss:\t0.3296\n    > [Train]\tLoss:\t0.3296\n    > [Train]\tLoss:\t0.3296\n    > [Test ]\tLoss:\t5.1452\tAcc:\t28.3500\n    > [Test ]\tLoss:\t5.1452\tAcc:\t28.3500\n    > [Test ]\tLoss:\t5.1452\tAcc:\t28.3500\n    > [Stats]\tMany:\t0.4926\tMedium:\t0.2866\tFew:\t0.0360\n    > [Stats]\tMany:\t0.4926\tMedium:\t0.2866\tFew:\t0.0360\n    > [Stats]\tMany:\t0.4926\tMedium:\t0.2866\tFew:\t0.0360\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.271347777561638\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [148 | 200]\n---> Epoch: [148 | 200]\n---> Epoch: [148 | 200]\n    > [Train]\tLoss:\t0.2817\n    > [Train]\tLoss:\t0.2817\n    > [Train]\tLoss:\t0.2817\n    > [Test ]\tLoss:\t4.7469\tAcc:\t31.2600\n    > [Test ]\tLoss:\t4.7469\tAcc:\t31.2600\n    > [Test ]\tLoss:\t4.7469\tAcc:\t31.2600\n    > [Stats]\tMany:\t0.5563\tMedium:\t0.2889\tFew:\t0.0560\n    > [Stats]\tMany:\t0.5563\tMedium:\t0.2889\tFew:\t0.0560\n    > [Stats]\tMany:\t0.5563\tMedium:\t0.2889\tFew:\t0.0560\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.2808270617758573\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [149 | 200]\n---> Epoch: [149 | 200]\n---> Epoch: [149 | 200]\n    > [Train]\tLoss:\t0.2989\n    > [Train]\tLoss:\t0.2989\n    > [Train]\tLoss:\t0.2989\n    > [Test ]\tLoss:\t4.9797\tAcc:\t28.6300\n    > [Test ]\tLoss:\t4.9797\tAcc:\t28.6300\n    > [Test ]\tLoss:\t4.9797\tAcc:\t28.6300\n    > [Stats]\tMany:\t0.5223\tMedium:\t0.2314\tFew:\t0.0750\n    > [Stats]\tMany:\t0.5223\tMedium:\t0.2314\tFew:\t0.0750\n    > [Stats]\tMany:\t0.5223\tMedium:\t0.2314\tFew:\t0.0750\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.2646374002100385\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [150 | 200]\n---> Epoch: [150 | 200]\n---> Epoch: [150 | 200]\n    > [Train]\tLoss:\t0.3450\n    > [Train]\tLoss:\t0.3450\n    > [Train]\tLoss:\t0.3450\n    > [Test ]\tLoss:\t4.9967\tAcc:\t28.7100\n    > [Test ]\tLoss:\t4.9967\tAcc:\t28.7100\n    > [Test ]\tLoss:\t4.9967\tAcc:\t28.7100\n    > [Stats]\tMany:\t0.4934\tMedium:\t0.2843\tFew:\t0.0497\n    > [Stats]\tMany:\t0.4934\tMedium:\t0.2843\tFew:\t0.0497\n    > [Stats]\tMany:\t0.4934\tMedium:\t0.2843\tFew:\t0.0497\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Best ]\tAcc:\t32.9800\tMany:\t54.2000\tMedium:\t34.7714\tFew:\t6.1333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.257924869942538\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [151 | 200]\n---> Epoch: [151 | 200]\n---> Epoch: [151 | 200]\n    > [Train]\tLoss:\t0.3639\n    > [Train]\tLoss:\t0.3639\n    > [Train]\tLoss:\t0.3639\n    > [Test ]\tLoss:\t4.4385\tAcc:\t33.2400\n    > [Test ]\tLoss:\t4.4385\tAcc:\t33.2400\n    > [Test ]\tLoss:\t4.4385\tAcc:\t33.2400\n    > [Stats]\tMany:\t0.5831\tMedium:\t0.3086\tFew:\t0.0677\n    > [Stats]\tMany:\t0.5831\tMedium:\t0.3086\tFew:\t0.0677\n    > [Stats]\tMany:\t0.5831\tMedium:\t0.3086\tFew:\t0.0677\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.2411677861430843\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [152 | 200]\n---> Epoch: [152 | 200]\n---> Epoch: [152 | 200]\n    > [Train]\tLoss:\t0.3167\n    > [Train]\tLoss:\t0.3167\n    > [Train]\tLoss:\t0.3167\n    > [Test ]\tLoss:\t5.3078\tAcc:\t27.1000\n    > [Test ]\tLoss:\t5.3078\tAcc:\t27.1000\n    > [Test ]\tLoss:\t5.3078\tAcc:\t27.1000\n    > [Stats]\tMany:\t0.4937\tMedium:\t0.2294\tFew:\t0.0597\n    > [Stats]\tMany:\t0.4937\tMedium:\t0.2294\tFew:\t0.0597\n    > [Stats]\tMany:\t0.4937\tMedium:\t0.2294\tFew:\t0.0597\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.2477087824394335\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [153 | 200]\n---> Epoch: [153 | 200]\n---> Epoch: [153 | 200]\n    > [Train]\tLoss:\t0.2892\n    > [Train]\tLoss:\t0.2892\n    > [Train]\tLoss:\t0.2892\n    > [Test ]\tLoss:\t4.8662\tAcc:\t31.4300\n    > [Test ]\tLoss:\t4.8662\tAcc:\t31.4300\n    > [Test ]\tLoss:\t4.8662\tAcc:\t31.4300\n    > [Stats]\tMany:\t0.5500\tMedium:\t0.2883\tFew:\t0.0697\n    > [Stats]\tMany:\t0.5500\tMedium:\t0.2883\tFew:\t0.0697\n    > [Stats]\tMany:\t0.5500\tMedium:\t0.2883\tFew:\t0.0697\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.239822465331932\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [154 | 200]\n---> Epoch: [154 | 200]\n---> Epoch: [154 | 200]\n    > [Train]\tLoss:\t0.3041\n    > [Train]\tLoss:\t0.3041\n    > [Train]\tLoss:\t0.3041\n    > [Test ]\tLoss:\t4.9106\tAcc:\t31.2000\n    > [Test ]\tLoss:\t4.9106\tAcc:\t31.2000\n    > [Test ]\tLoss:\t4.9106\tAcc:\t31.2000\n    > [Stats]\tMany:\t0.5663\tMedium:\t0.2831\tFew:\t0.0490\n    > [Stats]\tMany:\t0.5663\tMedium:\t0.2831\tFew:\t0.0490\n    > [Stats]\tMany:\t0.5663\tMedium:\t0.2831\tFew:\t0.0490\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.257559378913828\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [155 | 200]\n---> Epoch: [155 | 200]\n---> Epoch: [155 | 200]\n    > [Train]\tLoss:\t0.3298\n    > [Train]\tLoss:\t0.3298\n    > [Train]\tLoss:\t0.3298\n    > [Test ]\tLoss:\t5.0821\tAcc:\t31.2400\n    > [Test ]\tLoss:\t5.0821\tAcc:\t31.2400\n    > [Test ]\tLoss:\t5.0821\tAcc:\t31.2400\n    > [Stats]\tMany:\t0.5649\tMedium:\t0.2871\tFew:\t0.0473\n    > [Stats]\tMany:\t0.5649\tMedium:\t0.2871\tFew:\t0.0473\n    > [Stats]\tMany:\t0.5649\tMedium:\t0.2871\tFew:\t0.0473\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.221959217176187\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [156 | 200]\n---> Epoch: [156 | 200]\n---> Epoch: [156 | 200]\n    > [Train]\tLoss:\t0.3398\n    > [Train]\tLoss:\t0.3398\n    > [Train]\tLoss:\t0.3398\n    > [Test ]\tLoss:\t4.4253\tAcc:\t30.7600\n    > [Test ]\tLoss:\t4.4253\tAcc:\t30.7600\n    > [Test ]\tLoss:\t4.4253\tAcc:\t30.7600\n    > [Stats]\tMany:\t0.5586\tMedium:\t0.2771\tFew:\t0.0503\n    > [Stats]\tMany:\t0.5586\tMedium:\t0.2771\tFew:\t0.0503\n    > [Stats]\tMany:\t0.5586\tMedium:\t0.2771\tFew:\t0.0503\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.237034118918976\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [157 | 200]\n---> Epoch: [157 | 200]\n---> Epoch: [157 | 200]\n    > [Train]\tLoss:\t0.3189\n    > [Train]\tLoss:\t0.3189\n    > [Train]\tLoss:\t0.3189\n    > [Test ]\tLoss:\t5.1075\tAcc:\t30.8700\n    > [Test ]\tLoss:\t5.1075\tAcc:\t30.8700\n    > [Test ]\tLoss:\t5.1075\tAcc:\t30.8700\n    > [Stats]\tMany:\t0.5520\tMedium:\t0.2886\tFew:\t0.0483\n    > [Stats]\tMany:\t0.5520\tMedium:\t0.2886\tFew:\t0.0483\n    > [Stats]\tMany:\t0.5520\tMedium:\t0.2886\tFew:\t0.0483\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.242771991199815\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [158 | 200]\n---> Epoch: [158 | 200]\n---> Epoch: [158 | 200]\n    > [Train]\tLoss:\t0.3266\n    > [Train]\tLoss:\t0.3266\n    > [Train]\tLoss:\t0.3266\n    > [Test ]\tLoss:\t5.3671\tAcc:\t28.7000\n    > [Test ]\tLoss:\t5.3671\tAcc:\t28.7000\n    > [Test ]\tLoss:\t5.3671\tAcc:\t28.7000\n    > [Stats]\tMany:\t0.5366\tMedium:\t0.2431\tFew:\t0.0470\n    > [Stats]\tMany:\t0.5366\tMedium:\t0.2431\tFew:\t0.0470\n    > [Stats]\tMany:\t0.5366\tMedium:\t0.2431\tFew:\t0.0470\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.2558438194406603\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [159 | 200]\n---> Epoch: [159 | 200]\n---> Epoch: [159 | 200]\n    > [Train]\tLoss:\t0.3591\n    > [Train]\tLoss:\t0.3591\n    > [Train]\tLoss:\t0.3591\n    > [Test ]\tLoss:\t4.8734\tAcc:\t29.7100\n    > [Test ]\tLoss:\t4.8734\tAcc:\t29.7100\n    > [Test ]\tLoss:\t4.8734\tAcc:\t29.7100\n    > [Stats]\tMany:\t0.5097\tMedium:\t0.2951\tFew:\t0.0513\n    > [Stats]\tMany:\t0.5097\tMedium:\t0.2951\tFew:\t0.0513\n    > [Stats]\tMany:\t0.5097\tMedium:\t0.2951\tFew:\t0.0513\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.2773153625431823\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [160 | 200]\n---> Epoch: [160 | 200]\n---> Epoch: [160 | 200]\n    > [Train]\tLoss:\t0.3429\n    > [Train]\tLoss:\t0.3429\n    > [Train]\tLoss:\t0.3429\n    > [Test ]\tLoss:\t5.0793\tAcc:\t30.1000\n    > [Test ]\tLoss:\t5.0793\tAcc:\t30.1000\n    > [Test ]\tLoss:\t5.0793\tAcc:\t30.1000\n    > [Stats]\tMany:\t0.5591\tMedium:\t0.2583\tFew:\t0.0497\n    > [Stats]\tMany:\t0.5591\tMedium:\t0.2583\tFew:\t0.0497\n    > [Stats]\tMany:\t0.5591\tMedium:\t0.2583\tFew:\t0.0497\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Best ]\tAcc:\t33.2400\tMany:\t58.3143\tMedium:\t30.8571\tFew:\t6.7667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.237804879615698\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [161 | 200]\n---> Epoch: [161 | 200]\n---> Epoch: [161 | 200]\n    > [Train]\tLoss:\t0.2368\n    > [Train]\tLoss:\t0.2368\n    > [Train]\tLoss:\t0.2368\n    > [Test ]\tLoss:\t4.1676\tAcc:\t35.4800\n    > [Test ]\tLoss:\t4.1676\tAcc:\t35.4800\n    > [Test ]\tLoss:\t4.1676\tAcc:\t35.4800\n    > [Stats]\tMany:\t0.6323\tMedium:\t0.3231\tFew:\t0.0680\n    > [Stats]\tMany:\t0.6323\tMedium:\t0.3231\tFew:\t0.0680\n    > [Stats]\tMany:\t0.6323\tMedium:\t0.3231\tFew:\t0.0680\n    > [Best ]\tAcc:\t35.4800\tMany:\t63.2286\tMedium:\t32.3143\tFew:\t6.8000\n    > [Best ]\tAcc:\t35.4800\tMany:\t63.2286\tMedium:\t32.3143\tFew:\t6.8000\n    > [Best ]\tAcc:\t35.4800\tMany:\t63.2286\tMedium:\t32.3143\tFew:\t6.8000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.238463283302273\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [162 | 200]\n---> Epoch: [162 | 200]\n---> Epoch: [162 | 200]\n    > [Train]\tLoss:\t0.1797\n    > [Train]\tLoss:\t0.1797\n    > [Train]\tLoss:\t0.1797\n    > [Test ]\tLoss:\t4.0506\tAcc:\t36.0500\n    > [Test ]\tLoss:\t4.0506\tAcc:\t36.0500\n    > [Test ]\tLoss:\t4.0506\tAcc:\t36.0500\n    > [Stats]\tMany:\t0.6400\tMedium:\t0.3306\tFew:\t0.0693\n    > [Stats]\tMany:\t0.6400\tMedium:\t0.3306\tFew:\t0.0693\n    > [Stats]\tMany:\t0.6400\tMedium:\t0.3306\tFew:\t0.0693\n    > [Best ]\tAcc:\t36.0500\tMany:\t64.0000\tMedium:\t33.0571\tFew:\t6.9333\n    > [Best ]\tAcc:\t36.0500\tMany:\t64.0000\tMedium:\t33.0571\tFew:\t6.9333\n    > [Best ]\tAcc:\t36.0500\tMany:\t64.0000\tMedium:\t33.0571\tFew:\t6.9333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2386897985157526\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [163 | 200]\n---> Epoch: [163 | 200]\n---> Epoch: [163 | 200]\n    > [Train]\tLoss:\t0.1498\n    > [Train]\tLoss:\t0.1498\n    > [Train]\tLoss:\t0.1498\n    > [Test ]\tLoss:\t4.0146\tAcc:\t36.3200\n    > [Test ]\tLoss:\t4.0146\tAcc:\t36.3200\n    > [Test ]\tLoss:\t4.0146\tAcc:\t36.3200\n    > [Stats]\tMany:\t0.6394\tMedium:\t0.3366\tFew:\t0.0720\n    > [Stats]\tMany:\t0.6394\tMedium:\t0.3366\tFew:\t0.0720\n    > [Stats]\tMany:\t0.6394\tMedium:\t0.3366\tFew:\t0.0720\n    > [Best ]\tAcc:\t36.3200\tMany:\t63.9429\tMedium:\t33.6571\tFew:\t7.2000\n    > [Best ]\tAcc:\t36.3200\tMany:\t63.9429\tMedium:\t33.6571\tFew:\t7.2000\n    > [Best ]\tAcc:\t36.3200\tMany:\t63.9429\tMedium:\t33.6571\tFew:\t7.2000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2386526483575984\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [164 | 200]\n---> Epoch: [164 | 200]\n---> Epoch: [164 | 200]\n    > [Train]\tLoss:\t0.1378\n    > [Train]\tLoss:\t0.1378\n    > [Train]\tLoss:\t0.1378\n    > [Test ]\tLoss:\t4.0048\tAcc:\t36.6300\n    > [Test ]\tLoss:\t4.0048\tAcc:\t36.6300\n    > [Test ]\tLoss:\t4.0048\tAcc:\t36.6300\n    > [Stats]\tMany:\t0.6437\tMedium:\t0.3409\tFew:\t0.0723\n    > [Stats]\tMany:\t0.6437\tMedium:\t0.3409\tFew:\t0.0723\n    > [Stats]\tMany:\t0.6437\tMedium:\t0.3409\tFew:\t0.0723\n    > [Best ]\tAcc:\t36.6300\tMany:\t64.3714\tMedium:\t34.0857\tFew:\t7.2333\n    > [Best ]\tAcc:\t36.6300\tMany:\t64.3714\tMedium:\t34.0857\tFew:\t7.2333\n    > [Best ]\tAcc:\t36.6300\tMany:\t64.3714\tMedium:\t34.0857\tFew:\t7.2333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2389055442938943\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [165 | 200]\n---> Epoch: [165 | 200]\n---> Epoch: [165 | 200]\n    > [Train]\tLoss:\t0.1215\n    > [Train]\tLoss:\t0.1215\n    > [Train]\tLoss:\t0.1215\n    > [Test ]\tLoss:\t3.9804\tAcc:\t36.8700\n    > [Test ]\tLoss:\t3.9804\tAcc:\t36.8700\n    > [Test ]\tLoss:\t3.9804\tAcc:\t36.8700\n    > [Stats]\tMany:\t0.6491\tMedium:\t0.3420\tFew:\t0.0727\n    > [Stats]\tMany:\t0.6491\tMedium:\t0.3420\tFew:\t0.0727\n    > [Stats]\tMany:\t0.6491\tMedium:\t0.3420\tFew:\t0.0727\n    > [Best ]\tAcc:\t36.8700\tMany:\t64.9143\tMedium:\t34.2000\tFew:\t7.2667\n    > [Best ]\tAcc:\t36.8700\tMany:\t64.9143\tMedium:\t34.2000\tFew:\t7.2667\n    > [Best ]\tAcc:\t36.8700\tMany:\t64.9143\tMedium:\t34.2000\tFew:\t7.2667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2391674178688823\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [166 | 200]\n---> Epoch: [166 | 200]\n---> Epoch: [166 | 200]\n    > [Train]\tLoss:\t0.1163\n    > [Train]\tLoss:\t0.1163\n    > [Train]\tLoss:\t0.1163\n    > [Test ]\tLoss:\t3.9233\tAcc:\t36.9400\n    > [Test ]\tLoss:\t3.9233\tAcc:\t36.9400\n    > [Test ]\tLoss:\t3.9233\tAcc:\t36.9400\n    > [Stats]\tMany:\t0.6440\tMedium:\t0.3483\tFew:\t0.0737\n    > [Stats]\tMany:\t0.6440\tMedium:\t0.3483\tFew:\t0.0737\n    > [Stats]\tMany:\t0.6440\tMedium:\t0.3483\tFew:\t0.0737\n    > [Best ]\tAcc:\t36.9400\tMany:\t64.4000\tMedium:\t34.8286\tFew:\t7.3667\n    > [Best ]\tAcc:\t36.9400\tMany:\t64.4000\tMedium:\t34.8286\tFew:\t7.3667\n    > [Best ]\tAcc:\t36.9400\tMany:\t64.4000\tMedium:\t34.8286\tFew:\t7.3667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2393127324019235\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [167 | 200]\n---> Epoch: [167 | 200]\n---> Epoch: [167 | 200]\n    > [Train]\tLoss:\t0.1121\n    > [Train]\tLoss:\t0.1121\n    > [Train]\tLoss:\t0.1121\n    > [Test ]\tLoss:\t3.9246\tAcc:\t37.0500\n    > [Test ]\tLoss:\t3.9246\tAcc:\t37.0500\n    > [Test ]\tLoss:\t3.9246\tAcc:\t37.0500\n    > [Stats]\tMany:\t0.6466\tMedium:\t0.3500\tFew:\t0.0723\n    > [Stats]\tMany:\t0.6466\tMedium:\t0.3500\tFew:\t0.0723\n    > [Stats]\tMany:\t0.6466\tMedium:\t0.3500\tFew:\t0.0723\n    > [Best ]\tAcc:\t37.0500\tMany:\t64.6571\tMedium:\t35.0000\tFew:\t7.2333\n    > [Best ]\tAcc:\t37.0500\tMany:\t64.6571\tMedium:\t35.0000\tFew:\t7.2333\n    > [Best ]\tAcc:\t37.0500\tMany:\t64.6571\tMedium:\t35.0000\tFew:\t7.2333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.239331898473507\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [168 | 200]\n---> Epoch: [168 | 200]\n---> Epoch: [168 | 200]\n    > [Train]\tLoss:\t0.1030\n    > [Train]\tLoss:\t0.1030\n    > [Train]\tLoss:\t0.1030\n    > [Test ]\tLoss:\t3.9103\tAcc:\t37.2400\n    > [Test ]\tLoss:\t3.9103\tAcc:\t37.2400\n    > [Test ]\tLoss:\t3.9103\tAcc:\t37.2400\n    > [Stats]\tMany:\t0.6483\tMedium:\t0.3526\tFew:\t0.0737\n    > [Stats]\tMany:\t0.6483\tMedium:\t0.3526\tFew:\t0.0737\n    > [Stats]\tMany:\t0.6483\tMedium:\t0.3526\tFew:\t0.0737\n    > [Best ]\tAcc:\t37.2400\tMany:\t64.8286\tMedium:\t35.2571\tFew:\t7.3667\n    > [Best ]\tAcc:\t37.2400\tMany:\t64.8286\tMedium:\t35.2571\tFew:\t7.3667\n    > [Best ]\tAcc:\t37.2400\tMany:\t64.8286\tMedium:\t35.2571\tFew:\t7.3667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2396838374043697\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [169 | 200]\n---> Epoch: [169 | 200]\n---> Epoch: [169 | 200]\n    > [Train]\tLoss:\t0.0960\n    > [Train]\tLoss:\t0.0960\n    > [Train]\tLoss:\t0.0960\n    > [Test ]\tLoss:\t3.9045\tAcc:\t37.1800\n    > [Test ]\tLoss:\t3.9045\tAcc:\t37.1800\n    > [Test ]\tLoss:\t3.9045\tAcc:\t37.1800\n    > [Stats]\tMany:\t0.6451\tMedium:\t0.3503\tFew:\t0.0780\n    > [Stats]\tMany:\t0.6451\tMedium:\t0.3503\tFew:\t0.0780\n    > [Stats]\tMany:\t0.6451\tMedium:\t0.3503\tFew:\t0.0780\n    > [Best ]\tAcc:\t37.2400\tMany:\t64.8286\tMedium:\t35.2571\tFew:\t7.3667\n    > [Best ]\tAcc:\t37.2400\tMany:\t64.8286\tMedium:\t35.2571\tFew:\t7.3667\n    > [Best ]\tAcc:\t37.2400\tMany:\t64.8286\tMedium:\t35.2571\tFew:\t7.3667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2399979470463443\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [170 | 200]\n---> Epoch: [170 | 200]\n---> Epoch: [170 | 200]\n    > [Train]\tLoss:\t0.0948\n    > [Train]\tLoss:\t0.0948\n    > [Train]\tLoss:\t0.0948\n    > [Test ]\tLoss:\t3.8864\tAcc:\t37.3900\n    > [Test ]\tLoss:\t3.8864\tAcc:\t37.3900\n    > [Test ]\tLoss:\t3.8864\tAcc:\t37.3900\n    > [Stats]\tMany:\t0.6480\tMedium:\t0.3560\tFew:\t0.0750\n    > [Stats]\tMany:\t0.6480\tMedium:\t0.3560\tFew:\t0.0750\n    > [Stats]\tMany:\t0.6480\tMedium:\t0.3560\tFew:\t0.0750\n    > [Best ]\tAcc:\t37.3900\tMany:\t64.8000\tMedium:\t35.6000\tFew:\t7.5000\n    > [Best ]\tAcc:\t37.3900\tMany:\t64.8000\tMedium:\t35.6000\tFew:\t7.5000\n    > [Best ]\tAcc:\t37.3900\tMany:\t64.8000\tMedium:\t35.6000\tFew:\t7.5000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.240195246589888\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [171 | 200]\n---> Epoch: [171 | 200]\n---> Epoch: [171 | 200]\n    > [Train]\tLoss:\t0.0933\n    > [Train]\tLoss:\t0.0933\n    > [Train]\tLoss:\t0.0933\n    > [Test ]\tLoss:\t3.9056\tAcc:\t37.3900\n    > [Test ]\tLoss:\t3.9056\tAcc:\t37.3900\n    > [Test ]\tLoss:\t3.9056\tAcc:\t37.3900\n    > [Stats]\tMany:\t0.6477\tMedium:\t0.3569\tFew:\t0.0743\n    > [Stats]\tMany:\t0.6477\tMedium:\t0.3569\tFew:\t0.0743\n    > [Stats]\tMany:\t0.6477\tMedium:\t0.3569\tFew:\t0.0743\n    > [Best ]\tAcc:\t37.3900\tMany:\t64.7714\tMedium:\t35.6857\tFew:\t7.4333\n    > [Best ]\tAcc:\t37.3900\tMany:\t64.7714\tMedium:\t35.6857\tFew:\t7.4333\n    > [Best ]\tAcc:\t37.3900\tMany:\t64.7714\tMedium:\t35.6857\tFew:\t7.4333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2406731234819466\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [172 | 200]\n---> Epoch: [172 | 200]\n---> Epoch: [172 | 200]\n    > [Train]\tLoss:\t0.0872\n    > [Train]\tLoss:\t0.0872\n    > [Train]\tLoss:\t0.0872\n    > [Test ]\tLoss:\t3.8937\tAcc:\t37.3600\n    > [Test ]\tLoss:\t3.8937\tAcc:\t37.3600\n    > [Test ]\tLoss:\t3.8937\tAcc:\t37.3600\n    > [Stats]\tMany:\t0.6454\tMedium:\t0.3566\tFew:\t0.0763\n    > [Stats]\tMany:\t0.6454\tMedium:\t0.3566\tFew:\t0.0763\n    > [Stats]\tMany:\t0.6454\tMedium:\t0.3566\tFew:\t0.0763\n    > [Best ]\tAcc:\t37.3900\tMany:\t64.7714\tMedium:\t35.6857\tFew:\t7.4333\n    > [Best ]\tAcc:\t37.3900\tMany:\t64.7714\tMedium:\t35.6857\tFew:\t7.4333\n    > [Best ]\tAcc:\t37.3900\tMany:\t64.7714\tMedium:\t35.6857\tFew:\t7.4333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2411670019570518\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [173 | 200]\n---> Epoch: [173 | 200]\n---> Epoch: [173 | 200]\n    > [Train]\tLoss:\t0.0853\n    > [Train]\tLoss:\t0.0853\n    > [Train]\tLoss:\t0.0853\n    > [Test ]\tLoss:\t3.9005\tAcc:\t37.5700\n    > [Test ]\tLoss:\t3.9005\tAcc:\t37.5700\n    > [Test ]\tLoss:\t3.9005\tAcc:\t37.5700\n    > [Stats]\tMany:\t0.6500\tMedium:\t0.3577\tFew:\t0.0767\n    > [Stats]\tMany:\t0.6500\tMedium:\t0.3577\tFew:\t0.0767\n    > [Stats]\tMany:\t0.6500\tMedium:\t0.3577\tFew:\t0.0767\n    > [Best ]\tAcc:\t37.5700\tMany:\t65.0000\tMedium:\t35.7714\tFew:\t7.6667\n    > [Best ]\tAcc:\t37.5700\tMany:\t65.0000\tMedium:\t35.7714\tFew:\t7.6667\n    > [Best ]\tAcc:\t37.5700\tMany:\t65.0000\tMedium:\t35.7714\tFew:\t7.6667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2412116758175538\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [174 | 200]\n---> Epoch: [174 | 200]\n---> Epoch: [174 | 200]\n    > [Train]\tLoss:\t0.0845\n    > [Train]\tLoss:\t0.0845\n    > [Train]\tLoss:\t0.0845\n    > [Test ]\tLoss:\t3.8635\tAcc:\t37.6200\n    > [Test ]\tLoss:\t3.8635\tAcc:\t37.6200\n    > [Test ]\tLoss:\t3.8635\tAcc:\t37.6200\n    > [Stats]\tMany:\t0.6511\tMedium:\t0.3583\tFew:\t0.0763\n    > [Stats]\tMany:\t0.6511\tMedium:\t0.3583\tFew:\t0.0763\n    > [Stats]\tMany:\t0.6511\tMedium:\t0.3583\tFew:\t0.0763\n    > [Best ]\tAcc:\t37.6200\tMany:\t65.1143\tMedium:\t35.8286\tFew:\t7.6333\n    > [Best ]\tAcc:\t37.6200\tMany:\t65.1143\tMedium:\t35.8286\tFew:\t7.6333\n    > [Best ]\tAcc:\t37.6200\tMany:\t65.1143\tMedium:\t35.8286\tFew:\t7.6333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2415202202428692\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [175 | 200]\n---> Epoch: [175 | 200]\n---> Epoch: [175 | 200]\n    > [Train]\tLoss:\t0.0808\n    > [Train]\tLoss:\t0.0808\n    > [Train]\tLoss:\t0.0808\n    > [Test ]\tLoss:\t3.8676\tAcc:\t37.7600\n    > [Test ]\tLoss:\t3.8676\tAcc:\t37.7600\n    > [Test ]\tLoss:\t3.8676\tAcc:\t37.7600\n    > [Stats]\tMany:\t0.6514\tMedium:\t0.3620\tFew:\t0.0763\n    > [Stats]\tMany:\t0.6514\tMedium:\t0.3620\tFew:\t0.0763\n    > [Stats]\tMany:\t0.6514\tMedium:\t0.3620\tFew:\t0.0763\n    > [Best ]\tAcc:\t37.7600\tMany:\t65.1429\tMedium:\t36.2000\tFew:\t7.6333\n    > [Best ]\tAcc:\t37.7600\tMany:\t65.1429\tMedium:\t36.2000\tFew:\t7.6333\n    > [Best ]\tAcc:\t37.7600\tMany:\t65.1429\tMedium:\t36.2000\tFew:\t7.6333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2416213983393676\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [176 | 200]\n---> Epoch: [176 | 200]\n---> Epoch: [176 | 200]\n    > [Train]\tLoss:\t0.0817\n    > [Train]\tLoss:\t0.0817\n    > [Train]\tLoss:\t0.0817\n    > [Test ]\tLoss:\t3.8775\tAcc:\t37.5300\n    > [Test ]\tLoss:\t3.8775\tAcc:\t37.5300\n    > [Test ]\tLoss:\t3.8775\tAcc:\t37.5300\n    > [Stats]\tMany:\t0.6486\tMedium:\t0.3591\tFew:\t0.0753\n    > [Stats]\tMany:\t0.6486\tMedium:\t0.3591\tFew:\t0.0753\n    > [Stats]\tMany:\t0.6486\tMedium:\t0.3591\tFew:\t0.0753\n    > [Best ]\tAcc:\t37.7600\tMany:\t65.1429\tMedium:\t36.2000\tFew:\t7.6333\n    > [Best ]\tAcc:\t37.7600\tMany:\t65.1429\tMedium:\t36.2000\tFew:\t7.6333\n    > [Best ]\tAcc:\t37.7600\tMany:\t65.1429\tMedium:\t36.2000\tFew:\t7.6333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.241721796919973\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [177 | 200]\n---> Epoch: [177 | 200]\n---> Epoch: [177 | 200]\n    > [Train]\tLoss:\t0.0765\n    > [Train]\tLoss:\t0.0765\n    > [Train]\tLoss:\t0.0765\n    > [Test ]\tLoss:\t3.8515\tAcc:\t37.7400\n    > [Test ]\tLoss:\t3.8515\tAcc:\t37.7400\n    > [Test ]\tLoss:\t3.8515\tAcc:\t37.7400\n    > [Stats]\tMany:\t0.6509\tMedium:\t0.3609\tFew:\t0.0777\n    > [Stats]\tMany:\t0.6509\tMedium:\t0.3609\tFew:\t0.0777\n    > [Stats]\tMany:\t0.6509\tMedium:\t0.3609\tFew:\t0.0777\n    > [Best ]\tAcc:\t37.7600\tMany:\t65.1429\tMedium:\t36.2000\tFew:\t7.6333\n    > [Best ]\tAcc:\t37.7600\tMany:\t65.1429\tMedium:\t36.2000\tFew:\t7.6333\n    > [Best ]\tAcc:\t37.7600\tMany:\t65.1429\tMedium:\t36.2000\tFew:\t7.6333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2419702179169154\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [178 | 200]\n---> Epoch: [178 | 200]\n---> Epoch: [178 | 200]\n    > [Train]\tLoss:\t0.0755\n    > [Train]\tLoss:\t0.0755\n    > [Train]\tLoss:\t0.0755\n    > [Test ]\tLoss:\t3.8398\tAcc:\t37.8600\n    > [Test ]\tLoss:\t3.8398\tAcc:\t37.8600\n    > [Test ]\tLoss:\t3.8398\tAcc:\t37.8600\n    > [Stats]\tMany:\t0.6520\tMedium:\t0.3629\tFew:\t0.0780\n    > [Stats]\tMany:\t0.6520\tMedium:\t0.3629\tFew:\t0.0780\n    > [Stats]\tMany:\t0.6520\tMedium:\t0.3629\tFew:\t0.0780\n    > [Best ]\tAcc:\t37.8600\tMany:\t65.2000\tMedium:\t36.2857\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.8600\tMany:\t65.2000\tMedium:\t36.2857\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.8600\tMany:\t65.2000\tMedium:\t36.2857\tFew:\t7.8000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2423817170455003\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [179 | 200]\n---> Epoch: [179 | 200]\n---> Epoch: [179 | 200]\n    > [Train]\tLoss:\t0.0735\n    > [Train]\tLoss:\t0.0735\n    > [Train]\tLoss:\t0.0735\n    > [Test ]\tLoss:\t3.8522\tAcc:\t37.9200\n    > [Test ]\tLoss:\t3.8522\tAcc:\t37.9200\n    > [Test ]\tLoss:\t3.8522\tAcc:\t37.9200\n    > [Stats]\tMany:\t0.6514\tMedium:\t0.3651\tFew:\t0.0780\n    > [Stats]\tMany:\t0.6514\tMedium:\t0.3651\tFew:\t0.0780\n    > [Stats]\tMany:\t0.6514\tMedium:\t0.3651\tFew:\t0.0780\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.2426362118768575\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [180 | 200]\n---> Epoch: [180 | 200]\n---> Epoch: [180 | 200]\n    > [Train]\tLoss:\t0.0698\n    > [Train]\tLoss:\t0.0698\n    > [Train]\tLoss:\t0.0698\n    > [Test ]\tLoss:\t3.8678\tAcc:\t37.8500\n    > [Test ]\tLoss:\t3.8678\tAcc:\t37.8500\n    > [Test ]\tLoss:\t3.8678\tAcc:\t37.8500\n    > [Stats]\tMany:\t0.6509\tMedium:\t0.3629\tFew:\t0.0790\n    > [Stats]\tMany:\t0.6509\tMedium:\t0.3629\tFew:\t0.0790\n    > [Stats]\tMany:\t0.6509\tMedium:\t0.3629\tFew:\t0.0790\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"3.242621545503344\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [181 | 200]\n---> Epoch: [181 | 200]\n---> Epoch: [181 | 200]\n    > [Train]\tLoss:\t0.0722\n    > [Train]\tLoss:\t0.0722\n    > [Train]\tLoss:\t0.0722\n    > [Test ]\tLoss:\t3.8575\tAcc:\t37.7400\n    > [Test ]\tLoss:\t3.8575\tAcc:\t37.7400\n    > [Test ]\tLoss:\t3.8575\tAcc:\t37.7400\n    > [Stats]\tMany:\t0.6491\tMedium:\t0.3643\tFew:\t0.0757\n    > [Stats]\tMany:\t0.6491\tMedium:\t0.3643\tFew:\t0.0757\n    > [Stats]\tMany:\t0.6491\tMedium:\t0.3643\tFew:\t0.0757\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.242620968602477\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [182 | 200]\n---> Epoch: [182 | 200]\n---> Epoch: [182 | 200]\n    > [Train]\tLoss:\t0.0699\n    > [Train]\tLoss:\t0.0699\n    > [Train]\tLoss:\t0.0699\n    > [Test ]\tLoss:\t3.8472\tAcc:\t37.6200\n    > [Test ]\tLoss:\t3.8472\tAcc:\t37.6200\n    > [Test ]\tLoss:\t3.8472\tAcc:\t37.6200\n    > [Stats]\tMany:\t0.6480\tMedium:\t0.3617\tFew:\t0.0760\n    > [Stats]\tMany:\t0.6480\tMedium:\t0.3617\tFew:\t0.0760\n    > [Stats]\tMany:\t0.6480\tMedium:\t0.3617\tFew:\t0.0760\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.242621628569601\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [183 | 200]\n---> Epoch: [183 | 200]\n---> Epoch: [183 | 200]\n    > [Train]\tLoss:\t0.0714\n    > [Train]\tLoss:\t0.0714\n    > [Train]\tLoss:\t0.0714\n    > [Test ]\tLoss:\t3.8313\tAcc:\t37.8600\n    > [Test ]\tLoss:\t3.8313\tAcc:\t37.8600\n    > [Test ]\tLoss:\t3.8313\tAcc:\t37.8600\n    > [Stats]\tMany:\t0.6506\tMedium:\t0.3646\tFew:\t0.0777\n    > [Stats]\tMany:\t0.6506\tMedium:\t0.3646\tFew:\t0.0777\n    > [Stats]\tMany:\t0.6506\tMedium:\t0.3646\tFew:\t0.0777\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.2426187176467343\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [184 | 200]\n---> Epoch: [184 | 200]\n---> Epoch: [184 | 200]\n    > [Train]\tLoss:\t0.0687\n    > [Train]\tLoss:\t0.0687\n    > [Train]\tLoss:\t0.0687\n    > [Test ]\tLoss:\t3.8513\tAcc:\t37.8800\n    > [Test ]\tLoss:\t3.8513\tAcc:\t37.8800\n    > [Test ]\tLoss:\t3.8513\tAcc:\t37.8800\n    > [Stats]\tMany:\t0.6509\tMedium:\t0.3646\tFew:\t0.0780\n    > [Stats]\tMany:\t0.6509\tMedium:\t0.3646\tFew:\t0.0780\n    > [Stats]\tMany:\t0.6509\tMedium:\t0.3646\tFew:\t0.0780\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.2426206586310182\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [185 | 200]\n---> Epoch: [185 | 200]\n---> Epoch: [185 | 200]\n    > [Train]\tLoss:\t0.0746\n    > [Train]\tLoss:\t0.0746\n    > [Train]\tLoss:\t0.0746\n    > [Test ]\tLoss:\t3.8614\tAcc:\t37.6700\n    > [Test ]\tLoss:\t3.8614\tAcc:\t37.6700\n    > [Test ]\tLoss:\t3.8614\tAcc:\t37.6700\n    > [Stats]\tMany:\t0.6526\tMedium:\t0.3591\tFew:\t0.0753\n    > [Stats]\tMany:\t0.6526\tMedium:\t0.3591\tFew:\t0.0753\n    > [Stats]\tMany:\t0.6526\tMedium:\t0.3591\tFew:\t0.0753\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.2426224131550825\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [186 | 200]\n---> Epoch: [186 | 200]\n---> Epoch: [186 | 200]\n    > [Train]\tLoss:\t0.0689\n    > [Train]\tLoss:\t0.0689\n    > [Train]\tLoss:\t0.0689\n    > [Test ]\tLoss:\t3.8356\tAcc:\t37.8000\n    > [Test ]\tLoss:\t3.8356\tAcc:\t37.8000\n    > [Test ]\tLoss:\t3.8356\tAcc:\t37.8000\n    > [Stats]\tMany:\t0.6491\tMedium:\t0.3640\tFew:\t0.0780\n    > [Stats]\tMany:\t0.6491\tMedium:\t0.3640\tFew:\t0.0780\n    > [Stats]\tMany:\t0.6491\tMedium:\t0.3640\tFew:\t0.0780\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.2426218962689632\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [187 | 200]\n---> Epoch: [187 | 200]\n---> Epoch: [187 | 200]\n    > [Train]\tLoss:\t0.0731\n    > [Train]\tLoss:\t0.0731\n    > [Train]\tLoss:\t0.0731\n    > [Test ]\tLoss:\t3.8762\tAcc:\t37.6700\n    > [Test ]\tLoss:\t3.8762\tAcc:\t37.6700\n    > [Test ]\tLoss:\t3.8762\tAcc:\t37.6700\n    > [Stats]\tMany:\t0.6503\tMedium:\t0.3603\tFew:\t0.0767\n    > [Stats]\tMany:\t0.6503\tMedium:\t0.3603\tFew:\t0.0767\n    > [Stats]\tMany:\t0.6503\tMedium:\t0.3603\tFew:\t0.0767\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.242626635671196\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [188 | 200]\n---> Epoch: [188 | 200]\n---> Epoch: [188 | 200]\n    > [Train]\tLoss:\t0.0708\n    > [Train]\tLoss:\t0.0708\n    > [Train]\tLoss:\t0.0708\n    > [Test ]\tLoss:\t3.8580\tAcc:\t37.6900\n    > [Test ]\tLoss:\t3.8580\tAcc:\t37.6900\n    > [Test ]\tLoss:\t3.8580\tAcc:\t37.6900\n    > [Stats]\tMany:\t0.6491\tMedium:\t0.3609\tFew:\t0.0780\n    > [Stats]\tMany:\t0.6491\tMedium:\t0.3609\tFew:\t0.0780\n    > [Stats]\tMany:\t0.6491\tMedium:\t0.3609\tFew:\t0.0780\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.242627617156859\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [189 | 200]\n---> Epoch: [189 | 200]\n---> Epoch: [189 | 200]\n    > [Train]\tLoss:\t0.0689\n    > [Train]\tLoss:\t0.0689\n    > [Train]\tLoss:\t0.0689\n    > [Test ]\tLoss:\t3.8424\tAcc:\t37.7500\n    > [Test ]\tLoss:\t3.8424\tAcc:\t37.7500\n    > [Test ]\tLoss:\t3.8424\tAcc:\t37.7500\n    > [Stats]\tMany:\t0.6474\tMedium:\t0.3651\tFew:\t0.0770\n    > [Stats]\tMany:\t0.6474\tMedium:\t0.3651\tFew:\t0.0770\n    > [Stats]\tMany:\t0.6474\tMedium:\t0.3651\tFew:\t0.0770\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.2426302737455948\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [190 | 200]\n---> Epoch: [190 | 200]\n---> Epoch: [190 | 200]\n    > [Train]\tLoss:\t0.0730\n    > [Train]\tLoss:\t0.0730\n    > [Train]\tLoss:\t0.0730\n    > [Test ]\tLoss:\t3.8673\tAcc:\t37.8300\n    > [Test ]\tLoss:\t3.8673\tAcc:\t37.8300\n    > [Test ]\tLoss:\t3.8673\tAcc:\t37.8300\n    > [Stats]\tMany:\t0.6529\tMedium:\t0.3626\tFew:\t0.0763\n    > [Stats]\tMany:\t0.6529\tMedium:\t0.3626\tFew:\t0.0763\n    > [Stats]\tMany:\t0.6529\tMedium:\t0.3626\tFew:\t0.0763\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.2426316116401037\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [191 | 200]\n---> Epoch: [191 | 200]\n---> Epoch: [191 | 200]\n    > [Train]\tLoss:\t0.0749\n    > [Train]\tLoss:\t0.0749\n    > [Train]\tLoss:\t0.0749\n    > [Test ]\tLoss:\t3.8663\tAcc:\t37.7200\n    > [Test ]\tLoss:\t3.8663\tAcc:\t37.7200\n    > [Test ]\tLoss:\t3.8663\tAcc:\t37.7200\n    > [Stats]\tMany:\t0.6497\tMedium:\t0.3606\tFew:\t0.0787\n    > [Stats]\tMany:\t0.6497\tMedium:\t0.3606\tFew:\t0.0787\n    > [Stats]\tMany:\t0.6497\tMedium:\t0.3606\tFew:\t0.0787\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.242634505086712\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [192 | 200]\n---> Epoch: [192 | 200]\n---> Epoch: [192 | 200]\n    > [Train]\tLoss:\t0.0730\n    > [Train]\tLoss:\t0.0730\n    > [Train]\tLoss:\t0.0730\n    > [Test ]\tLoss:\t3.8669\tAcc:\t37.8000\n    > [Test ]\tLoss:\t3.8669\tAcc:\t37.8000\n    > [Test ]\tLoss:\t3.8669\tAcc:\t37.8000\n    > [Stats]\tMany:\t0.6500\tMedium:\t0.3640\tFew:\t0.0770\n    > [Stats]\tMany:\t0.6500\tMedium:\t0.3640\tFew:\t0.0770\n    > [Stats]\tMany:\t0.6500\tMedium:\t0.3640\tFew:\t0.0770\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.242636878507738\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [193 | 200]\n---> Epoch: [193 | 200]\n---> Epoch: [193 | 200]\n    > [Train]\tLoss:\t0.0706\n    > [Train]\tLoss:\t0.0706\n    > [Train]\tLoss:\t0.0706\n    > [Test ]\tLoss:\t3.8508\tAcc:\t37.7600\n    > [Test ]\tLoss:\t3.8508\tAcc:\t37.7600\n    > [Test ]\tLoss:\t3.8508\tAcc:\t37.7600\n    > [Stats]\tMany:\t0.6517\tMedium:\t0.3626\tFew:\t0.0753\n    > [Stats]\tMany:\t0.6517\tMedium:\t0.3626\tFew:\t0.0753\n    > [Stats]\tMany:\t0.6517\tMedium:\t0.3626\tFew:\t0.0753\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.242635578836835\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [194 | 200]\n---> Epoch: [194 | 200]\n---> Epoch: [194 | 200]\n    > [Train]\tLoss:\t0.0701\n    > [Train]\tLoss:\t0.0701\n    > [Train]\tLoss:\t0.0701\n    > [Test ]\tLoss:\t3.8476\tAcc:\t37.6500\n    > [Test ]\tLoss:\t3.8476\tAcc:\t37.6500\n    > [Test ]\tLoss:\t3.8476\tAcc:\t37.6500\n    > [Stats]\tMany:\t0.6489\tMedium:\t0.3623\tFew:\t0.0753\n    > [Stats]\tMany:\t0.6489\tMedium:\t0.3623\tFew:\t0.0753\n    > [Stats]\tMany:\t0.6489\tMedium:\t0.3623\tFew:\t0.0753\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.242639382201525\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [195 | 200]\n---> Epoch: [195 | 200]\n---> Epoch: [195 | 200]\n    > [Train]\tLoss:\t0.0713\n    > [Train]\tLoss:\t0.0713\n    > [Train]\tLoss:\t0.0713\n    > [Test ]\tLoss:\t3.8712\tAcc:\t37.7400\n    > [Test ]\tLoss:\t3.8712\tAcc:\t37.7400\n    > [Test ]\tLoss:\t3.8712\tAcc:\t37.7400\n    > [Stats]\tMany:\t0.6480\tMedium:\t0.3654\tFew:\t0.0757\n    > [Stats]\tMany:\t0.6480\tMedium:\t0.3654\tFew:\t0.0757\n    > [Stats]\tMany:\t0.6480\tMedium:\t0.3654\tFew:\t0.0757\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.242640658706083\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [196 | 200]\n---> Epoch: [196 | 200]\n---> Epoch: [196 | 200]\n    > [Train]\tLoss:\t0.0730\n    > [Train]\tLoss:\t0.0730\n    > [Train]\tLoss:\t0.0730\n    > [Test ]\tLoss:\t3.8508\tAcc:\t37.7700\n    > [Test ]\tLoss:\t3.8508\tAcc:\t37.7700\n    > [Test ]\tLoss:\t3.8508\tAcc:\t37.7700\n    > [Stats]\tMany:\t0.6489\tMedium:\t0.3643\tFew:\t0.0770\n    > [Stats]\tMany:\t0.6489\tMedium:\t0.3643\tFew:\t0.0770\n    > [Stats]\tMany:\t0.6489\tMedium:\t0.3643\tFew:\t0.0770\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.242644941220582\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [197 | 200]\n---> Epoch: [197 | 200]\n---> Epoch: [197 | 200]\n    > [Train]\tLoss:\t0.0709\n    > [Train]\tLoss:\t0.0709\n    > [Train]\tLoss:\t0.0709\n    > [Test ]\tLoss:\t3.8656\tAcc:\t37.7500\n    > [Test ]\tLoss:\t3.8656\tAcc:\t37.7500\n    > [Test ]\tLoss:\t3.8656\tAcc:\t37.7500\n    > [Stats]\tMany:\t0.6526\tMedium:\t0.3603\tFew:\t0.0767\n    > [Stats]\tMany:\t0.6526\tMedium:\t0.3603\tFew:\t0.0767\n    > [Stats]\tMany:\t0.6526\tMedium:\t0.3603\tFew:\t0.0767\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.242646481279862\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [198 | 200]\n---> Epoch: [198 | 200]\n---> Epoch: [198 | 200]\n    > [Train]\tLoss:\t0.0727\n    > [Train]\tLoss:\t0.0727\n    > [Train]\tLoss:\t0.0727\n    > [Test ]\tLoss:\t3.8767\tAcc:\t37.8200\n    > [Test ]\tLoss:\t3.8767\tAcc:\t37.8200\n    > [Test ]\tLoss:\t3.8767\tAcc:\t37.8200\n    > [Stats]\tMany:\t0.6520\tMedium:\t0.3623\tFew:\t0.0773\n    > [Stats]\tMany:\t0.6520\tMedium:\t0.3623\tFew:\t0.0773\n    > [Stats]\tMany:\t0.6520\tMedium:\t0.3623\tFew:\t0.0773\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.2426499070999477\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [199 | 200]\n---> Epoch: [199 | 200]\n---> Epoch: [199 | 200]\n    > [Train]\tLoss:\t0.0701\n    > [Train]\tLoss:\t0.0701\n    > [Train]\tLoss:\t0.0701\n    > [Test ]\tLoss:\t3.8814\tAcc:\t37.7700\n    > [Test ]\tLoss:\t3.8814\tAcc:\t37.7700\n    > [Test ]\tLoss:\t3.8814\tAcc:\t37.7700\n    > [Stats]\tMany:\t0.6497\tMedium:\t0.3623\tFew:\t0.0783\n    > [Stats]\tMany:\t0.6497\tMedium:\t0.3623\tFew:\t0.0783\n    > [Stats]\tMany:\t0.6497\tMedium:\t0.3623\tFew:\t0.0783\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.1429\tMedium:\t36.5143\tFew:\t7.8000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"3.242650938876032\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [200 | 200]\n---> Epoch: [200 | 200]\n---> Epoch: [200 | 200]\n    > [Train]\tLoss:\t0.0696\n    > [Train]\tLoss:\t0.0696\n    > [Train]\tLoss:\t0.0696\n    > [Test ]\tLoss:\t3.8639\tAcc:\t37.9200\n    > [Test ]\tLoss:\t3.8639\tAcc:\t37.9200\n    > [Test ]\tLoss:\t3.8639\tAcc:\t37.9200\n    > [Stats]\tMany:\t0.6534\tMedium:\t0.3629\tFew:\t0.0783\n    > [Stats]\tMany:\t0.6534\tMedium:\t0.3629\tFew:\t0.0783\n    > [Stats]\tMany:\t0.6534\tMedium:\t0.3629\tFew:\t0.0783\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.3429\tMedium:\t36.2857\tFew:\t7.8333\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.3429\tMedium:\t36.2857\tFew:\t7.8333\n    > [Best ]\tAcc:\t37.9200\tMany:\t65.3429\tMedium:\t36.2857\tFew:\t7.8333\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n---> Final performance...\n---> Final performance...\n---> Final performance...\n    > best bAcc (test):\t37.92\n    > best bAcc (test):\t37.92\n    > best bAcc (test):\t37.92\n    > best statistics:\tMany:\t0.6534286141395569\tMed:\t0.3628571033477783\tFew:\t0.07833333313465118\n    > best statistics:\tMany:\t0.6534286141395569\tMed:\t0.3628571033477783\tFew:\t0.07833333313465118\n    > best statistics:\tMany:\t0.6534286141395569\tMed:\t0.3628571033477783\tFew:\t0.07833333313465118\n---> Training Time: 0:14:24.03\n---> Training Time: 0:14:24.03\n---> Training Time: 0:14:24.03\n","output_type":"stream"},{"name":"stdout","text":"3.2426506847258314\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[41], line 157\u001b[0m\n\u001b[1;32m    154\u001b[0m         args\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mmap_save(maps)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[41], line 154\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m args\u001b[38;5;241m.\u001b[39mlogger(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhms_string(end_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m--> 154\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mmap_save(\u001b[43mmaps\u001b[49m)\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'maps' referenced before assignment"],"ename":"UnboundLocalError","evalue":"local variable 'maps' referenced before assignment","output_type":"error"}]},{"cell_type":"code","source":"variance_list","metadata":{"execution":{"iopub.status.busy":"2024-01-24T13:01:21.030796Z","iopub.execute_input":"2024-01-24T13:01:21.031190Z","iopub.status.idle":"2024-01-24T13:01:21.040403Z","shell.execute_reply.started":"2024-01-24T13:01:21.031155Z","shell.execute_reply":"2024-01-24T13:01:21.039454Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"[0.7206641586721775,\n 0.6908714917546606,\n 0.7639585779840647,\n 0.882950800236471,\n 1.102954229553409,\n 1.3442035474762943,\n 1.5875812493301715,\n 1.7908585590534924,\n 2.0174678675243842,\n 2.2054443577891045,\n 2.3803349688898376,\n 2.4914933012592524,\n 2.620005473768445,\n 2.763507438284746,\n 2.7921078828860595,\n 2.9137387889045048,\n 2.9898086741237373,\n 3.0668657325821815,\n 3.1264808905804085,\n 3.201317766251162,\n 3.260583052978525,\n 3.305204783667277,\n 3.3624607537631923,\n 3.3763333957226576,\n 3.423104870601601,\n 3.4391188735864624,\n 3.4605108713528514,\n 3.4895289884008545,\n 3.5311426236772023,\n 3.5509998947610715,\n 3.5672974169616363,\n 3.565970712896374,\n 3.5987983362390956,\n 3.6298011905503027,\n 3.6442152732716186,\n 3.6412157191477506,\n 3.670726285435755,\n 3.7020446009863055,\n 3.68446535338106,\n 3.7135086735208795,\n 3.700135248433819,\n 3.720932482000155,\n 3.716955812553133,\n 3.7129997679340256,\n 3.726978649649172,\n 3.7611647360474576,\n 3.7835408265048165,\n 3.802506372549201,\n 3.815011628574721,\n 3.8081328768725715,\n 3.753635376786268,\n 3.766116192067389,\n 3.766257821126788,\n 3.747176735682484,\n 3.7396291847148895,\n 3.7465952977971964,\n 3.755300882000568,\n 3.7492206456178563,\n 3.7747892318946903,\n 3.7477557799504244,\n 3.7419271280291717,\n 3.7433758796681906,\n 3.7515740082914375,\n 3.729787466450391,\n 3.725064076743056,\n 3.725345485553217,\n 3.7122398055352663,\n 3.678506785835707,\n 3.6791071911536424,\n 3.7217450314150353,\n 3.7190714856542586,\n 3.6870472552230162,\n 3.702615090220856,\n 3.684585221088283,\n 3.67993275411912,\n 3.6665606053881077,\n 3.6819514306868335,\n 3.682435449385259,\n 3.6646350822501894,\n 3.674790652518728,\n 3.6663934322308527,\n 3.6618401478881575,\n 3.640628464840174,\n 3.63964028055979,\n 3.624826289434785,\n 3.6216404467820675,\n 3.6217400263819526,\n 3.607224397838301,\n 3.597703906327397,\n 3.5829744245481225,\n 3.56730318579851,\n 3.602694591883653,\n 3.589419963260789,\n 3.6015941180871303,\n 3.5885551523945134,\n 3.5742165088809337,\n 3.5583445761873085,\n 3.5429143419172635,\n 3.531953234083478,\n 3.5518758989398576,\n 3.5282255677284184,\n 3.499708769299523,\n 3.506097512179128,\n 3.512562720414096,\n 3.4816732004714557,\n 3.498489417683147,\n 3.5014855109366168,\n 3.46620011028114,\n 3.450689110433217,\n 3.431971858622021,\n 3.4427112599999283,\n 3.4222090029929535,\n 3.4162237249713074,\n 3.429861101370578,\n 3.436714360296111,\n 3.4354787620876386,\n 3.42659536614801,\n 3.4514857883628443,\n 3.4590375307151175,\n 3.4477647532378013,\n 3.433684805241373,\n 3.4051975218446624,\n 3.3875076383817633,\n 3.407903406715475,\n 3.393249962462379,\n 3.4005538853764694,\n 3.3864105263297977,\n 3.365118167963014,\n 3.3800131372583277,\n 3.3600699169255623,\n 3.3643867934720295,\n 3.330366194953882,\n 3.33271168874675,\n 3.327671308373242,\n 3.3080260968026374,\n 3.325642134977182,\n 3.3247150952350304,\n 3.3275020679802445,\n 3.322832855733423,\n 3.3055996752343106,\n 3.2904690379378416,\n 3.2764451817328033,\n 3.2552676610674425,\n 3.268707797234289,\n 3.269740951047754,\n 3.2727137811197746,\n 3.271347777561638,\n 3.2808270617758573,\n 3.2646374002100385,\n 3.257924869942538,\n 3.2411677861430843,\n 3.2477087824394335,\n 3.239822465331932,\n 3.257559378913828,\n 3.221959217176187,\n 3.237034118918976,\n 3.242771991199815,\n 3.2558438194406603,\n 3.2773153625431823,\n 3.237804879615698,\n 3.238463283302273,\n 3.2386897985157526,\n 3.2386526483575984,\n 3.2389055442938943,\n 3.2391674178688823,\n 3.2393127324019235,\n 3.239331898473507,\n 3.2396838374043697,\n 3.2399979470463443,\n 3.240195246589888,\n 3.2406731234819466,\n 3.2411670019570518,\n 3.2412116758175538,\n 3.2415202202428692,\n 3.2416213983393676,\n 3.241721796919973,\n 3.2419702179169154,\n 3.2423817170455003,\n 3.2426362118768575,\n 3.242621545503344,\n 3.242620968602477,\n 3.242621628569601,\n 3.2426187176467343,\n 3.2426206586310182,\n 3.2426224131550825,\n 3.2426218962689632,\n 3.242626635671196,\n 3.242627617156859,\n 3.2426302737455948,\n 3.2426316116401037,\n 3.242634505086712,\n 3.242636878507738,\n 3.242635578836835,\n 3.242639382201525,\n 3.242640658706083,\n 3.242644941220582,\n 3.242646481279862,\n 3.2426499070999477,\n 3.242650938876032,\n 3.2426506847258314]"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming variance_l1_norm is the calculated variance # If you have multiple variances, create a list\n\nplt.figure(figsize=(8, 6))\nplt.plot(range(len(variance_list)), variance_list, color='blue', linestyle='-', linewidth=2)\nplt.xlabel('Classes')\nplt.ylabel('Variance of L1-norm')\nplt.title('Variance of L1-norm Among Classes')\nplt.xticks(range(len(variance_list)), [f'Class_{i}' for i in range(len(variance_list))])\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:07:24.660758Z","iopub.status.idle":"2024-01-24T12:07:24.661075Z","shell.execute_reply.started":"2024-01-24T12:07:24.660918Z","shell.execute_reply":"2024-01-24T12:07:24.660933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import losses\n\n#from datasets.cifar100 import *\n\n#from train.train import *\n#from train.validate import *\n\n#from models.net import *\n\n#from losses.loss import *\n\n#from utils.config import *\n#from utils.plot import *\n#from utils.common import make_imb_data, save_checkpoint, hms_string\n\n#from utils.logger import logger\n\n#args = parse_args()\n\nfrom argparse import Namespace\n\n# Replace the command below with your actual values\nargs=Namespace(network='resnet32', epochs=200, batch_size=128, update_epoch=1,\n               lr=0.1, lr_decay=0.01, momentum=0.9, wd=0.0002, nesterov=False,\n               scheduler='warmup', warmup=5, aug_prob=0.5, cutout=False, cmo=False,\n               posthoc_la=False, cuda=True, aug_type='none', sim_type='none', max_d=30,\n               num_test=10, accept_rate=0.6, verbose=True, use_norm=False,\n               out='/kaggle/working/log3',\n               data_dir='~/dataset/', workers=4, seed='None',\n               gpu='0', dataset='cifar100', num_max=500, imb_ratio=100,\n               loss_fn='ce', num_experts=3, ride_distill=False)\n\nreproducibility(args.seed)\nargs = dataset_argument(args)\nargs.logger = logger(args)\n\nbest_acc = 0 # best test accuracy\ncurr_state_ac=[]\nlabel_ac=[]\nvariance_list1=[]\n\ndef main():\n    global best_acc,curr_state_ac,label_ac,variance_list1\n\n    try:\n        assert args.num_max <= 50000. / args.num_class\n    except AssertionError:\n        args.num_max = int(50000 / args.num_class)\n    \n    print(f'==> Preparing imbalanced CIFAR-100')\n    # N_SAMPLES_PER_CLASS = make_imb_data(args.num_max, args.num_class, args.imb_ratio)\n    trainset, testset = get_cifar100(os.path.join(args.data_dir, 'cifar100/'), args)\n    N_SAMPLES_PER_CLASS = trainset.img_num_list\n        \n    trainloader = data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, drop_last= args.loss_fn == 'ncl', pin_memory=True, sampler=None)\n    testloader = data.DataLoader(testset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True) \n    \n    if args.cmo:\n        cls_num_list = N_SAMPLES_PER_CLASS\n        cls_weight = 1.0 / (np.array(cls_num_list))\n        cls_weight = cls_weight / np.sum(cls_weight) * len(cls_num_list)\n        labels = trainloader.dataset.targets\n        samples_weight = np.array([cls_weight[t] for t in labels])\n        samples_weight = torch.from_numpy(samples_weight)\n        samples_weight = samples_weight.double()\n        print(\"samples_weight\", samples_weight)\n        sampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(labels), replacement=True)\n        weighted_trainloader = data.DataLoader(trainset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True, sampler=sampler)\n    else:\n        weighted_trainloader = None\n    \n\n    # Model\n    print (\"==> creating {}\".format(args.network))\n    model = get_model(args, N_SAMPLES_PER_CLASS)\n    train_criterion = get_loss(args, N_SAMPLES_PER_CLASS)\n    criterion = nn.CrossEntropyLoss() # For test, validation \n    optimizer = get_optimizer(args, model)\n    scheduler = get_scheduler(args,optimizer)\n\n    teacher = load_model(args)\n\n\n    train = get_train_fn(args)\n    validate = get_valid_fn(args)\n    update_score = get_update_score_fn(args)\n    \n    start_time = time.time()\n    \n    test_accs = []\n    for epoch in range(args.epochs):\n        \n        lr = adjust_learning_rate(optimizer, epoch, scheduler, args)\n        if args.cuda:\n            if epoch % args.update_epoch == 0:\n                curr_state_ac, label_ac = update_score(trainloader, model, N_SAMPLES_PER_CLASS, posthoc_la = args.posthoc_la, num_test = args.num_test, accept_rate = args.accept_rate)\n                print(curr_state_ac)\n                \n            if args.verbose:\n                if epoch == 0:\n                    maps = np.zeros((args.epochs,args.num_class))\n                maps = plot_score_epoch(curr_state_ac,label_ac, epoch, maps, args.out)\n        train_loss = train(args, trainloader, model, optimizer,train_criterion, epoch, weighted_trainloader, teacher) \n\n\n        test_loss, test_acc, test_cls = validate(args, testloader, model, criterion, N_SAMPLES_PER_CLASS,  num_class=args.num_class, mode='test Valid')\n\n        if best_acc <= test_acc:\n            best_acc = test_acc\n            many_best = test_cls[0]\n            med_best = test_cls[1]\n            few_best = test_cls[2]\n            # Save models\n            save_checkpoint({\n                'epoch': epoch + 1,\n                'state_dict': model['model'].state_dict() if args.loss_fn == 'ncl' else model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n            }, epoch + 1, args.out)\n        test_accs.append(test_acc)\n        \n        model_weights = model.state_dict()\n        classwise_l1_norms = {}\n        for class_label in range(args.num_class):\n            class_params = {\n            'linear.weight': model_weights['linear.weight'][class_label],\n            'linear.bias': model_weights['linear.bias'][class_label]\n            }\n            class_l1_norm = sum(torch.abs(param).sum() for param in class_params.values())\n            classwise_l1_norms[f'Class_{class_label}'] = class_l1_norm\n        l1_norm_values = [tensor.item() for tensor in classwise_l1_norms.values()]\n        variance_l1_norm = np.var(l1_norm_values)\n        variance_list1.append(np.sqrt(variance_l1_norm))\n        print(np.sqrt(variance_l1_norm))\n        \n        args.logger(f'Epoch: [{epoch+1} | {args.epochs}]', level=1)\n        if args.cuda:\n            args.logger(f'Max_state: {int(torch.max(curr_state_ac))}, min_state: {int(torch.min(curr_state_ac))}', level=2)\n        args.logger(f'[Train]\\tLoss:\\t{train_loss:.4f}', level=2)\n        args.logger(f'[Test ]\\tLoss:\\t{test_loss:.4f}\\tAcc:\\t{test_acc:.4f}', level=2)\n        args.logger(f'[Stats]\\tMany:\\t{test_cls[0]:.4f}\\tMedium:\\t{test_cls[1]:.4f}\\tFew:\\t{test_cls[2]:.4f}', level=2)\n        args.logger(f'[Best ]\\tAcc:\\t{np.max(test_accs):.4f}\\tMany:\\t{100*many_best:.4f}\\tMedium:\\t{100*med_best:.4f}\\tFew:\\t{100*few_best:.4f}', level=2)\n        args.logger(f'[Param]\\tLR:\\t{lr:.8f}', level=2)\n    \n    end_time = time.time()\n\n    # Print the final results\n    args.logger(f'Final performance...', level=1)\n    args.logger(f'best bAcc (test):\\t{np.max(test_accs)}', level=2)\n    args.logger(f'best statistics:\\tMany:\\t{many_best}\\tMed:\\t{med_best}\\tFew:\\t{few_best}', level=2)\n    args.logger(f'Training Time: {hms_string(end_time - start_time)}', level=1)\n\n    \n    if args.verbose:\n        args.logger.map_save(maps)\n\nif __name__ == '__main__':\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-24T12:07:36.988414Z","iopub.execute_input":"2024-01-24T12:07:36.988791Z","iopub.status.idle":"2024-01-24T12:34:50.125594Z","shell.execute_reply.started":"2024-01-24T12:07:36.988755Z","shell.execute_reply":"2024-01-24T12:34:50.124680Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"---> ---cifar100---\n---> ---cifar100---\n---> Argument\n---> Argument\n    > network     : resnet32\n    > network     : resnet32\n    > epochs      : 200\n    > epochs      : 200\n    > batch_size  : 128\n    > batch_size  : 128\n    > update_epoch: 1\n    > update_epoch: 1\n    > lr          : 0.1\n    > lr          : 0.1\n    > lr_decay    : 0.01\n    > lr_decay    : 0.01\n    > momentum    : 0.9\n    > momentum    : 0.9\n    > wd          : 0.0002\n    > wd          : 0.0002\n    > nesterov    : False\n    > nesterov    : False\n    > scheduler   : warmup\n    > scheduler   : warmup\n    > warmup      : 5\n    > warmup      : 5\n    > aug_prob    : 0.5\n    > aug_prob    : 0.5\n    > cutout      : False\n    > cutout      : False\n    > cmo         : False\n    > cmo         : False\n    > posthoc_la  : False\n    > posthoc_la  : False\n    > cuda        : True\n    > cuda        : True\n    > aug_type    : none\n    > aug_type    : none\n    > sim_type    : none\n    > sim_type    : none\n    > max_d       : 30\n    > max_d       : 30\n    > num_test    : 10\n    > num_test    : 10\n    > accept_rate : 0.6\n    > accept_rate : 0.6\n    > verbose     : True\n    > verbose     : True\n    > use_norm    : False\n    > use_norm    : False\n    > out         : /kaggle/working/log3\n    > out         : /kaggle/working/log3\n    > data_dir    : ~/dataset/\n    > data_dir    : ~/dataset/\n    > workers     : 4\n    > workers     : 4\n    > seed        : None\n    > seed        : None\n    > gpu         : 0\n    > gpu         : 0\n    > dataset     : cifar100\n    > dataset     : cifar100\n    > num_max     : 500\n    > num_max     : 500\n    > imb_ratio   : 100\n    > imb_ratio   : 100\n    > loss_fn     : ce\n    > loss_fn     : ce\n    > num_experts : 3\n    > num_experts : 3\n    > ride_distill: False\n    > ride_distill: False\n    > num_class   : 100\n    > num_class   : 100\n","output_type":"stream"},{"name":"stdout","text":"==> Preparing imbalanced CIFAR-100\nFiles already downloaded and verified\nMagnitude set = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=torch.int32)\nOperation set = tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=torch.int32)\nFiles already downloaded and verified\n#Train: 10847, #Test: 10000\n==> creating resnet32\n    Total params: 0.47M\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 1 // Min state: 0\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [1 | 200]\n---> Epoch: [1 | 200]\n    > Max_state: 1, min_state: 0\n    > Max_state: 1, min_state: 0\n    > [Train]\tLoss:\t4.1298\n    > [Train]\tLoss:\t4.1298\n    > [Test ]\tLoss:\t5.4958\tAcc:\t4.8200\n    > [Test ]\tLoss:\t5.4958\tAcc:\t4.8200\n    > [Stats]\tMany:\t0.1374\tMedium:\t0.0003\tFew:\t0.0000\n    > [Stats]\tMany:\t0.1374\tMedium:\t0.0003\tFew:\t0.0000\n    > [Best ]\tAcc:\t4.8200\tMany:\t13.7429\tMedium:\t0.0286\tFew:\t0.0000\n    > [Best ]\tAcc:\t4.8200\tMany:\t13.7429\tMedium:\t0.0286\tFew:\t0.0000\n    > [Param]\tLR:\t0.02000000\n    > [Param]\tLR:\t0.02000000\n","output_type":"stream"},{"name":"stdout","text":"0.8236414878678344\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 1 // Min state: 0\ntensor([1., 1., 1.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [2 | 200]\n---> Epoch: [2 | 200]\n    > Max_state: 1, min_state: 0\n    > Max_state: 1, min_state: 0\n    > [Train]\tLoss:\t3.6259\n    > [Train]\tLoss:\t3.6259\n    > [Test ]\tLoss:\t5.1077\tAcc:\t6.6400\n    > [Test ]\tLoss:\t5.1077\tAcc:\t6.6400\n    > [Stats]\tMany:\t0.1880\tMedium:\t0.0017\tFew:\t0.0000\n    > [Stats]\tMany:\t0.1880\tMedium:\t0.0017\tFew:\t0.0000\n    > [Best ]\tAcc:\t6.6400\tMany:\t18.8000\tMedium:\t0.1714\tFew:\t0.0000\n    > [Best ]\tAcc:\t6.6400\tMany:\t18.8000\tMedium:\t0.1714\tFew:\t0.0000\n    > [Param]\tLR:\t0.04000000\n    > [Param]\tLR:\t0.04000000\n","output_type":"stream"},{"name":"stdout","text":"0.8587062662416769\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 1 // Min state: 0\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [3 | 200]\n---> Epoch: [3 | 200]\n    > Max_state: 1, min_state: 0\n    > Max_state: 1, min_state: 0\n    > [Train]\tLoss:\t3.4203\n    > [Train]\tLoss:\t3.4203\n    > [Test ]\tLoss:\t4.9712\tAcc:\t8.3200\n    > [Test ]\tLoss:\t4.9712\tAcc:\t8.3200\n    > [Stats]\tMany:\t0.2249\tMedium:\t0.0129\tFew:\t0.0000\n    > [Stats]\tMany:\t0.2249\tMedium:\t0.0129\tFew:\t0.0000\n    > [Best ]\tAcc:\t8.3200\tMany:\t22.4857\tMedium:\t1.2857\tFew:\t0.0000\n    > [Best ]\tAcc:\t8.3200\tMany:\t22.4857\tMedium:\t1.2857\tFew:\t0.0000\n    > [Param]\tLR:\t0.06000000\n    > [Param]\tLR:\t0.06000000\n","output_type":"stream"},{"name":"stdout","text":"0.9422301840706353\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 1 // Min state: 0\ntensor([1., 1., 1.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [4 | 200]\n---> Epoch: [4 | 200]\n    > Max_state: 1, min_state: 0\n    > Max_state: 1, min_state: 0\n    > [Train]\tLoss:\t3.2829\n    > [Train]\tLoss:\t3.2829\n    > [Test ]\tLoss:\t4.9329\tAcc:\t8.6200\n    > [Test ]\tLoss:\t4.9329\tAcc:\t8.6200\n    > [Stats]\tMany:\t0.2366\tMedium:\t0.0097\tFew:\t0.0000\n    > [Stats]\tMany:\t0.2366\tMedium:\t0.0097\tFew:\t0.0000\n    > [Best ]\tAcc:\t8.6200\tMany:\t23.6571\tMedium:\t0.9714\tFew:\t0.0000\n    > [Best ]\tAcc:\t8.6200\tMany:\t23.6571\tMedium:\t0.9714\tFew:\t0.0000\n    > [Param]\tLR:\t0.08000000\n    > [Param]\tLR:\t0.08000000\n","output_type":"stream"},{"name":"stdout","text":"1.0631543725398136\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 1 // Min state: 0\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [5 | 200]\n---> Epoch: [5 | 200]\n    > Max_state: 1, min_state: 0\n    > Max_state: 1, min_state: 0\n    > [Train]\tLoss:\t3.1019\n    > [Train]\tLoss:\t3.1019\n    > [Test ]\tLoss:\t4.8948\tAcc:\t9.7700\n    > [Test ]\tLoss:\t4.8948\tAcc:\t9.7700\n    > [Stats]\tMany:\t0.2551\tMedium:\t0.0240\tFew:\t0.0000\n    > [Stats]\tMany:\t0.2551\tMedium:\t0.0240\tFew:\t0.0000\n    > [Best ]\tAcc:\t9.7700\tMany:\t25.5143\tMedium:\t2.4000\tFew:\t0.0000\n    > [Best ]\tAcc:\t9.7700\tMany:\t25.5143\tMedium:\t2.4000\tFew:\t0.0000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"1.272139516963871\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 1 // Min state: 0\ntensor([1., 1., 1.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [6 | 200]\n---> Epoch: [6 | 200]\n    > Max_state: 1, min_state: 0\n    > Max_state: 1, min_state: 0\n    > [Train]\tLoss:\t2.9443\n    > [Train]\tLoss:\t2.9443\n    > [Test ]\tLoss:\t4.6115\tAcc:\t12.1800\n    > [Test ]\tLoss:\t4.6115\tAcc:\t12.1800\n    > [Stats]\tMany:\t0.2731\tMedium:\t0.0720\tFew:\t0.0033\n    > [Stats]\tMany:\t0.2731\tMedium:\t0.0720\tFew:\t0.0033\n    > [Best ]\tAcc:\t12.1800\tMany:\t27.3143\tMedium:\t7.2000\tFew:\t0.3333\n    > [Best ]\tAcc:\t12.1800\tMany:\t27.3143\tMedium:\t7.2000\tFew:\t0.3333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"1.4384933537552487\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 1 // Min state: 0\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [7 | 200]\n---> Epoch: [7 | 200]\n    > Max_state: 1, min_state: 0\n    > Max_state: 1, min_state: 0\n    > [Train]\tLoss:\t2.7442\n    > [Train]\tLoss:\t2.7442\n    > [Test ]\tLoss:\t4.6168\tAcc:\t13.1900\n    > [Test ]\tLoss:\t4.6168\tAcc:\t13.1900\n    > [Stats]\tMany:\t0.3263\tMedium:\t0.0506\tFew:\t0.0000\n    > [Stats]\tMany:\t0.3263\tMedium:\t0.0506\tFew:\t0.0000\n    > [Best ]\tAcc:\t13.1900\tMany:\t32.6286\tMedium:\t5.0571\tFew:\t0.0000\n    > [Best ]\tAcc:\t13.1900\tMany:\t32.6286\tMedium:\t5.0571\tFew:\t0.0000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"1.6867360554636994\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 1 // Min state: 0\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [8 | 200]\n---> Epoch: [8 | 200]\n    > Max_state: 1, min_state: 0\n    > Max_state: 1, min_state: 0\n    > [Train]\tLoss:\t2.6144\n    > [Train]\tLoss:\t2.6144\n    > [Test ]\tLoss:\t4.6868\tAcc:\t11.5900\n    > [Test ]\tLoss:\t4.6868\tAcc:\t11.5900\n    > [Stats]\tMany:\t0.2903\tMedium:\t0.0409\tFew:\t0.0000\n    > [Stats]\tMany:\t0.2903\tMedium:\t0.0409\tFew:\t0.0000\n    > [Best ]\tAcc:\t13.1900\tMany:\t32.6286\tMedium:\t5.0571\tFew:\t0.0000\n    > [Best ]\tAcc:\t13.1900\tMany:\t32.6286\tMedium:\t5.0571\tFew:\t0.0000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"1.8650536892802838\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 1 // Min state: 0\ntensor([1., 1., 1.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [9 | 200]\n---> Epoch: [9 | 200]\n    > Max_state: 1, min_state: 0\n    > Max_state: 1, min_state: 0\n    > [Train]\tLoss:\t2.4592\n    > [Train]\tLoss:\t2.4592\n    > [Test ]\tLoss:\t4.5347\tAcc:\t13.6100\n    > [Test ]\tLoss:\t4.5347\tAcc:\t13.6100\n    > [Stats]\tMany:\t0.3389\tMedium:\t0.0500\tFew:\t0.0000\n    > [Stats]\tMany:\t0.3389\tMedium:\t0.0500\tFew:\t0.0000\n    > [Best ]\tAcc:\t13.6100\tMany:\t33.8857\tMedium:\t5.0000\tFew:\t0.0000\n    > [Best ]\tAcc:\t13.6100\tMany:\t33.8857\tMedium:\t5.0000\tFew:\t0.0000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.0114882025709986\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 2 // Min state: 0\ntensor([2., 2., 2.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [10 | 200]\n---> Epoch: [10 | 200]\n    > Max_state: 2, min_state: 0\n    > Max_state: 2, min_state: 0\n    > [Train]\tLoss:\t2.3739\n    > [Train]\tLoss:\t2.3739\n    > [Test ]\tLoss:\t5.5602\tAcc:\t11.2500\n    > [Test ]\tLoss:\t5.5602\tAcc:\t11.2500\n    > [Stats]\tMany:\t0.2760\tMedium:\t0.0454\tFew:\t0.0000\n    > [Stats]\tMany:\t0.2760\tMedium:\t0.0454\tFew:\t0.0000\n    > [Best ]\tAcc:\t13.6100\tMany:\t33.8857\tMedium:\t5.0000\tFew:\t0.0000\n    > [Best ]\tAcc:\t13.6100\tMany:\t33.8857\tMedium:\t5.0000\tFew:\t0.0000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.1967383127600724\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 1 // Min state: 0\ntensor([1., 1., 1.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [11 | 200]\n---> Epoch: [11 | 200]\n    > Max_state: 1, min_state: 0\n    > Max_state: 1, min_state: 0\n    > [Train]\tLoss:\t2.2601\n    > [Train]\tLoss:\t2.2601\n    > [Test ]\tLoss:\t4.4095\tAcc:\t15.7300\n    > [Test ]\tLoss:\t4.4095\tAcc:\t15.7300\n    > [Stats]\tMany:\t0.3660\tMedium:\t0.0834\tFew:\t0.0000\n    > [Stats]\tMany:\t0.3660\tMedium:\t0.0834\tFew:\t0.0000\n    > [Best ]\tAcc:\t15.7300\tMany:\t36.6000\tMedium:\t8.3429\tFew:\t0.0000\n    > [Best ]\tAcc:\t15.7300\tMany:\t36.6000\tMedium:\t8.3429\tFew:\t0.0000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.313557736282622\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 1 // Min state: 0\ntensor([0., 0., 0.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [12 | 200]\n---> Epoch: [12 | 200]\n    > Max_state: 1, min_state: 0\n    > Max_state: 1, min_state: 0\n    > [Train]\tLoss:\t2.1598\n    > [Train]\tLoss:\t2.1598\n    > [Test ]\tLoss:\t4.3515\tAcc:\t16.5700\n    > [Test ]\tLoss:\t4.3515\tAcc:\t16.5700\n    > [Stats]\tMany:\t0.3946\tMedium:\t0.0783\tFew:\t0.0007\n    > [Stats]\tMany:\t0.3946\tMedium:\t0.0783\tFew:\t0.0007\n    > [Best ]\tAcc:\t16.5700\tMany:\t39.4571\tMedium:\t7.8286\tFew:\t0.0667\n    > [Best ]\tAcc:\t16.5700\tMany:\t39.4571\tMedium:\t7.8286\tFew:\t0.0667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.44969129549282\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 1 // Min state: 0\ntensor([1., 1., 1.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [13 | 200]\n---> Epoch: [13 | 200]\n    > Max_state: 1, min_state: 0\n    > Max_state: 1, min_state: 0\n    > [Train]\tLoss:\t2.1026\n    > [Train]\tLoss:\t2.1026\n    > [Test ]\tLoss:\t3.8700\tAcc:\t20.1100\n    > [Test ]\tLoss:\t3.8700\tAcc:\t20.1100\n    > [Stats]\tMany:\t0.4311\tMedium:\t0.1357\tFew:\t0.0090\n    > [Stats]\tMany:\t0.4311\tMedium:\t0.1357\tFew:\t0.0090\n    > [Best ]\tAcc:\t20.1100\tMany:\t43.1143\tMedium:\t13.5714\tFew:\t0.9000\n    > [Best ]\tAcc:\t20.1100\tMany:\t43.1143\tMedium:\t13.5714\tFew:\t0.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5332243302049653\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 2 // Min state: 0\ntensor([2., 2., 2.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [14 | 200]\n---> Epoch: [14 | 200]\n    > Max_state: 2, min_state: 0\n    > Max_state: 2, min_state: 0\n    > [Train]\tLoss:\t2.0959\n    > [Train]\tLoss:\t2.0959\n    > [Test ]\tLoss:\t4.4574\tAcc:\t17.4100\n    > [Test ]\tLoss:\t4.4574\tAcc:\t17.4100\n    > [Stats]\tMany:\t0.3929\tMedium:\t0.0997\tFew:\t0.0057\n    > [Stats]\tMany:\t0.3929\tMedium:\t0.0997\tFew:\t0.0057\n    > [Best ]\tAcc:\t20.1100\tMany:\t43.1143\tMedium:\t13.5714\tFew:\t0.9000\n    > [Best ]\tAcc:\t20.1100\tMany:\t43.1143\tMedium:\t13.5714\tFew:\t0.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.6030258519926144\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 3 // Min state: 0\ntensor([3., 3., 3.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [15 | 200]\n---> Epoch: [15 | 200]\n    > Max_state: 3, min_state: 0\n    > Max_state: 3, min_state: 0\n    > [Train]\tLoss:\t2.0403\n    > [Train]\tLoss:\t2.0403\n    > [Test ]\tLoss:\t4.5815\tAcc:\t17.7600\n    > [Test ]\tLoss:\t4.5815\tAcc:\t17.7600\n    > [Stats]\tMany:\t0.3754\tMedium:\t0.1309\tFew:\t0.0013\n    > [Stats]\tMany:\t0.3754\tMedium:\t0.1309\tFew:\t0.0013\n    > [Best ]\tAcc:\t20.1100\tMany:\t43.1143\tMedium:\t13.5714\tFew:\t0.9000\n    > [Best ]\tAcc:\t20.1100\tMany:\t43.1143\tMedium:\t13.5714\tFew:\t0.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.68428547176403\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 2 // Min state: 0\ntensor([2., 2., 2.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [16 | 200]\n---> Epoch: [16 | 200]\n    > Max_state: 2, min_state: 0\n    > Max_state: 2, min_state: 0\n    > [Train]\tLoss:\t1.9049\n    > [Train]\tLoss:\t1.9049\n    > [Test ]\tLoss:\t4.1868\tAcc:\t18.7800\n    > [Test ]\tLoss:\t4.1868\tAcc:\t18.7800\n    > [Stats]\tMany:\t0.4049\tMedium:\t0.1294\tFew:\t0.0027\n    > [Stats]\tMany:\t0.4049\tMedium:\t0.1294\tFew:\t0.0027\n    > [Best ]\tAcc:\t20.1100\tMany:\t43.1143\tMedium:\t13.5714\tFew:\t0.9000\n    > [Best ]\tAcc:\t20.1100\tMany:\t43.1143\tMedium:\t13.5714\tFew:\t0.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.796738573039424\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 3 // Min state: 0\ntensor([3., 3., 3.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [17 | 200]\n---> Epoch: [17 | 200]\n    > Max_state: 3, min_state: 0\n    > Max_state: 3, min_state: 0\n    > [Train]\tLoss:\t1.8998\n    > [Train]\tLoss:\t1.8998\n    > [Test ]\tLoss:\t3.9286\tAcc:\t21.3500\n    > [Test ]\tLoss:\t3.9286\tAcc:\t21.3500\n    > [Stats]\tMany:\t0.4346\tMedium:\t0.1711\tFew:\t0.0050\n    > [Stats]\tMany:\t0.4346\tMedium:\t0.1711\tFew:\t0.0050\n    > [Best ]\tAcc:\t21.3500\tMany:\t43.4571\tMedium:\t17.1143\tFew:\t0.5000\n    > [Best ]\tAcc:\t21.3500\tMany:\t43.4571\tMedium:\t17.1143\tFew:\t0.5000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.829480429198641\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 2 // Min state: 0\ntensor([2., 2., 2.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [18 | 200]\n---> Epoch: [18 | 200]\n    > Max_state: 2, min_state: 0\n    > Max_state: 2, min_state: 0\n    > [Train]\tLoss:\t1.7497\n    > [Train]\tLoss:\t1.7497\n    > [Test ]\tLoss:\t4.2804\tAcc:\t20.7400\n    > [Test ]\tLoss:\t4.2804\tAcc:\t20.7400\n    > [Stats]\tMany:\t0.4677\tMedium:\t0.1186\tFew:\t0.0073\n    > [Stats]\tMany:\t0.4677\tMedium:\t0.1186\tFew:\t0.0073\n    > [Best ]\tAcc:\t21.3500\tMany:\t43.4571\tMedium:\t17.1143\tFew:\t0.5000\n    > [Best ]\tAcc:\t21.3500\tMany:\t43.4571\tMedium:\t17.1143\tFew:\t0.5000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.8974898124726614\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 3 // Min state: 0\ntensor([3., 3., 3.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [19 | 200]\n---> Epoch: [19 | 200]\n    > Max_state: 3, min_state: 0\n    > Max_state: 3, min_state: 0\n    > [Train]\tLoss:\t1.7676\n    > [Train]\tLoss:\t1.7676\n    > [Test ]\tLoss:\t3.9282\tAcc:\t23.5200\n    > [Test ]\tLoss:\t3.9282\tAcc:\t23.5200\n    > [Stats]\tMany:\t0.4889\tMedium:\t0.1803\tFew:\t0.0033\n    > [Stats]\tMany:\t0.4889\tMedium:\t0.1803\tFew:\t0.0033\n    > [Best ]\tAcc:\t23.5200\tMany:\t48.8857\tMedium:\t18.0286\tFew:\t0.3333\n    > [Best ]\tAcc:\t23.5200\tMany:\t48.8857\tMedium:\t18.0286\tFew:\t0.3333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.913750014989409\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 2 // Min state: 0\ntensor([2., 2., 2.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [20 | 200]\n---> Epoch: [20 | 200]\n    > Max_state: 2, min_state: 0\n    > Max_state: 2, min_state: 0\n    > [Train]\tLoss:\t1.7454\n    > [Train]\tLoss:\t1.7454\n    > [Test ]\tLoss:\t3.9651\tAcc:\t23.4800\n    > [Test ]\tLoss:\t3.9651\tAcc:\t23.4800\n    > [Stats]\tMany:\t0.4963\tMedium:\t0.1717\tFew:\t0.0033\n    > [Stats]\tMany:\t0.4963\tMedium:\t0.1717\tFew:\t0.0033\n    > [Best ]\tAcc:\t23.5200\tMany:\t48.8857\tMedium:\t18.0286\tFew:\t0.3333\n    > [Best ]\tAcc:\t23.5200\tMany:\t48.8857\tMedium:\t18.0286\tFew:\t0.3333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.9719469154640805\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 3 // Min state: 0\ntensor([1., 1., 1.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [21 | 200]\n---> Epoch: [21 | 200]\n    > Max_state: 3, min_state: 0\n    > Max_state: 3, min_state: 0\n    > [Train]\tLoss:\t1.6871\n    > [Train]\tLoss:\t1.6871\n    > [Test ]\tLoss:\t4.0671\tAcc:\t22.9000\n    > [Test ]\tLoss:\t4.0671\tAcc:\t22.9000\n    > [Stats]\tMany:\t0.4700\tMedium:\t0.1731\tFew:\t0.0130\n    > [Stats]\tMany:\t0.4700\tMedium:\t0.1731\tFew:\t0.0130\n    > [Best ]\tAcc:\t23.5200\tMany:\t48.8857\tMedium:\t18.0286\tFew:\t0.3333\n    > [Best ]\tAcc:\t23.5200\tMany:\t48.8857\tMedium:\t18.0286\tFew:\t0.3333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.013847964786132\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 2 // Min state: 0\ntensor([2., 2., 2.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [22 | 200]\n---> Epoch: [22 | 200]\n    > Max_state: 2, min_state: 0\n    > Max_state: 2, min_state: 0\n    > [Train]\tLoss:\t1.6939\n    > [Train]\tLoss:\t1.6939\n    > [Test ]\tLoss:\t3.7258\tAcc:\t24.2300\n    > [Test ]\tLoss:\t3.7258\tAcc:\t24.2300\n    > [Stats]\tMany:\t0.5060\tMedium:\t0.1746\tFew:\t0.0137\n    > [Stats]\tMany:\t0.5060\tMedium:\t0.1746\tFew:\t0.0137\n    > [Best ]\tAcc:\t24.2300\tMany:\t50.6000\tMedium:\t17.4571\tFew:\t1.3667\n    > [Best ]\tAcc:\t24.2300\tMany:\t50.6000\tMedium:\t17.4571\tFew:\t1.3667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.029129554907648\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 3 // Min state: 0\ntensor([3., 3., 3.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [23 | 200]\n---> Epoch: [23 | 200]\n    > Max_state: 3, min_state: 0\n    > Max_state: 3, min_state: 0\n    > [Train]\tLoss:\t1.6405\n    > [Train]\tLoss:\t1.6405\n    > [Test ]\tLoss:\t3.7683\tAcc:\t23.1400\n    > [Test ]\tLoss:\t3.7683\tAcc:\t23.1400\n    > [Stats]\tMany:\t0.4709\tMedium:\t0.1826\tFew:\t0.0090\n    > [Stats]\tMany:\t0.4709\tMedium:\t0.1826\tFew:\t0.0090\n    > [Best ]\tAcc:\t24.2300\tMany:\t50.6000\tMedium:\t17.4571\tFew:\t1.3667\n    > [Best ]\tAcc:\t24.2300\tMany:\t50.6000\tMedium:\t17.4571\tFew:\t1.3667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.042925941033132\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([2., 2., 2.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [24 | 200]\n---> Epoch: [24 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t1.5504\n    > [Train]\tLoss:\t1.5504\n    > [Test ]\tLoss:\t4.0246\tAcc:\t23.7700\n    > [Test ]\tLoss:\t4.0246\tAcc:\t23.7700\n    > [Stats]\tMany:\t0.4920\tMedium:\t0.1803\tFew:\t0.0080\n    > [Stats]\tMany:\t0.4920\tMedium:\t0.1803\tFew:\t0.0080\n    > [Best ]\tAcc:\t24.2300\tMany:\t50.6000\tMedium:\t17.4571\tFew:\t1.3667\n    > [Best ]\tAcc:\t24.2300\tMany:\t50.6000\tMedium:\t17.4571\tFew:\t1.3667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.0802149937957397\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 3 // Min state: 0\ntensor([1., 1., 1.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [25 | 200]\n---> Epoch: [25 | 200]\n    > Max_state: 3, min_state: 0\n    > Max_state: 3, min_state: 0\n    > [Train]\tLoss:\t1.6254\n    > [Train]\tLoss:\t1.6254\n    > [Test ]\tLoss:\t4.2289\tAcc:\t20.9300\n    > [Test ]\tLoss:\t4.2289\tAcc:\t20.9300\n    > [Stats]\tMany:\t0.4280\tMedium:\t0.1597\tFew:\t0.0120\n    > [Stats]\tMany:\t0.4280\tMedium:\t0.1597\tFew:\t0.0120\n    > [Best ]\tAcc:\t24.2300\tMany:\t50.6000\tMedium:\t17.4571\tFew:\t1.3667\n    > [Best ]\tAcc:\t24.2300\tMany:\t50.6000\tMedium:\t17.4571\tFew:\t1.3667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.1195388137070066\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 2 // Min state: 0\ntensor([2., 2., 2.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [26 | 200]\n---> Epoch: [26 | 200]\n    > Max_state: 2, min_state: 0\n    > Max_state: 2, min_state: 0\n    > [Train]\tLoss:\t1.4707\n    > [Train]\tLoss:\t1.4707\n    > [Test ]\tLoss:\t3.6781\tAcc:\t25.9000\n    > [Test ]\tLoss:\t3.6781\tAcc:\t25.9000\n    > [Stats]\tMany:\t0.5083\tMedium:\t0.2163\tFew:\t0.0180\n    > [Stats]\tMany:\t0.5083\tMedium:\t0.2163\tFew:\t0.0180\n    > [Best ]\tAcc:\t25.9000\tMany:\t50.8286\tMedium:\t21.6286\tFew:\t1.8000\n    > [Best ]\tAcc:\t25.9000\tMany:\t50.8286\tMedium:\t21.6286\tFew:\t1.8000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.121281264292013\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 3 // Min state: 0\ntensor([1., 1., 1.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [27 | 200]\n---> Epoch: [27 | 200]\n    > Max_state: 3, min_state: 0\n    > Max_state: 3, min_state: 0\n    > [Train]\tLoss:\t1.4907\n    > [Train]\tLoss:\t1.4907\n    > [Test ]\tLoss:\t3.9561\tAcc:\t24.3400\n    > [Test ]\tLoss:\t3.9561\tAcc:\t24.3400\n    > [Stats]\tMany:\t0.4749\tMedium:\t0.2040\tFew:\t0.0193\n    > [Stats]\tMany:\t0.4749\tMedium:\t0.2040\tFew:\t0.0193\n    > [Best ]\tAcc:\t25.9000\tMany:\t50.8286\tMedium:\t21.6286\tFew:\t1.8000\n    > [Best ]\tAcc:\t25.9000\tMany:\t50.8286\tMedium:\t21.6286\tFew:\t1.8000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.156911908175873\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 2 // Min state: 0\ntensor([2., 2., 2.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [28 | 200]\n---> Epoch: [28 | 200]\n    > Max_state: 2, min_state: 0\n    > Max_state: 2, min_state: 0\n    > [Train]\tLoss:\t1.4823\n    > [Train]\tLoss:\t1.4823\n    > [Test ]\tLoss:\t3.8009\tAcc:\t25.4300\n    > [Test ]\tLoss:\t3.8009\tAcc:\t25.4300\n    > [Stats]\tMany:\t0.5011\tMedium:\t0.2077\tFew:\t0.0207\n    > [Stats]\tMany:\t0.5011\tMedium:\t0.2077\tFew:\t0.0207\n    > [Best ]\tAcc:\t25.9000\tMany:\t50.8286\tMedium:\t21.6286\tFew:\t1.8000\n    > [Best ]\tAcc:\t25.9000\tMany:\t50.8286\tMedium:\t21.6286\tFew:\t1.8000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.1592621296668177\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 3 // Min state: 0\ntensor([1., 1., 1.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [29 | 200]\n---> Epoch: [29 | 200]\n    > Max_state: 3, min_state: 0\n    > Max_state: 3, min_state: 0\n    > [Train]\tLoss:\t1.4747\n    > [Train]\tLoss:\t1.4747\n    > [Test ]\tLoss:\t4.2629\tAcc:\t23.3900\n    > [Test ]\tLoss:\t4.2629\tAcc:\t23.3900\n    > [Stats]\tMany:\t0.4726\tMedium:\t0.1877\tFew:\t0.0093\n    > [Stats]\tMany:\t0.4726\tMedium:\t0.1877\tFew:\t0.0093\n    > [Best ]\tAcc:\t25.9000\tMany:\t50.8286\tMedium:\t21.6286\tFew:\t1.8000\n    > [Best ]\tAcc:\t25.9000\tMany:\t50.8286\tMedium:\t21.6286\tFew:\t1.8000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.186283350210148\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([2., 2., 2.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [30 | 200]\n---> Epoch: [30 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t1.4910\n    > [Train]\tLoss:\t1.4910\n    > [Test ]\tLoss:\t3.4799\tAcc:\t27.8000\n    > [Test ]\tLoss:\t3.4799\tAcc:\t27.8000\n    > [Stats]\tMany:\t0.5443\tMedium:\t0.2357\tFew:\t0.0167\n    > [Stats]\tMany:\t0.5443\tMedium:\t0.2357\tFew:\t0.0167\n    > [Best ]\tAcc:\t27.8000\tMany:\t54.4286\tMedium:\t23.5714\tFew:\t1.6667\n    > [Best ]\tAcc:\t27.8000\tMany:\t54.4286\tMedium:\t23.5714\tFew:\t1.6667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.1622886411882316\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 3 // Min state: 0\ntensor([3., 3., 3.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [31 | 200]\n---> Epoch: [31 | 200]\n    > Max_state: 3, min_state: 0\n    > Max_state: 3, min_state: 0\n    > [Train]\tLoss:\t1.4363\n    > [Train]\tLoss:\t1.4363\n    > [Test ]\tLoss:\t3.7206\tAcc:\t27.5900\n    > [Test ]\tLoss:\t3.7206\tAcc:\t27.5900\n    > [Stats]\tMany:\t0.5477\tMedium:\t0.2200\tFew:\t0.0240\n    > [Stats]\tMany:\t0.5477\tMedium:\t0.2200\tFew:\t0.0240\n    > [Best ]\tAcc:\t27.8000\tMany:\t54.4286\tMedium:\t23.5714\tFew:\t1.6667\n    > [Best ]\tAcc:\t27.8000\tMany:\t54.4286\tMedium:\t23.5714\tFew:\t1.6667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.189443995110617\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([2., 2., 2.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [32 | 200]\n---> Epoch: [32 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t1.4889\n    > [Train]\tLoss:\t1.4889\n    > [Test ]\tLoss:\t3.6590\tAcc:\t27.5100\n    > [Test ]\tLoss:\t3.6590\tAcc:\t27.5100\n    > [Stats]\tMany:\t0.5394\tMedium:\t0.2126\tFew:\t0.0397\n    > [Stats]\tMany:\t0.5394\tMedium:\t0.2126\tFew:\t0.0397\n    > [Best ]\tAcc:\t27.8000\tMany:\t54.4286\tMedium:\t23.5714\tFew:\t1.6667\n    > [Best ]\tAcc:\t27.8000\tMany:\t54.4286\tMedium:\t23.5714\tFew:\t1.6667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.176688125471654\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 3 // Min state: 0\ntensor([3., 3., 3.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [33 | 200]\n---> Epoch: [33 | 200]\n    > Max_state: 3, min_state: 0\n    > Max_state: 3, min_state: 0\n    > [Train]\tLoss:\t1.4174\n    > [Train]\tLoss:\t1.4174\n    > [Test ]\tLoss:\t3.5590\tAcc:\t28.2100\n    > [Test ]\tLoss:\t3.5590\tAcc:\t28.2100\n    > [Stats]\tMany:\t0.5217\tMedium:\t0.2629\tFew:\t0.0250\n    > [Stats]\tMany:\t0.5217\tMedium:\t0.2629\tFew:\t0.0250\n    > [Best ]\tAcc:\t28.2100\tMany:\t52.1714\tMedium:\t26.2857\tFew:\t2.5000\n    > [Best ]\tAcc:\t28.2100\tMany:\t52.1714\tMedium:\t26.2857\tFew:\t2.5000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.1910121327810423\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [34 | 200]\n---> Epoch: [34 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t1.4182\n    > [Train]\tLoss:\t1.4182\n    > [Test ]\tLoss:\t3.8346\tAcc:\t27.3600\n    > [Test ]\tLoss:\t3.8346\tAcc:\t27.3600\n    > [Stats]\tMany:\t0.5434\tMedium:\t0.2189\tFew:\t0.0227\n    > [Stats]\tMany:\t0.5434\tMedium:\t0.2189\tFew:\t0.0227\n    > [Best ]\tAcc:\t28.2100\tMany:\t52.1714\tMedium:\t26.2857\tFew:\t2.5000\n    > [Best ]\tAcc:\t28.2100\tMany:\t52.1714\tMedium:\t26.2857\tFew:\t2.5000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.1617297165979616\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([3., 3., 3.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [35 | 200]\n---> Epoch: [35 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t1.4072\n    > [Train]\tLoss:\t1.4072\n    > [Test ]\tLoss:\t3.4204\tAcc:\t28.8100\n    > [Test ]\tLoss:\t3.4204\tAcc:\t28.8100\n    > [Stats]\tMany:\t0.5626\tMedium:\t0.2326\tFew:\t0.0327\n    > [Stats]\tMany:\t0.5626\tMedium:\t0.2326\tFew:\t0.0327\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.144449342971358\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [36 | 200]\n---> Epoch: [36 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t1.3257\n    > [Train]\tLoss:\t1.3257\n    > [Test ]\tLoss:\t3.8369\tAcc:\t27.5200\n    > [Test ]\tLoss:\t3.8369\tAcc:\t27.5200\n    > [Stats]\tMany:\t0.5011\tMedium:\t0.2666\tFew:\t0.0217\n    > [Stats]\tMany:\t0.5011\tMedium:\t0.2666\tFew:\t0.0217\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.1321309070034236\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 3 // Min state: 0\ntensor([3., 3., 3.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [37 | 200]\n---> Epoch: [37 | 200]\n    > Max_state: 3, min_state: 0\n    > Max_state: 3, min_state: 0\n    > [Train]\tLoss:\t1.3058\n    > [Train]\tLoss:\t1.3058\n    > [Test ]\tLoss:\t3.7859\tAcc:\t28.2300\n    > [Test ]\tLoss:\t3.7859\tAcc:\t28.2300\n    > [Stats]\tMany:\t0.5131\tMedium:\t0.2414\tFew:\t0.0607\n    > [Stats]\tMany:\t0.5131\tMedium:\t0.2414\tFew:\t0.0607\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.1228523469277416\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([2., 2., 2.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [38 | 200]\n---> Epoch: [38 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t1.2288\n    > [Train]\tLoss:\t1.2288\n    > [Test ]\tLoss:\t3.8391\tAcc:\t27.9800\n    > [Test ]\tLoss:\t3.8391\tAcc:\t27.9800\n    > [Stats]\tMany:\t0.4960\tMedium:\t0.2786\tFew:\t0.0290\n    > [Stats]\tMany:\t0.4960\tMedium:\t0.2786\tFew:\t0.0290\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.1326709913183937\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 3 // Min state: 0\ntensor([3., 3., 3.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [39 | 200]\n---> Epoch: [39 | 200]\n    > Max_state: 3, min_state: 0\n    > Max_state: 3, min_state: 0\n    > [Train]\tLoss:\t1.2163\n    > [Train]\tLoss:\t1.2163\n    > [Test ]\tLoss:\t3.6995\tAcc:\t28.3500\n    > [Test ]\tLoss:\t3.6995\tAcc:\t28.3500\n    > [Stats]\tMany:\t0.5274\tMedium:\t0.2497\tFew:\t0.0383\n    > [Stats]\tMany:\t0.5274\tMedium:\t0.2497\tFew:\t0.0383\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.131179026279458\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [40 | 200]\n---> Epoch: [40 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t1.3139\n    > [Train]\tLoss:\t1.3139\n    > [Test ]\tLoss:\t3.6816\tAcc:\t27.8400\n    > [Test ]\tLoss:\t3.6816\tAcc:\t27.8400\n    > [Stats]\tMany:\t0.5083\tMedium:\t0.2571\tFew:\t0.0350\n    > [Stats]\tMany:\t0.5083\tMedium:\t0.2571\tFew:\t0.0350\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.0842712993087793\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([3., 3., 3.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [41 | 200]\n---> Epoch: [41 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t1.2215\n    > [Train]\tLoss:\t1.2215\n    > [Test ]\tLoss:\t3.8017\tAcc:\t28.1900\n    > [Test ]\tLoss:\t3.8017\tAcc:\t28.1900\n    > [Stats]\tMany:\t0.5360\tMedium:\t0.2443\tFew:\t0.0293\n    > [Stats]\tMany:\t0.5360\tMedium:\t0.2443\tFew:\t0.0293\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.099672160697587\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [42 | 200]\n---> Epoch: [42 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t1.2014\n    > [Train]\tLoss:\t1.2014\n    > [Test ]\tLoss:\t3.6526\tAcc:\t28.4100\n    > [Test ]\tLoss:\t3.6526\tAcc:\t28.4100\n    > [Stats]\tMany:\t0.5269\tMedium:\t0.2606\tFew:\t0.0283\n    > [Stats]\tMany:\t0.5269\tMedium:\t0.2606\tFew:\t0.0283\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Best ]\tAcc:\t28.8100\tMany:\t56.2571\tMedium:\t23.2571\tFew:\t3.2667\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.1074402962884866\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [43 | 200]\n---> Epoch: [43 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t1.2399\n    > [Train]\tLoss:\t1.2399\n    > [Test ]\tLoss:\t3.5438\tAcc:\t31.3800\n    > [Test ]\tLoss:\t3.5438\tAcc:\t31.3800\n    > [Stats]\tMany:\t0.5397\tMedium:\t0.3317\tFew:\t0.0293\n    > [Stats]\tMany:\t0.5397\tMedium:\t0.3317\tFew:\t0.0293\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.0866045291773414\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [44 | 200]\n---> Epoch: [44 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t1.2440\n    > [Train]\tLoss:\t1.2440\n    > [Test ]\tLoss:\t3.9003\tAcc:\t28.1800\n    > [Test ]\tLoss:\t3.9003\tAcc:\t28.1800\n    > [Stats]\tMany:\t0.5423\tMedium:\t0.2471\tFew:\t0.0183\n    > [Stats]\tMany:\t0.5423\tMedium:\t0.2471\tFew:\t0.0183\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.076510960364623\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [45 | 200]\n---> Epoch: [45 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t1.2148\n    > [Train]\tLoss:\t1.2148\n    > [Test ]\tLoss:\t3.6121\tAcc:\t29.5800\n    > [Test ]\tLoss:\t3.6121\tAcc:\t29.5800\n    > [Stats]\tMany:\t0.5340\tMedium:\t0.2840\tFew:\t0.0317\n    > [Stats]\tMany:\t0.5340\tMedium:\t0.2840\tFew:\t0.0317\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.070886954158685\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [46 | 200]\n---> Epoch: [46 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t1.2350\n    > [Train]\tLoss:\t1.2350\n    > [Test ]\tLoss:\t3.6493\tAcc:\t29.9200\n    > [Test ]\tLoss:\t3.6493\tAcc:\t29.9200\n    > [Stats]\tMany:\t0.5620\tMedium:\t0.2591\tFew:\t0.0393\n    > [Stats]\tMany:\t0.5620\tMedium:\t0.2591\tFew:\t0.0393\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.0280480392154554\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [47 | 200]\n---> Epoch: [47 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t1.2370\n    > [Train]\tLoss:\t1.2370\n    > [Test ]\tLoss:\t3.8484\tAcc:\t28.7200\n    > [Test ]\tLoss:\t3.8484\tAcc:\t28.7200\n    > [Stats]\tMany:\t0.4894\tMedium:\t0.2783\tFew:\t0.0617\n    > [Stats]\tMany:\t0.4894\tMedium:\t0.2783\tFew:\t0.0617\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.0150021378318703\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [48 | 200]\n---> Epoch: [48 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t1.1544\n    > [Train]\tLoss:\t1.1544\n    > [Test ]\tLoss:\t4.0912\tAcc:\t28.2200\n    > [Test ]\tLoss:\t4.0912\tAcc:\t28.2200\n    > [Stats]\tMany:\t0.5163\tMedium:\t0.2654\tFew:\t0.0287\n    > [Stats]\tMany:\t0.5163\tMedium:\t0.2654\tFew:\t0.0287\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.022762295211832\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [49 | 200]\n---> Epoch: [49 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t1.1798\n    > [Train]\tLoss:\t1.1798\n    > [Test ]\tLoss:\t3.7715\tAcc:\t28.3600\n    > [Test ]\tLoss:\t3.7715\tAcc:\t28.3600\n    > [Stats]\tMany:\t0.5086\tMedium:\t0.2749\tFew:\t0.0313\n    > [Stats]\tMany:\t0.5086\tMedium:\t0.2749\tFew:\t0.0313\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.0302529523410593\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [50 | 200]\n---> Epoch: [50 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t1.1225\n    > [Train]\tLoss:\t1.1225\n    > [Test ]\tLoss:\t4.1608\tAcc:\t27.3200\n    > [Test ]\tLoss:\t4.1608\tAcc:\t27.3200\n    > [Stats]\tMany:\t0.5389\tMedium:\t0.2206\tFew:\t0.0247\n    > [Stats]\tMany:\t0.5389\tMedium:\t0.2206\tFew:\t0.0247\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.046634533755339\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [51 | 200]\n---> Epoch: [51 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t1.1023\n    > [Train]\tLoss:\t1.1023\n    > [Test ]\tLoss:\t3.6343\tAcc:\t30.4200\n    > [Test ]\tLoss:\t3.6343\tAcc:\t30.4200\n    > [Stats]\tMany:\t0.5654\tMedium:\t0.2600\tFew:\t0.0510\n    > [Stats]\tMany:\t0.5654\tMedium:\t0.2600\tFew:\t0.0510\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.0590244885632156\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [52 | 200]\n---> Epoch: [52 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t1.1927\n    > [Train]\tLoss:\t1.1927\n    > [Test ]\tLoss:\t3.7872\tAcc:\t29.7600\n    > [Test ]\tLoss:\t3.7872\tAcc:\t29.7600\n    > [Stats]\tMany:\t0.5543\tMedium:\t0.2731\tFew:\t0.0267\n    > [Stats]\tMany:\t0.5543\tMedium:\t0.2731\tFew:\t0.0267\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.0172089890698035\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [53 | 200]\n---> Epoch: [53 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t1.0958\n    > [Train]\tLoss:\t1.0958\n    > [Test ]\tLoss:\t3.6484\tAcc:\t30.0500\n    > [Test ]\tLoss:\t3.6484\tAcc:\t30.0500\n    > [Stats]\tMany:\t0.5697\tMedium:\t0.2674\tFew:\t0.0250\n    > [Stats]\tMany:\t0.5697\tMedium:\t0.2674\tFew:\t0.0250\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.038970301657372\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [54 | 200]\n---> Epoch: [54 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t1.0836\n    > [Train]\tLoss:\t1.0836\n    > [Test ]\tLoss:\t3.6755\tAcc:\t30.9300\n    > [Test ]\tLoss:\t3.6755\tAcc:\t30.9300\n    > [Stats]\tMany:\t0.5471\tMedium:\t0.3051\tFew:\t0.0367\n    > [Stats]\tMany:\t0.5471\tMedium:\t0.3051\tFew:\t0.0367\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"3.029018278899936\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [55 | 200]\n---> Epoch: [55 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t1.1142\n    > [Train]\tLoss:\t1.1142\n    > [Test ]\tLoss:\t3.5385\tAcc:\t31.1800\n    > [Test ]\tLoss:\t3.5385\tAcc:\t31.1800\n    > [Stats]\tMany:\t0.5569\tMedium:\t0.2937\tFew:\t0.0470\n    > [Stats]\tMany:\t0.5569\tMedium:\t0.2937\tFew:\t0.0470\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.980255962811501\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [56 | 200]\n---> Epoch: [56 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t1.1118\n    > [Train]\tLoss:\t1.1118\n    > [Test ]\tLoss:\t4.5630\tAcc:\t28.8500\n    > [Test ]\tLoss:\t4.5630\tAcc:\t28.8500\n    > [Stats]\tMany:\t0.5429\tMedium:\t0.2560\tFew:\t0.0297\n    > [Stats]\tMany:\t0.5429\tMedium:\t0.2560\tFew:\t0.0297\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.978942177399658\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [57 | 200]\n---> Epoch: [57 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t1.0023\n    > [Train]\tLoss:\t1.0023\n    > [Test ]\tLoss:\t4.0209\tAcc:\t29.7800\n    > [Test ]\tLoss:\t4.0209\tAcc:\t29.7800\n    > [Stats]\tMany:\t0.5500\tMedium:\t0.2731\tFew:\t0.0323\n    > [Stats]\tMany:\t0.5500\tMedium:\t0.2731\tFew:\t0.0323\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.9715154034526945\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [58 | 200]\n---> Epoch: [58 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t1.0517\n    > [Train]\tLoss:\t1.0517\n    > [Test ]\tLoss:\t3.7651\tAcc:\t30.3400\n    > [Test ]\tLoss:\t3.7651\tAcc:\t30.3400\n    > [Stats]\tMany:\t0.5917\tMedium:\t0.2483\tFew:\t0.0313\n    > [Stats]\tMany:\t0.5917\tMedium:\t0.2483\tFew:\t0.0313\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.9695436281036045\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [59 | 200]\n---> Epoch: [59 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t1.0419\n    > [Train]\tLoss:\t1.0419\n    > [Test ]\tLoss:\t3.9019\tAcc:\t28.7800\n    > [Test ]\tLoss:\t3.9019\tAcc:\t28.7800\n    > [Stats]\tMany:\t0.5434\tMedium:\t0.2509\tFew:\t0.0327\n    > [Stats]\tMany:\t0.5434\tMedium:\t0.2509\tFew:\t0.0327\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.9694925102440237\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [60 | 200]\n---> Epoch: [60 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t1.1002\n    > [Train]\tLoss:\t1.1002\n    > [Test ]\tLoss:\t3.7527\tAcc:\t29.3400\n    > [Test ]\tLoss:\t3.7527\tAcc:\t29.3400\n    > [Stats]\tMany:\t0.5446\tMedium:\t0.2711\tFew:\t0.0263\n    > [Stats]\tMany:\t0.5446\tMedium:\t0.2711\tFew:\t0.0263\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.9200872499593435\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [61 | 200]\n---> Epoch: [61 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t1.1060\n    > [Train]\tLoss:\t1.1060\n    > [Test ]\tLoss:\t4.1587\tAcc:\t28.7700\n    > [Test ]\tLoss:\t4.1587\tAcc:\t28.7700\n    > [Stats]\tMany:\t0.5271\tMedium:\t0.2666\tFew:\t0.0330\n    > [Stats]\tMany:\t0.5271\tMedium:\t0.2666\tFew:\t0.0330\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.906156088230942\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 8 // Min state: 0\ntensor([8., 8., 8.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [62 | 200]\n---> Epoch: [62 | 200]\n    > Max_state: 8, min_state: 0\n    > Max_state: 8, min_state: 0\n    > [Train]\tLoss:\t0.9916\n    > [Train]\tLoss:\t0.9916\n    > [Test ]\tLoss:\t3.7445\tAcc:\t30.4000\n    > [Test ]\tLoss:\t3.7445\tAcc:\t30.4000\n    > [Stats]\tMany:\t0.5691\tMedium:\t0.2660\tFew:\t0.0390\n    > [Stats]\tMany:\t0.5691\tMedium:\t0.2660\tFew:\t0.0390\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.899474897892799\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 9 // Min state: 0\ntensor([9., 9., 9.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [63 | 200]\n---> Epoch: [63 | 200]\n    > Max_state: 9, min_state: 0\n    > Max_state: 9, min_state: 0\n    > [Train]\tLoss:\t1.1043\n    > [Train]\tLoss:\t1.1043\n    > [Test ]\tLoss:\t3.8755\tAcc:\t30.9800\n    > [Test ]\tLoss:\t3.8755\tAcc:\t30.9800\n    > [Stats]\tMany:\t0.5689\tMedium:\t0.2729\tFew:\t0.0507\n    > [Stats]\tMany:\t0.5689\tMedium:\t0.2729\tFew:\t0.0507\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.8605045144797354\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 8 // Min state: 0\ntensor([8., 8., 8.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [64 | 200]\n---> Epoch: [64 | 200]\n    > Max_state: 8, min_state: 0\n    > Max_state: 8, min_state: 0\n    > [Train]\tLoss:\t0.9858\n    > [Train]\tLoss:\t0.9858\n    > [Test ]\tLoss:\t4.1171\tAcc:\t29.8600\n    > [Test ]\tLoss:\t4.1171\tAcc:\t29.8600\n    > [Stats]\tMany:\t0.5403\tMedium:\t0.2863\tFew:\t0.0310\n    > [Stats]\tMany:\t0.5403\tMedium:\t0.2863\tFew:\t0.0310\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.8614290439856487\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 9 // Min state: 0\ntensor([9., 9., 9.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [65 | 200]\n---> Epoch: [65 | 200]\n    > Max_state: 9, min_state: 0\n    > Max_state: 9, min_state: 0\n    > [Train]\tLoss:\t1.0451\n    > [Train]\tLoss:\t1.0451\n    > [Test ]\tLoss:\t3.8078\tAcc:\t31.3300\n    > [Test ]\tLoss:\t3.8078\tAcc:\t31.3300\n    > [Stats]\tMany:\t0.5657\tMedium:\t0.2906\tFew:\t0.0453\n    > [Stats]\tMany:\t0.5657\tMedium:\t0.2906\tFew:\t0.0453\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.8669852863401637\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 8 // Min state: 0\ntensor([8., 8., 8.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [66 | 200]\n---> Epoch: [66 | 200]\n    > Max_state: 8, min_state: 0\n    > Max_state: 8, min_state: 0\n    > [Train]\tLoss:\t1.0507\n    > [Train]\tLoss:\t1.0507\n    > [Test ]\tLoss:\t4.0974\tAcc:\t30.5400\n    > [Test ]\tLoss:\t4.0974\tAcc:\t30.5400\n    > [Stats]\tMany:\t0.5577\tMedium:\t0.2677\tFew:\t0.0550\n    > [Stats]\tMany:\t0.5577\tMedium:\t0.2677\tFew:\t0.0550\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.8709172684971667\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [67 | 200]\n---> Epoch: [67 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t1.0652\n    > [Train]\tLoss:\t1.0652\n    > [Test ]\tLoss:\t4.1049\tAcc:\t30.0500\n    > [Test ]\tLoss:\t4.1049\tAcc:\t30.0500\n    > [Stats]\tMany:\t0.5177\tMedium:\t0.3031\tFew:\t0.0440\n    > [Stats]\tMany:\t0.5177\tMedium:\t0.3031\tFew:\t0.0440\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.8423502194525843\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 8 // Min state: 0\ntensor([8., 8., 8.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [68 | 200]\n---> Epoch: [68 | 200]\n    > Max_state: 8, min_state: 0\n    > Max_state: 8, min_state: 0\n    > [Train]\tLoss:\t0.9940\n    > [Train]\tLoss:\t0.9940\n    > [Test ]\tLoss:\t4.3208\tAcc:\t29.0200\n    > [Test ]\tLoss:\t4.3208\tAcc:\t29.0200\n    > [Stats]\tMany:\t0.5131\tMedium:\t0.2694\tFew:\t0.0543\n    > [Stats]\tMany:\t0.5131\tMedium:\t0.2694\tFew:\t0.0543\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Best ]\tAcc:\t31.3800\tMany:\t53.9714\tMedium:\t33.1714\tFew:\t2.9333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.8760881320988827\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [69 | 200]\n---> Epoch: [69 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t1.0615\n    > [Train]\tLoss:\t1.0615\n    > [Test ]\tLoss:\t3.6577\tAcc:\t32.4400\n    > [Test ]\tLoss:\t3.6577\tAcc:\t32.4400\n    > [Stats]\tMany:\t0.5417\tMedium:\t0.3403\tFew:\t0.0523\n    > [Stats]\tMany:\t0.5417\tMedium:\t0.3403\tFew:\t0.0523\n    > [Best ]\tAcc:\t32.4400\tMany:\t54.1714\tMedium:\t34.0286\tFew:\t5.2333\n    > [Best ]\tAcc:\t32.4400\tMany:\t54.1714\tMedium:\t34.0286\tFew:\t5.2333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.87322315815309\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 8 // Min state: 0\ntensor([8., 8., 8.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [70 | 200]\n---> Epoch: [70 | 200]\n    > Max_state: 8, min_state: 0\n    > Max_state: 8, min_state: 0\n    > [Train]\tLoss:\t0.9220\n    > [Train]\tLoss:\t0.9220\n    > [Test ]\tLoss:\t3.6871\tAcc:\t32.0600\n    > [Test ]\tLoss:\t3.6871\tAcc:\t32.0600\n    > [Stats]\tMany:\t0.5889\tMedium:\t0.2929\tFew:\t0.0400\n    > [Stats]\tMany:\t0.5889\tMedium:\t0.2929\tFew:\t0.0400\n    > [Best ]\tAcc:\t32.4400\tMany:\t54.1714\tMedium:\t34.0286\tFew:\t5.2333\n    > [Best ]\tAcc:\t32.4400\tMany:\t54.1714\tMedium:\t34.0286\tFew:\t5.2333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.8894430837489122\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 9 // Min state: 0\ntensor([9., 9., 9.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [71 | 200]\n---> Epoch: [71 | 200]\n    > Max_state: 9, min_state: 0\n    > Max_state: 9, min_state: 0\n    > [Train]\tLoss:\t0.9323\n    > [Train]\tLoss:\t0.9323\n    > [Test ]\tLoss:\t4.0663\tAcc:\t30.9800\n    > [Test ]\tLoss:\t4.0663\tAcc:\t30.9800\n    > [Stats]\tMany:\t0.5634\tMedium:\t0.2800\tFew:\t0.0487\n    > [Stats]\tMany:\t0.5634\tMedium:\t0.2800\tFew:\t0.0487\n    > [Best ]\tAcc:\t32.4400\tMany:\t54.1714\tMedium:\t34.0286\tFew:\t5.2333\n    > [Best ]\tAcc:\t32.4400\tMany:\t54.1714\tMedium:\t34.0286\tFew:\t5.2333\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.8414084154998713\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([10., 10., 10.,  ...,  0.,  0.,  0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [72 | 200]\n---> Epoch: [72 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.9840\n    > [Train]\tLoss:\t0.9840\n    > [Test ]\tLoss:\t3.7931\tAcc:\t32.5700\n    > [Test ]\tLoss:\t3.7931\tAcc:\t32.5700\n    > [Stats]\tMany:\t0.5457\tMedium:\t0.3360\tFew:\t0.0570\n    > [Stats]\tMany:\t0.5457\tMedium:\t0.3360\tFew:\t0.0570\n    > [Best ]\tAcc:\t32.5700\tMany:\t54.5714\tMedium:\t33.6000\tFew:\t5.7000\n    > [Best ]\tAcc:\t32.5700\tMany:\t54.5714\tMedium:\t33.6000\tFew:\t5.7000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.8129654597874083\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([10., 10., 10.,  ...,  0.,  0.,  0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [73 | 200]\n---> Epoch: [73 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.9739\n    > [Train]\tLoss:\t0.9739\n    > [Test ]\tLoss:\t3.5559\tAcc:\t32.8200\n    > [Test ]\tLoss:\t3.5559\tAcc:\t32.8200\n    > [Stats]\tMany:\t0.5800\tMedium:\t0.3174\tFew:\t0.0470\n    > [Stats]\tMany:\t0.5800\tMedium:\t0.3174\tFew:\t0.0470\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.824997045077383\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([10., 10., 10.,  ...,  0.,  0.,  0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [74 | 200]\n---> Epoch: [74 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.9946\n    > [Train]\tLoss:\t0.9946\n    > [Test ]\tLoss:\t3.8102\tAcc:\t31.0400\n    > [Test ]\tLoss:\t3.8102\tAcc:\t31.0400\n    > [Stats]\tMany:\t0.5591\tMedium:\t0.3003\tFew:\t0.0320\n    > [Stats]\tMany:\t0.5591\tMedium:\t0.3003\tFew:\t0.0320\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.8096483370249823\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([10., 10., 10.,  ...,  0.,  0.,  0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [75 | 200]\n---> Epoch: [75 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.9263\n    > [Train]\tLoss:\t0.9263\n    > [Test ]\tLoss:\t3.8295\tAcc:\t30.9500\n    > [Test ]\tLoss:\t3.8295\tAcc:\t30.9500\n    > [Stats]\tMany:\t0.5426\tMedium:\t0.2920\tFew:\t0.0580\n    > [Stats]\tMany:\t0.5426\tMedium:\t0.2920\tFew:\t0.0580\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.8162759438212532\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([10., 10., 10.,  ...,  0.,  0.,  0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [76 | 200]\n---> Epoch: [76 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.9609\n    > [Train]\tLoss:\t0.9609\n    > [Test ]\tLoss:\t4.0637\tAcc:\t31.4000\n    > [Test ]\tLoss:\t4.0637\tAcc:\t31.4000\n    > [Stats]\tMany:\t0.5826\tMedium:\t0.2794\tFew:\t0.0410\n    > [Stats]\tMany:\t0.5826\tMedium:\t0.2794\tFew:\t0.0410\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.7966119627597665\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([10., 10., 10.,  ...,  0.,  0.,  0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [77 | 200]\n---> Epoch: [77 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.9143\n    > [Train]\tLoss:\t0.9143\n    > [Test ]\tLoss:\t3.7782\tAcc:\t32.0300\n    > [Test ]\tLoss:\t3.7782\tAcc:\t32.0300\n    > [Stats]\tMany:\t0.5760\tMedium:\t0.2934\tFew:\t0.0533\n    > [Stats]\tMany:\t0.5760\tMedium:\t0.2934\tFew:\t0.0533\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.7972920386893256\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 9 // Min state: 0\ntensor([9., 9., 9.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [78 | 200]\n---> Epoch: [78 | 200]\n    > Max_state: 9, min_state: 0\n    > Max_state: 9, min_state: 0\n    > [Train]\tLoss:\t1.0007\n    > [Train]\tLoss:\t1.0007\n    > [Test ]\tLoss:\t3.8789\tAcc:\t32.7500\n    > [Test ]\tLoss:\t3.8789\tAcc:\t32.7500\n    > [Stats]\tMany:\t0.5654\tMedium:\t0.3017\tFew:\t0.0800\n    > [Stats]\tMany:\t0.5654\tMedium:\t0.3017\tFew:\t0.0800\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.7738876677463913\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([10., 10., 10.,  ...,  0.,  0.,  0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [79 | 200]\n---> Epoch: [79 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.9565\n    > [Train]\tLoss:\t0.9565\n    > [Test ]\tLoss:\t3.7730\tAcc:\t31.1000\n    > [Test ]\tLoss:\t3.7730\tAcc:\t31.1000\n    > [Stats]\tMany:\t0.5740\tMedium:\t0.2957\tFew:\t0.0220\n    > [Stats]\tMany:\t0.5740\tMedium:\t0.2957\tFew:\t0.0220\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.785168084724005\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 9 // Min state: 0\ntensor([9., 9., 9.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [80 | 200]\n---> Epoch: [80 | 200]\n    > Max_state: 9, min_state: 0\n    > Max_state: 9, min_state: 0\n    > [Train]\tLoss:\t0.9360\n    > [Train]\tLoss:\t0.9360\n    > [Test ]\tLoss:\t3.6951\tAcc:\t30.3700\n    > [Test ]\tLoss:\t3.6951\tAcc:\t30.3700\n    > [Stats]\tMany:\t0.5600\tMedium:\t0.2671\tFew:\t0.0473\n    > [Stats]\tMany:\t0.5600\tMedium:\t0.2671\tFew:\t0.0473\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.7702478336334075\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([10., 10., 10.,  ...,  0.,  0.,  0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [81 | 200]\n---> Epoch: [81 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.9035\n    > [Train]\tLoss:\t0.9035\n    > [Test ]\tLoss:\t3.9885\tAcc:\t31.5200\n    > [Test ]\tLoss:\t3.9885\tAcc:\t31.5200\n    > [Stats]\tMany:\t0.5523\tMedium:\t0.2966\tFew:\t0.0603\n    > [Stats]\tMany:\t0.5523\tMedium:\t0.2966\tFew:\t0.0603\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.7319607885098844\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([10., 10., 10.,  ...,  1.,  1.,  1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [82 | 200]\n---> Epoch: [82 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.9469\n    > [Train]\tLoss:\t0.9469\n    > [Test ]\tLoss:\t3.9768\tAcc:\t31.4300\n    > [Test ]\tLoss:\t3.9768\tAcc:\t31.4300\n    > [Stats]\tMany:\t0.5809\tMedium:\t0.2743\tFew:\t0.0500\n    > [Stats]\tMany:\t0.5809\tMedium:\t0.2743\tFew:\t0.0500\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.751083566703872\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 9 // Min state: 0\ntensor([9., 9., 9.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [83 | 200]\n---> Epoch: [83 | 200]\n    > Max_state: 9, min_state: 0\n    > Max_state: 9, min_state: 0\n    > [Train]\tLoss:\t0.9846\n    > [Train]\tLoss:\t0.9846\n    > [Test ]\tLoss:\t4.0836\tAcc:\t31.3900\n    > [Test ]\tLoss:\t4.0836\tAcc:\t31.3900\n    > [Stats]\tMany:\t0.5520\tMedium:\t0.3140\tFew:\t0.0360\n    > [Stats]\tMany:\t0.5520\tMedium:\t0.3140\tFew:\t0.0360\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Best ]\tAcc:\t32.8200\tMany:\t58.0000\tMedium:\t31.7429\tFew:\t4.7000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.716800737208355\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([10., 10., 10.,  ...,  0.,  0.,  0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [84 | 200]\n---> Epoch: [84 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.9542\n    > [Train]\tLoss:\t0.9542\n    > [Test ]\tLoss:\t3.6797\tAcc:\t33.6600\n    > [Test ]\tLoss:\t3.6797\tAcc:\t33.6600\n    > [Stats]\tMany:\t0.5794\tMedium:\t0.3377\tFew:\t0.0520\n    > [Stats]\tMany:\t0.5794\tMedium:\t0.3377\tFew:\t0.0520\n    > [Best ]\tAcc:\t33.6600\tMany:\t57.9429\tMedium:\t33.7714\tFew:\t5.2000\n    > [Best ]\tAcc:\t33.6600\tMany:\t57.9429\tMedium:\t33.7714\tFew:\t5.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.7008495705960174\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 9 // Min state: 0\ntensor([9., 9., 9.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [85 | 200]\n---> Epoch: [85 | 200]\n    > Max_state: 9, min_state: 0\n    > Max_state: 9, min_state: 0\n    > [Train]\tLoss:\t0.9800\n    > [Train]\tLoss:\t0.9800\n    > [Test ]\tLoss:\t4.5056\tAcc:\t28.6800\n    > [Test ]\tLoss:\t4.5056\tAcc:\t28.6800\n    > [Stats]\tMany:\t0.5294\tMedium:\t0.2749\tFew:\t0.0177\n    > [Stats]\tMany:\t0.5294\tMedium:\t0.2749\tFew:\t0.0177\n    > [Best ]\tAcc:\t33.6600\tMany:\t57.9429\tMedium:\t33.7714\tFew:\t5.2000\n    > [Best ]\tAcc:\t33.6600\tMany:\t57.9429\tMedium:\t33.7714\tFew:\t5.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.689372383598457\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 8 // Min state: 0\ntensor([8., 8., 8.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [86 | 200]\n---> Epoch: [86 | 200]\n    > Max_state: 8, min_state: 0\n    > Max_state: 8, min_state: 0\n    > [Train]\tLoss:\t0.8399\n    > [Train]\tLoss:\t0.8399\n    > [Test ]\tLoss:\t4.2240\tAcc:\t30.6500\n    > [Test ]\tLoss:\t4.2240\tAcc:\t30.6500\n    > [Stats]\tMany:\t0.5766\tMedium:\t0.2646\tFew:\t0.0403\n    > [Stats]\tMany:\t0.5766\tMedium:\t0.2646\tFew:\t0.0403\n    > [Best ]\tAcc:\t33.6600\tMany:\t57.9429\tMedium:\t33.7714\tFew:\t5.2000\n    > [Best ]\tAcc:\t33.6600\tMany:\t57.9429\tMedium:\t33.7714\tFew:\t5.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.722626975311191\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [87 | 200]\n---> Epoch: [87 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.9137\n    > [Train]\tLoss:\t0.9137\n    > [Test ]\tLoss:\t3.7705\tAcc:\t32.9800\n    > [Test ]\tLoss:\t3.7705\tAcc:\t32.9800\n    > [Stats]\tMany:\t0.5594\tMedium:\t0.3306\tFew:\t0.0610\n    > [Stats]\tMany:\t0.5594\tMedium:\t0.3306\tFew:\t0.0610\n    > [Best ]\tAcc:\t33.6600\tMany:\t57.9429\tMedium:\t33.7714\tFew:\t5.2000\n    > [Best ]\tAcc:\t33.6600\tMany:\t57.9429\tMedium:\t33.7714\tFew:\t5.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.729955458220628\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 8 // Min state: 0\ntensor([8., 8., 8.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [88 | 200]\n---> Epoch: [88 | 200]\n    > Max_state: 8, min_state: 0\n    > Max_state: 8, min_state: 0\n    > [Train]\tLoss:\t1.0686\n    > [Train]\tLoss:\t1.0686\n    > [Test ]\tLoss:\t4.1821\tAcc:\t29.9900\n    > [Test ]\tLoss:\t4.1821\tAcc:\t29.9900\n    > [Stats]\tMany:\t0.5591\tMedium:\t0.2746\tFew:\t0.0270\n    > [Stats]\tMany:\t0.5591\tMedium:\t0.2746\tFew:\t0.0270\n    > [Best ]\tAcc:\t33.6600\tMany:\t57.9429\tMedium:\t33.7714\tFew:\t5.2000\n    > [Best ]\tAcc:\t33.6600\tMany:\t57.9429\tMedium:\t33.7714\tFew:\t5.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.675561667695428\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [89 | 200]\n---> Epoch: [89 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.8750\n    > [Train]\tLoss:\t0.8750\n    > [Test ]\tLoss:\t3.7719\tAcc:\t31.5100\n    > [Test ]\tLoss:\t3.7719\tAcc:\t31.5100\n    > [Stats]\tMany:\t0.5509\tMedium:\t0.2914\tFew:\t0.0677\n    > [Stats]\tMany:\t0.5509\tMedium:\t0.2914\tFew:\t0.0677\n    > [Best ]\tAcc:\t33.6600\tMany:\t57.9429\tMedium:\t33.7714\tFew:\t5.2000\n    > [Best ]\tAcc:\t33.6600\tMany:\t57.9429\tMedium:\t33.7714\tFew:\t5.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.692480545188322\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [90 | 200]\n---> Epoch: [90 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.9413\n    > [Train]\tLoss:\t0.9413\n    > [Test ]\tLoss:\t4.0906\tAcc:\t29.7100\n    > [Test ]\tLoss:\t4.0906\tAcc:\t29.7100\n    > [Stats]\tMany:\t0.5577\tMedium:\t0.2597\tFew:\t0.0367\n    > [Stats]\tMany:\t0.5577\tMedium:\t0.2597\tFew:\t0.0367\n    > [Best ]\tAcc:\t33.6600\tMany:\t57.9429\tMedium:\t33.7714\tFew:\t5.2000\n    > [Best ]\tAcc:\t33.6600\tMany:\t57.9429\tMedium:\t33.7714\tFew:\t5.2000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.671691193996411\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [91 | 200]\n---> Epoch: [91 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.8483\n    > [Train]\tLoss:\t0.8483\n    > [Test ]\tLoss:\t3.6995\tAcc:\t34.2300\n    > [Test ]\tLoss:\t3.6995\tAcc:\t34.2300\n    > [Stats]\tMany:\t0.5886\tMedium:\t0.3303\tFew:\t0.0690\n    > [Stats]\tMany:\t0.5886\tMedium:\t0.3303\tFew:\t0.0690\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.6977896529323813\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([4., 4., 4.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [92 | 200]\n---> Epoch: [92 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t0.9644\n    > [Train]\tLoss:\t0.9644\n    > [Test ]\tLoss:\t4.1468\tAcc:\t30.4200\n    > [Test ]\tLoss:\t4.1468\tAcc:\t30.4200\n    > [Stats]\tMany:\t0.5731\tMedium:\t0.2620\tFew:\t0.0397\n    > [Stats]\tMany:\t0.5731\tMedium:\t0.2620\tFew:\t0.0397\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.667028040876228\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [93 | 200]\n---> Epoch: [93 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.8048\n    > [Train]\tLoss:\t0.8048\n    > [Test ]\tLoss:\t4.2278\tAcc:\t30.0100\n    > [Test ]\tLoss:\t4.2278\tAcc:\t30.0100\n    > [Stats]\tMany:\t0.5329\tMedium:\t0.2829\tFew:\t0.0487\n    > [Stats]\tMany:\t0.5329\tMedium:\t0.2829\tFew:\t0.0487\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.6814587440575552\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [94 | 200]\n---> Epoch: [94 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.8036\n    > [Train]\tLoss:\t0.8036\n    > [Test ]\tLoss:\t3.8628\tAcc:\t32.3600\n    > [Test ]\tLoss:\t3.8628\tAcc:\t32.3600\n    > [Stats]\tMany:\t0.5623\tMedium:\t0.2931\tFew:\t0.0807\n    > [Stats]\tMany:\t0.5623\tMedium:\t0.2931\tFew:\t0.0807\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.6947909040952327\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [95 | 200]\n---> Epoch: [95 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.9073\n    > [Train]\tLoss:\t0.9073\n    > [Test ]\tLoss:\t4.2697\tAcc:\t30.3500\n    > [Test ]\tLoss:\t4.2697\tAcc:\t30.3500\n    > [Stats]\tMany:\t0.5729\tMedium:\t0.2343\tFew:\t0.0700\n    > [Stats]\tMany:\t0.5729\tMedium:\t0.2343\tFew:\t0.0700\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.6715600596694227\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [96 | 200]\n---> Epoch: [96 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t0.9047\n    > [Train]\tLoss:\t0.9047\n    > [Test ]\tLoss:\t3.9447\tAcc:\t32.0100\n    > [Test ]\tLoss:\t3.9447\tAcc:\t32.0100\n    > [Stats]\tMany:\t0.5186\tMedium:\t0.3477\tFew:\t0.0563\n    > [Stats]\tMany:\t0.5186\tMedium:\t0.3477\tFew:\t0.0563\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.6561677573730624\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [97 | 200]\n---> Epoch: [97 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.9142\n    > [Train]\tLoss:\t0.9142\n    > [Test ]\tLoss:\t4.3153\tAcc:\t30.5000\n    > [Test ]\tLoss:\t4.3153\tAcc:\t30.5000\n    > [Stats]\tMany:\t0.5571\tMedium:\t0.2774\tFew:\t0.0430\n    > [Stats]\tMany:\t0.5571\tMedium:\t0.2774\tFew:\t0.0430\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.643806090257338\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [98 | 200]\n---> Epoch: [98 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.9537\n    > [Train]\tLoss:\t0.9537\n    > [Test ]\tLoss:\t3.6660\tAcc:\t30.7200\n    > [Test ]\tLoss:\t3.6660\tAcc:\t30.7200\n    > [Stats]\tMany:\t0.5874\tMedium:\t0.2571\tFew:\t0.0387\n    > [Stats]\tMany:\t0.5874\tMedium:\t0.2571\tFew:\t0.0387\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.609063606437769\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [99 | 200]\n---> Epoch: [99 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.9301\n    > [Train]\tLoss:\t0.9301\n    > [Test ]\tLoss:\t3.7161\tAcc:\t33.9000\n    > [Test ]\tLoss:\t3.7161\tAcc:\t33.9000\n    > [Stats]\tMany:\t0.6066\tMedium:\t0.3069\tFew:\t0.0643\n    > [Stats]\tMany:\t0.6066\tMedium:\t0.3069\tFew:\t0.0643\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5988859483859947\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 8 // Min state: 0\ntensor([8., 8., 8.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [100 | 200]\n---> Epoch: [100 | 200]\n    > Max_state: 8, min_state: 0\n    > Max_state: 8, min_state: 0\n    > [Train]\tLoss:\t0.9362\n    > [Train]\tLoss:\t0.9362\n    > [Test ]\tLoss:\t3.9060\tAcc:\t32.6200\n    > [Test ]\tLoss:\t3.9060\tAcc:\t32.6200\n    > [Stats]\tMany:\t0.5637\tMedium:\t0.3214\tFew:\t0.0547\n    > [Stats]\tMany:\t0.5637\tMedium:\t0.3214\tFew:\t0.0547\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.607028832203573\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [101 | 200]\n---> Epoch: [101 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.8888\n    > [Train]\tLoss:\t0.8888\n    > [Test ]\tLoss:\t4.4912\tAcc:\t30.1200\n    > [Test ]\tLoss:\t4.4912\tAcc:\t30.1200\n    > [Stats]\tMany:\t0.5297\tMedium:\t0.2863\tFew:\t0.0520\n    > [Stats]\tMany:\t0.5297\tMedium:\t0.2863\tFew:\t0.0520\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.629407149829807\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [102 | 200]\n---> Epoch: [102 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.7880\n    > [Train]\tLoss:\t0.7880\n    > [Test ]\tLoss:\t4.2345\tAcc:\t30.2600\n    > [Test ]\tLoss:\t4.2345\tAcc:\t30.2600\n    > [Stats]\tMany:\t0.5131\tMedium:\t0.3037\tFew:\t0.0557\n    > [Stats]\tMany:\t0.5131\tMedium:\t0.3037\tFew:\t0.0557\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.6509667975846027\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [103 | 200]\n---> Epoch: [103 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.8986\n    > [Train]\tLoss:\t0.8986\n    > [Test ]\tLoss:\t4.5204\tAcc:\t28.3500\n    > [Test ]\tLoss:\t4.5204\tAcc:\t28.3500\n    > [Stats]\tMany:\t0.5429\tMedium:\t0.2380\tFew:\t0.0340\n    > [Stats]\tMany:\t0.5429\tMedium:\t0.2380\tFew:\t0.0340\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.617143488280471\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([4., 4., 4.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [104 | 200]\n---> Epoch: [104 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.8612\n    > [Train]\tLoss:\t0.8612\n    > [Test ]\tLoss:\t4.2168\tAcc:\t31.2500\n    > [Test ]\tLoss:\t4.2168\tAcc:\t31.2500\n    > [Stats]\tMany:\t0.5474\tMedium:\t0.3043\tFew:\t0.0480\n    > [Stats]\tMany:\t0.5474\tMedium:\t0.3043\tFew:\t0.0480\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.6007691851512367\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([3., 3., 3.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [105 | 200]\n---> Epoch: [105 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.8664\n    > [Train]\tLoss:\t0.8664\n    > [Test ]\tLoss:\t3.6697\tAcc:\t33.2100\n    > [Test ]\tLoss:\t3.6697\tAcc:\t33.2100\n    > [Stats]\tMany:\t0.5809\tMedium:\t0.3263\tFew:\t0.0487\n    > [Stats]\tMany:\t0.5809\tMedium:\t0.3263\tFew:\t0.0487\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5802276381262264\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [106 | 200]\n---> Epoch: [106 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.8597\n    > [Train]\tLoss:\t0.8597\n    > [Test ]\tLoss:\t3.8881\tAcc:\t32.5800\n    > [Test ]\tLoss:\t3.8881\tAcc:\t32.5800\n    > [Stats]\tMany:\t0.5283\tMedium:\t0.3154\tFew:\t0.1017\n    > [Stats]\tMany:\t0.5283\tMedium:\t0.3154\tFew:\t0.1017\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.59393166365625\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([3., 3., 3.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [107 | 200]\n---> Epoch: [107 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.8525\n    > [Train]\tLoss:\t0.8525\n    > [Test ]\tLoss:\t4.0480\tAcc:\t32.7700\n    > [Test ]\tLoss:\t4.0480\tAcc:\t32.7700\n    > [Stats]\tMany:\t0.6091\tMedium:\t0.2800\tFew:\t0.0550\n    > [Stats]\tMany:\t0.6091\tMedium:\t0.2800\tFew:\t0.0550\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.618291831492175\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [108 | 200]\n---> Epoch: [108 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.9432\n    > [Train]\tLoss:\t0.9432\n    > [Test ]\tLoss:\t3.6914\tAcc:\t33.1200\n    > [Test ]\tLoss:\t3.6914\tAcc:\t33.1200\n    > [Stats]\tMany:\t0.5677\tMedium:\t0.3080\tFew:\t0.0823\n    > [Stats]\tMany:\t0.5677\tMedium:\t0.3080\tFew:\t0.0823\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.544853571139662\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([3., 3., 3.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [109 | 200]\n---> Epoch: [109 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.9055\n    > [Train]\tLoss:\t0.9055\n    > [Test ]\tLoss:\t4.0261\tAcc:\t33.8300\n    > [Test ]\tLoss:\t4.0261\tAcc:\t33.8300\n    > [Stats]\tMany:\t0.5723\tMedium:\t0.3514\tFew:\t0.0500\n    > [Stats]\tMany:\t0.5723\tMedium:\t0.3514\tFew:\t0.0500\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Best ]\tAcc:\t34.2300\tMany:\t58.8571\tMedium:\t33.0286\tFew:\t6.9000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.547348323049611\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [110 | 200]\n---> Epoch: [110 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.8504\n    > [Train]\tLoss:\t0.8504\n    > [Test ]\tLoss:\t3.4943\tAcc:\t36.2800\n    > [Test ]\tLoss:\t3.4943\tAcc:\t36.2800\n    > [Stats]\tMany:\t0.6183\tMedium:\t0.3489\tFew:\t0.0810\n    > [Stats]\tMany:\t0.6183\tMedium:\t0.3489\tFew:\t0.0810\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.553994318657979\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([3., 3., 3.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [111 | 200]\n---> Epoch: [111 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.9497\n    > [Train]\tLoss:\t0.9497\n    > [Test ]\tLoss:\t4.0068\tAcc:\t32.7100\n    > [Test ]\tLoss:\t4.0068\tAcc:\t32.7100\n    > [Stats]\tMany:\t0.5606\tMedium:\t0.3229\tFew:\t0.0597\n    > [Stats]\tMany:\t0.5606\tMedium:\t0.3229\tFew:\t0.0597\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.554086374575768\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [112 | 200]\n---> Epoch: [112 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t0.8867\n    > [Train]\tLoss:\t0.8867\n    > [Test ]\tLoss:\t4.4369\tAcc:\t31.7600\n    > [Test ]\tLoss:\t4.4369\tAcc:\t31.7600\n    > [Stats]\tMany:\t0.5760\tMedium:\t0.2983\tFew:\t0.0387\n    > [Stats]\tMany:\t0.5760\tMedium:\t0.2983\tFew:\t0.0387\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.552654855682019\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [113 | 200]\n---> Epoch: [113 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.8771\n    > [Train]\tLoss:\t0.8771\n    > [Test ]\tLoss:\t4.3711\tAcc:\t31.1600\n    > [Test ]\tLoss:\t4.3711\tAcc:\t31.1600\n    > [Stats]\tMany:\t0.5523\tMedium:\t0.2920\tFew:\t0.0537\n    > [Stats]\tMany:\t0.5523\tMedium:\t0.2920\tFew:\t0.0537\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.530821708355841\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [114 | 200]\n---> Epoch: [114 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.8990\n    > [Train]\tLoss:\t0.8990\n    > [Test ]\tLoss:\t3.9025\tAcc:\t31.2300\n    > [Test ]\tLoss:\t3.9025\tAcc:\t31.2300\n    > [Stats]\tMany:\t0.5869\tMedium:\t0.2631\tFew:\t0.0493\n    > [Stats]\tMany:\t0.5869\tMedium:\t0.2631\tFew:\t0.0493\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5447812485792345\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [115 | 200]\n---> Epoch: [115 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.8341\n    > [Train]\tLoss:\t0.8341\n    > [Test ]\tLoss:\t4.0147\tAcc:\t32.3900\n    > [Test ]\tLoss:\t4.0147\tAcc:\t32.3900\n    > [Stats]\tMany:\t0.5523\tMedium:\t0.3414\tFew:\t0.0370\n    > [Stats]\tMany:\t0.5523\tMedium:\t0.3414\tFew:\t0.0370\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.602694538448104\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [116 | 200]\n---> Epoch: [116 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.8552\n    > [Train]\tLoss:\t0.8552\n    > [Test ]\tLoss:\t3.9690\tAcc:\t33.8300\n    > [Test ]\tLoss:\t3.9690\tAcc:\t33.8300\n    > [Stats]\tMany:\t0.5903\tMedium:\t0.3177\tFew:\t0.0683\n    > [Stats]\tMany:\t0.5903\tMedium:\t0.3177\tFew:\t0.0683\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.598751217291797\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [117 | 200]\n---> Epoch: [117 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.8918\n    > [Train]\tLoss:\t0.8918\n    > [Test ]\tLoss:\t4.1347\tAcc:\t32.8900\n    > [Test ]\tLoss:\t4.1347\tAcc:\t32.8900\n    > [Stats]\tMany:\t0.6006\tMedium:\t0.2906\tFew:\t0.0567\n    > [Stats]\tMany:\t0.6006\tMedium:\t0.2906\tFew:\t0.0567\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5764916252425616\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [118 | 200]\n---> Epoch: [118 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.8421\n    > [Train]\tLoss:\t0.8421\n    > [Test ]\tLoss:\t3.9625\tAcc:\t33.8800\n    > [Test ]\tLoss:\t3.9625\tAcc:\t33.8800\n    > [Stats]\tMany:\t0.5823\tMedium:\t0.3437\tFew:\t0.0490\n    > [Stats]\tMany:\t0.5823\tMedium:\t0.3437\tFew:\t0.0490\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.570983621735816\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [119 | 200]\n---> Epoch: [119 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.9135\n    > [Train]\tLoss:\t0.9135\n    > [Test ]\tLoss:\t4.3518\tAcc:\t29.4300\n    > [Test ]\tLoss:\t4.3518\tAcc:\t29.4300\n    > [Stats]\tMany:\t0.5171\tMedium:\t0.2809\tFew:\t0.0500\n    > [Stats]\tMany:\t0.5171\tMedium:\t0.2809\tFew:\t0.0500\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5221823711724047\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 8 // Min state: 0\ntensor([8., 8., 8.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [120 | 200]\n---> Epoch: [120 | 200]\n    > Max_state: 8, min_state: 0\n    > Max_state: 8, min_state: 0\n    > [Train]\tLoss:\t0.8454\n    > [Train]\tLoss:\t0.8454\n    > [Test ]\tLoss:\t4.0950\tAcc:\t31.6300\n    > [Test ]\tLoss:\t4.0950\tAcc:\t31.6300\n    > [Stats]\tMany:\t0.5411\tMedium:\t0.3046\tFew:\t0.0677\n    > [Stats]\tMany:\t0.5411\tMedium:\t0.3046\tFew:\t0.0677\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5365305364359054\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 9 // Min state: 0\ntensor([9., 9., 9.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [121 | 200]\n---> Epoch: [121 | 200]\n    > Max_state: 9, min_state: 0\n    > Max_state: 9, min_state: 0\n    > [Train]\tLoss:\t0.8349\n    > [Train]\tLoss:\t0.8349\n    > [Test ]\tLoss:\t4.0646\tAcc:\t32.5000\n    > [Test ]\tLoss:\t4.0646\tAcc:\t32.5000\n    > [Stats]\tMany:\t0.5911\tMedium:\t0.2951\tFew:\t0.0493\n    > [Stats]\tMany:\t0.5911\tMedium:\t0.2951\tFew:\t0.0493\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.505599113135001\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 8 // Min state: 0\ntensor([8., 8., 8.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [122 | 200]\n---> Epoch: [122 | 200]\n    > Max_state: 8, min_state: 0\n    > Max_state: 8, min_state: 0\n    > [Train]\tLoss:\t0.7713\n    > [Train]\tLoss:\t0.7713\n    > [Test ]\tLoss:\t4.3309\tAcc:\t30.5200\n    > [Test ]\tLoss:\t4.3309\tAcc:\t30.5200\n    > [Stats]\tMany:\t0.5477\tMedium:\t0.2666\tFew:\t0.0673\n    > [Stats]\tMany:\t0.5477\tMedium:\t0.2666\tFew:\t0.0673\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.540168677360378\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [123 | 200]\n---> Epoch: [123 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.8795\n    > [Train]\tLoss:\t0.8795\n    > [Test ]\tLoss:\t4.3557\tAcc:\t30.9400\n    > [Test ]\tLoss:\t4.3557\tAcc:\t30.9400\n    > [Stats]\tMany:\t0.5349\tMedium:\t0.2643\tFew:\t0.0990\n    > [Stats]\tMany:\t0.5349\tMedium:\t0.2643\tFew:\t0.0990\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.531384779067302\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [124 | 200]\n---> Epoch: [124 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.7894\n    > [Train]\tLoss:\t0.7894\n    > [Test ]\tLoss:\t3.6912\tAcc:\t34.3100\n    > [Test ]\tLoss:\t3.6912\tAcc:\t34.3100\n    > [Stats]\tMany:\t0.6203\tMedium:\t0.3131\tFew:\t0.0547\n    > [Stats]\tMany:\t0.6203\tMedium:\t0.3131\tFew:\t0.0547\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.561744515303383\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [125 | 200]\n---> Epoch: [125 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.8860\n    > [Train]\tLoss:\t0.8860\n    > [Test ]\tLoss:\t4.3174\tAcc:\t31.8800\n    > [Test ]\tLoss:\t4.3174\tAcc:\t31.8800\n    > [Stats]\tMany:\t0.5497\tMedium:\t0.3000\tFew:\t0.0713\n    > [Stats]\tMany:\t0.5497\tMedium:\t0.3000\tFew:\t0.0713\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5247295068688693\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 8 // Min state: 0\ntensor([4., 4., 4.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [126 | 200]\n---> Epoch: [126 | 200]\n    > Max_state: 8, min_state: 0\n    > Max_state: 8, min_state: 0\n    > [Train]\tLoss:\t0.7121\n    > [Train]\tLoss:\t0.7121\n    > [Test ]\tLoss:\t3.8306\tAcc:\t34.7700\n    > [Test ]\tLoss:\t3.8306\tAcc:\t34.7700\n    > [Stats]\tMany:\t0.6103\tMedium:\t0.3291\tFew:\t0.0630\n    > [Stats]\tMany:\t0.6103\tMedium:\t0.3291\tFew:\t0.0630\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5568771213059818\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [127 | 200]\n---> Epoch: [127 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.8952\n    > [Train]\tLoss:\t0.8952\n    > [Test ]\tLoss:\t3.9488\tAcc:\t31.6900\n    > [Test ]\tLoss:\t3.9488\tAcc:\t31.6900\n    > [Stats]\tMany:\t0.5760\tMedium:\t0.2840\tFew:\t0.0530\n    > [Stats]\tMany:\t0.5760\tMedium:\t0.2840\tFew:\t0.0530\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.497677509368321\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [128 | 200]\n---> Epoch: [128 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.7544\n    > [Train]\tLoss:\t0.7544\n    > [Test ]\tLoss:\t3.9517\tAcc:\t34.5600\n    > [Test ]\tLoss:\t3.9517\tAcc:\t34.5600\n    > [Stats]\tMany:\t0.5820\tMedium:\t0.3397\tFew:\t0.0767\n    > [Stats]\tMany:\t0.5820\tMedium:\t0.3397\tFew:\t0.0767\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5236616801436225\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [129 | 200]\n---> Epoch: [129 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.9319\n    > [Train]\tLoss:\t0.9319\n    > [Test ]\tLoss:\t4.1703\tAcc:\t33.5200\n    > [Test ]\tLoss:\t4.1703\tAcc:\t33.5200\n    > [Stats]\tMany:\t0.5863\tMedium:\t0.3157\tFew:\t0.0650\n    > [Stats]\tMany:\t0.5863\tMedium:\t0.3157\tFew:\t0.0650\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.508532892626038\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([4., 4., 4.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [130 | 200]\n---> Epoch: [130 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t0.7983\n    > [Train]\tLoss:\t0.7983\n    > [Test ]\tLoss:\t4.4185\tAcc:\t30.1900\n    > [Test ]\tLoss:\t4.4185\tAcc:\t30.1900\n    > [Stats]\tMany:\t0.5409\tMedium:\t0.2751\tFew:\t0.0543\n    > [Stats]\tMany:\t0.5409\tMedium:\t0.2751\tFew:\t0.0543\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5416712111436586\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [131 | 200]\n---> Epoch: [131 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.8135\n    > [Train]\tLoss:\t0.8135\n    > [Test ]\tLoss:\t4.1310\tAcc:\t33.2500\n    > [Test ]\tLoss:\t4.1310\tAcc:\t33.2500\n    > [Stats]\tMany:\t0.5823\tMedium:\t0.3074\tFew:\t0.0703\n    > [Stats]\tMany:\t0.5823\tMedium:\t0.3074\tFew:\t0.0703\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5176437646956886\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [132 | 200]\n---> Epoch: [132 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.9105\n    > [Train]\tLoss:\t0.9105\n    > [Test ]\tLoss:\t4.1894\tAcc:\t29.5800\n    > [Test ]\tLoss:\t4.1894\tAcc:\t29.5800\n    > [Stats]\tMany:\t0.5751\tMedium:\t0.2383\tFew:\t0.0370\n    > [Stats]\tMany:\t0.5751\tMedium:\t0.2383\tFew:\t0.0370\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5137387708817616\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [133 | 200]\n---> Epoch: [133 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.8165\n    > [Train]\tLoss:\t0.8165\n    > [Test ]\tLoss:\t3.8135\tAcc:\t33.5200\n    > [Test ]\tLoss:\t3.8135\tAcc:\t33.5200\n    > [Stats]\tMany:\t0.5949\tMedium:\t0.3114\tFew:\t0.0600\n    > [Stats]\tMany:\t0.5949\tMedium:\t0.3114\tFew:\t0.0600\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.524394080716072\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 8 // Min state: 0\ntensor([8., 8., 8.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [134 | 200]\n---> Epoch: [134 | 200]\n    > Max_state: 8, min_state: 0\n    > Max_state: 8, min_state: 0\n    > [Train]\tLoss:\t0.9134\n    > [Train]\tLoss:\t0.9134\n    > [Test ]\tLoss:\t4.0652\tAcc:\t32.1200\n    > [Test ]\tLoss:\t4.0652\tAcc:\t32.1200\n    > [Stats]\tMany:\t0.5451\tMedium:\t0.3214\tFew:\t0.0597\n    > [Stats]\tMany:\t0.5451\tMedium:\t0.3214\tFew:\t0.0597\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5149456691395784\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [135 | 200]\n---> Epoch: [135 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.9068\n    > [Train]\tLoss:\t0.9068\n    > [Test ]\tLoss:\t3.8732\tAcc:\t33.4200\n    > [Test ]\tLoss:\t3.8732\tAcc:\t33.4200\n    > [Stats]\tMany:\t0.5969\tMedium:\t0.3100\tFew:\t0.0560\n    > [Stats]\tMany:\t0.5969\tMedium:\t0.3100\tFew:\t0.0560\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5082688568183116\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [136 | 200]\n---> Epoch: [136 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.8999\n    > [Train]\tLoss:\t0.8999\n    > [Test ]\tLoss:\t3.8832\tAcc:\t31.8400\n    > [Test ]\tLoss:\t3.8832\tAcc:\t31.8400\n    > [Stats]\tMany:\t0.5866\tMedium:\t0.2866\tFew:\t0.0427\n    > [Stats]\tMany:\t0.5866\tMedium:\t0.2866\tFew:\t0.0427\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.492755915093056\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [137 | 200]\n---> Epoch: [137 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.7786\n    > [Train]\tLoss:\t0.7786\n    > [Test ]\tLoss:\t4.0187\tAcc:\t32.9800\n    > [Test ]\tLoss:\t4.0187\tAcc:\t32.9800\n    > [Stats]\tMany:\t0.5531\tMedium:\t0.3340\tFew:\t0.0643\n    > [Stats]\tMany:\t0.5531\tMedium:\t0.3340\tFew:\t0.0643\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.513462776952832\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 8 // Min state: 0\ntensor([8., 8., 8.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [138 | 200]\n---> Epoch: [138 | 200]\n    > Max_state: 8, min_state: 0\n    > Max_state: 8, min_state: 0\n    > [Train]\tLoss:\t0.7959\n    > [Train]\tLoss:\t0.7959\n    > [Test ]\tLoss:\t4.2880\tAcc:\t32.0700\n    > [Test ]\tLoss:\t4.2880\tAcc:\t32.0700\n    > [Stats]\tMany:\t0.5651\tMedium:\t0.3069\tFew:\t0.0517\n    > [Stats]\tMany:\t0.5651\tMedium:\t0.3069\tFew:\t0.0517\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.505948725602071\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [139 | 200]\n---> Epoch: [139 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.8290\n    > [Train]\tLoss:\t0.8290\n    > [Test ]\tLoss:\t4.6420\tAcc:\t29.0200\n    > [Test ]\tLoss:\t4.6420\tAcc:\t29.0200\n    > [Stats]\tMany:\t0.5009\tMedium:\t0.2754\tFew:\t0.0617\n    > [Stats]\tMany:\t0.5009\tMedium:\t0.2754\tFew:\t0.0617\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.4848021131168485\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [140 | 200]\n---> Epoch: [140 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.7738\n    > [Train]\tLoss:\t0.7738\n    > [Test ]\tLoss:\t3.8959\tAcc:\t32.0500\n    > [Test ]\tLoss:\t3.8959\tAcc:\t32.0500\n    > [Stats]\tMany:\t0.5800\tMedium:\t0.2980\tFew:\t0.0440\n    > [Stats]\tMany:\t0.5800\tMedium:\t0.2980\tFew:\t0.0440\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.4848643625591333\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [141 | 200]\n---> Epoch: [141 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.7535\n    > [Train]\tLoss:\t0.7535\n    > [Test ]\tLoss:\t4.2462\tAcc:\t32.6700\n    > [Test ]\tLoss:\t4.2462\tAcc:\t32.6700\n    > [Stats]\tMany:\t0.5783\tMedium:\t0.3189\tFew:\t0.0423\n    > [Stats]\tMany:\t0.5783\tMedium:\t0.3189\tFew:\t0.0423\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5154146175525285\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [142 | 200]\n---> Epoch: [142 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.8158\n    > [Train]\tLoss:\t0.8158\n    > [Test ]\tLoss:\t4.4585\tAcc:\t29.9400\n    > [Test ]\tLoss:\t4.4585\tAcc:\t29.9400\n    > [Stats]\tMany:\t0.5726\tMedium:\t0.2474\tFew:\t0.0413\n    > [Stats]\tMany:\t0.5726\tMedium:\t0.2474\tFew:\t0.0413\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.52462885192997\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [143 | 200]\n---> Epoch: [143 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.7762\n    > [Train]\tLoss:\t0.7762\n    > [Test ]\tLoss:\t4.2981\tAcc:\t32.8100\n    > [Test ]\tLoss:\t4.2981\tAcc:\t32.8100\n    > [Stats]\tMany:\t0.6009\tMedium:\t0.3017\tFew:\t0.0407\n    > [Stats]\tMany:\t0.6009\tMedium:\t0.3017\tFew:\t0.0407\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5363054266898266\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [144 | 200]\n---> Epoch: [144 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.9247\n    > [Train]\tLoss:\t0.9247\n    > [Test ]\tLoss:\t4.1897\tAcc:\t32.5400\n    > [Test ]\tLoss:\t4.1897\tAcc:\t32.5400\n    > [Stats]\tMany:\t0.5609\tMedium:\t0.3306\tFew:\t0.0447\n    > [Stats]\tMany:\t0.5609\tMedium:\t0.3306\tFew:\t0.0447\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.510052238449536\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [145 | 200]\n---> Epoch: [145 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.7745\n    > [Train]\tLoss:\t0.7745\n    > [Test ]\tLoss:\t3.9204\tAcc:\t33.3300\n    > [Test ]\tLoss:\t3.9204\tAcc:\t33.3300\n    > [Stats]\tMany:\t0.5851\tMedium:\t0.3066\tFew:\t0.0707\n    > [Stats]\tMany:\t0.5851\tMedium:\t0.3066\tFew:\t0.0707\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.538849529509342\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([4., 4., 4.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [146 | 200]\n---> Epoch: [146 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t0.9132\n    > [Train]\tLoss:\t0.9132\n    > [Test ]\tLoss:\t4.6967\tAcc:\t30.0300\n    > [Test ]\tLoss:\t4.6967\tAcc:\t30.0300\n    > [Stats]\tMany:\t0.5134\tMedium:\t0.3066\tFew:\t0.0443\n    > [Stats]\tMany:\t0.5134\tMedium:\t0.3066\tFew:\t0.0443\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.4980962638739315\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([3., 3., 3.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [147 | 200]\n---> Epoch: [147 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.7415\n    > [Train]\tLoss:\t0.7415\n    > [Test ]\tLoss:\t3.9615\tAcc:\t35.5500\n    > [Test ]\tLoss:\t3.9615\tAcc:\t35.5500\n    > [Stats]\tMany:\t0.5760\tMedium:\t0.3663\tFew:\t0.0857\n    > [Stats]\tMany:\t0.5760\tMedium:\t0.3663\tFew:\t0.0857\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5174344752577387\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [148 | 200]\n---> Epoch: [148 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.8458\n    > [Train]\tLoss:\t0.8458\n    > [Test ]\tLoss:\t3.7495\tAcc:\t34.9000\n    > [Test ]\tLoss:\t3.7495\tAcc:\t34.9000\n    > [Stats]\tMany:\t0.6340\tMedium:\t0.2994\tFew:\t0.0743\n    > [Stats]\tMany:\t0.6340\tMedium:\t0.2994\tFew:\t0.0743\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.5251043132254565\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [149 | 200]\n---> Epoch: [149 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.9423\n    > [Train]\tLoss:\t0.9423\n    > [Test ]\tLoss:\t4.2354\tAcc:\t33.1600\n    > [Test ]\tLoss:\t4.2354\tAcc:\t33.1600\n    > [Stats]\tMany:\t0.5700\tMedium:\t0.3229\tFew:\t0.0637\n    > [Stats]\tMany:\t0.5700\tMedium:\t0.3229\tFew:\t0.0637\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.4486838424007344\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 4 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [150 | 200]\n---> Epoch: [150 | 200]\n    > Max_state: 4, min_state: 0\n    > Max_state: 4, min_state: 0\n    > [Train]\tLoss:\t0.8824\n    > [Train]\tLoss:\t0.8824\n    > [Test ]\tLoss:\t4.1039\tAcc:\t31.9800\n    > [Test ]\tLoss:\t4.1039\tAcc:\t31.9800\n    > [Stats]\tMany:\t0.5563\tMedium:\t0.3117\tFew:\t0.0533\n    > [Stats]\tMany:\t0.5563\tMedium:\t0.3117\tFew:\t0.0533\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.4421031624732974\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [151 | 200]\n---> Epoch: [151 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.7750\n    > [Train]\tLoss:\t0.7750\n    > [Test ]\tLoss:\t3.9662\tAcc:\t33.6700\n    > [Test ]\tLoss:\t3.9662\tAcc:\t33.6700\n    > [Stats]\tMany:\t0.5960\tMedium:\t0.3174\tFew:\t0.0567\n    > [Stats]\tMany:\t0.5960\tMedium:\t0.3174\tFew:\t0.0567\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.454850735735789\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [152 | 200]\n---> Epoch: [152 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.8467\n    > [Train]\tLoss:\t0.8467\n    > [Test ]\tLoss:\t3.9046\tAcc:\t32.0200\n    > [Test ]\tLoss:\t3.9046\tAcc:\t32.0200\n    > [Stats]\tMany:\t0.5960\tMedium:\t0.2694\tFew:\t0.0577\n    > [Stats]\tMany:\t0.5960\tMedium:\t0.2694\tFew:\t0.0577\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.444327849917349\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([7., 7., 7.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [153 | 200]\n---> Epoch: [153 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.9079\n    > [Train]\tLoss:\t0.9079\n    > [Test ]\tLoss:\t3.7968\tAcc:\t34.2600\n    > [Test ]\tLoss:\t3.7968\tAcc:\t34.2600\n    > [Stats]\tMany:\t0.5634\tMedium:\t0.3369\tFew:\t0.0917\n    > [Stats]\tMany:\t0.5634\tMedium:\t0.3369\tFew:\t0.0917\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.4123234085378584\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([6., 6., 6.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [154 | 200]\n---> Epoch: [154 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.9205\n    > [Train]\tLoss:\t0.9205\n    > [Test ]\tLoss:\t4.0024\tAcc:\t33.7700\n    > [Test ]\tLoss:\t4.0024\tAcc:\t33.7700\n    > [Stats]\tMany:\t0.5600\tMedium:\t0.3403\tFew:\t0.0753\n    > [Stats]\tMany:\t0.5600\tMedium:\t0.3403\tFew:\t0.0753\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.4287514028189867\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [155 | 200]\n---> Epoch: [155 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.8225\n    > [Train]\tLoss:\t0.8225\n    > [Test ]\tLoss:\t3.6808\tAcc:\t35.6400\n    > [Test ]\tLoss:\t3.6808\tAcc:\t35.6400\n    > [Stats]\tMany:\t0.6017\tMedium:\t0.3620\tFew:\t0.0637\n    > [Stats]\tMany:\t0.6017\tMedium:\t0.3620\tFew:\t0.0637\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.462566597066897\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([4., 4., 4.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [156 | 200]\n---> Epoch: [156 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.8971\n    > [Train]\tLoss:\t0.8971\n    > [Test ]\tLoss:\t4.3081\tAcc:\t31.1000\n    > [Test ]\tLoss:\t4.3081\tAcc:\t31.1000\n    > [Stats]\tMany:\t0.5780\tMedium:\t0.2734\tFew:\t0.0433\n    > [Stats]\tMany:\t0.5780\tMedium:\t0.2734\tFew:\t0.0433\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.435194445976238\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 5 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [157 | 200]\n---> Epoch: [157 | 200]\n    > Max_state: 5, min_state: 0\n    > Max_state: 5, min_state: 0\n    > [Train]\tLoss:\t0.7971\n    > [Train]\tLoss:\t0.7971\n    > [Test ]\tLoss:\t4.2790\tAcc:\t32.0100\n    > [Test ]\tLoss:\t4.2790\tAcc:\t32.0100\n    > [Stats]\tMany:\t0.5814\tMedium:\t0.2926\tFew:\t0.0473\n    > [Stats]\tMany:\t0.5814\tMedium:\t0.2926\tFew:\t0.0473\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.467248995489373\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([4., 4., 4.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [158 | 200]\n---> Epoch: [158 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.8553\n    > [Train]\tLoss:\t0.8553\n    > [Test ]\tLoss:\t3.8650\tAcc:\t33.7200\n    > [Test ]\tLoss:\t3.8650\tAcc:\t33.7200\n    > [Stats]\tMany:\t0.5869\tMedium:\t0.3266\tFew:\t0.0583\n    > [Stats]\tMany:\t0.5869\tMedium:\t0.3266\tFew:\t0.0583\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.465726661067189\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([5., 5., 5.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [159 | 200]\n---> Epoch: [159 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.8473\n    > [Train]\tLoss:\t0.8473\n    > [Test ]\tLoss:\t4.6234\tAcc:\t31.1800\n    > [Test ]\tLoss:\t4.6234\tAcc:\t31.1800\n    > [Stats]\tMany:\t0.5500\tMedium:\t0.2866\tFew:\t0.0633\n    > [Stats]\tMany:\t0.5500\tMedium:\t0.2866\tFew:\t0.0633\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.459380833405355\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 6 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [160 | 200]\n---> Epoch: [160 | 200]\n    > Max_state: 6, min_state: 0\n    > Max_state: 6, min_state: 0\n    > [Train]\tLoss:\t0.8534\n    > [Train]\tLoss:\t0.8534\n    > [Test ]\tLoss:\t3.7853\tAcc:\t34.8900\n    > [Test ]\tLoss:\t3.7853\tAcc:\t34.8900\n    > [Stats]\tMany:\t0.6109\tMedium:\t0.3377\tFew:\t0.0563\n    > [Stats]\tMany:\t0.6109\tMedium:\t0.3377\tFew:\t0.0563\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Best ]\tAcc:\t36.2800\tMany:\t61.8286\tMedium:\t34.8857\tFew:\t8.1000\n    > [Param]\tLR:\t0.10000000\n    > [Param]\tLR:\t0.10000000\n","output_type":"stream"},{"name":"stdout","text":"2.422123754642155\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 7 // Min state: 0\ntensor([5., 5., 5.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [161 | 200]\n---> Epoch: [161 | 200]\n    > Max_state: 7, min_state: 0\n    > Max_state: 7, min_state: 0\n    > [Train]\tLoss:\t0.7799\n    > [Train]\tLoss:\t0.7799\n    > [Test ]\tLoss:\t3.3879\tAcc:\t38.2400\n    > [Test ]\tLoss:\t3.3879\tAcc:\t38.2400\n    > [Stats]\tMany:\t0.6674\tMedium:\t0.3640\tFew:\t0.0713\n    > [Stats]\tMany:\t0.6674\tMedium:\t0.3640\tFew:\t0.0713\n    > [Best ]\tAcc:\t38.2400\tMany:\t66.7429\tMedium:\t36.4000\tFew:\t7.1333\n    > [Best ]\tAcc:\t38.2400\tMany:\t66.7429\tMedium:\t36.4000\tFew:\t7.1333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.423159542373071\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 8 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [162 | 200]\n---> Epoch: [162 | 200]\n    > Max_state: 8, min_state: 0\n    > Max_state: 8, min_state: 0\n    > [Train]\tLoss:\t0.8007\n    > [Train]\tLoss:\t0.8007\n    > [Test ]\tLoss:\t3.3423\tAcc:\t38.7600\n    > [Test ]\tLoss:\t3.3423\tAcc:\t38.7600\n    > [Stats]\tMany:\t0.6723\tMedium:\t0.3720\tFew:\t0.0737\n    > [Stats]\tMany:\t0.6723\tMedium:\t0.3720\tFew:\t0.0737\n    > [Best ]\tAcc:\t38.7600\tMany:\t67.2286\tMedium:\t37.2000\tFew:\t7.3667\n    > [Best ]\tAcc:\t38.7600\tMany:\t67.2286\tMedium:\t37.2000\tFew:\t7.3667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.4238366429974056\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 9 // Min state: 0\ntensor([5., 5., 5.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [163 | 200]\n---> Epoch: [163 | 200]\n    > Max_state: 9, min_state: 0\n    > Max_state: 9, min_state: 0\n    > [Train]\tLoss:\t0.7421\n    > [Train]\tLoss:\t0.7421\n    > [Test ]\tLoss:\t3.3160\tAcc:\t38.9500\n    > [Test ]\tLoss:\t3.3160\tAcc:\t38.9500\n    > [Stats]\tMany:\t0.6766\tMedium:\t0.3723\tFew:\t0.0747\n    > [Stats]\tMany:\t0.6766\tMedium:\t0.3723\tFew:\t0.0747\n    > [Best ]\tAcc:\t38.9500\tMany:\t67.6571\tMedium:\t37.2286\tFew:\t7.4667\n    > [Best ]\tAcc:\t38.9500\tMany:\t67.6571\tMedium:\t37.2286\tFew:\t7.4667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.425192232558848\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([4., 4., 4.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [164 | 200]\n---> Epoch: [164 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8190\n    > [Train]\tLoss:\t0.8190\n    > [Test ]\tLoss:\t3.3033\tAcc:\t39.6800\n    > [Test ]\tLoss:\t3.3033\tAcc:\t39.6800\n    > [Stats]\tMany:\t0.6749\tMedium:\t0.3871\tFew:\t0.0837\n    > [Stats]\tMany:\t0.6749\tMedium:\t0.3871\tFew:\t0.0837\n    > [Best ]\tAcc:\t39.6800\tMany:\t67.4857\tMedium:\t38.7143\tFew:\t8.3667\n    > [Best ]\tAcc:\t39.6800\tMany:\t67.4857\tMedium:\t38.7143\tFew:\t8.3667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.425246258223853\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([5., 5., 5.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [165 | 200]\n---> Epoch: [165 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8183\n    > [Train]\tLoss:\t0.8183\n    > [Test ]\tLoss:\t3.3138\tAcc:\t39.8300\n    > [Test ]\tLoss:\t3.3138\tAcc:\t39.8300\n    > [Stats]\tMany:\t0.6809\tMedium:\t0.3880\tFew:\t0.0807\n    > [Stats]\tMany:\t0.6809\tMedium:\t0.3880\tFew:\t0.0807\n    > [Best ]\tAcc:\t39.8300\tMany:\t68.0857\tMedium:\t38.8000\tFew:\t8.0667\n    > [Best ]\tAcc:\t39.8300\tMany:\t68.0857\tMedium:\t38.8000\tFew:\t8.0667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.424743018765109\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([6., 6., 6.,  ..., 0., 0., 0.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [166 | 200]\n---> Epoch: [166 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8282\n    > [Train]\tLoss:\t0.8282\n    > [Test ]\tLoss:\t3.3042\tAcc:\t39.8800\n    > [Test ]\tLoss:\t3.3042\tAcc:\t39.8800\n    > [Stats]\tMany:\t0.6783\tMedium:\t0.3880\tFew:\t0.0853\n    > [Stats]\tMany:\t0.6783\tMedium:\t0.3880\tFew:\t0.0853\n    > [Best ]\tAcc:\t39.8800\tMany:\t67.8286\tMedium:\t38.8000\tFew:\t8.5333\n    > [Best ]\tAcc:\t39.8800\tMany:\t67.8286\tMedium:\t38.8000\tFew:\t8.5333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.423866896563987\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([5., 5., 5.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [167 | 200]\n---> Epoch: [167 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.7678\n    > [Train]\tLoss:\t0.7678\n    > [Test ]\tLoss:\t3.2908\tAcc:\t40.1900\n    > [Test ]\tLoss:\t3.2908\tAcc:\t40.1900\n    > [Stats]\tMany:\t0.6846\tMedium:\t0.3897\tFew:\t0.0863\n    > [Stats]\tMany:\t0.6846\tMedium:\t0.3897\tFew:\t0.0863\n    > [Best ]\tAcc:\t40.1900\tMany:\t68.4571\tMedium:\t38.9714\tFew:\t8.6333\n    > [Best ]\tAcc:\t40.1900\tMany:\t68.4571\tMedium:\t38.9714\tFew:\t8.6333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.4240124264759983\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [168 | 200]\n---> Epoch: [168 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.7948\n    > [Train]\tLoss:\t0.7948\n    > [Test ]\tLoss:\t3.2864\tAcc:\t40.2300\n    > [Test ]\tLoss:\t3.2864\tAcc:\t40.2300\n    > [Stats]\tMany:\t0.6854\tMedium:\t0.3897\tFew:\t0.0867\n    > [Stats]\tMany:\t0.6854\tMedium:\t0.3897\tFew:\t0.0867\n    > [Best ]\tAcc:\t40.2300\tMany:\t68.5429\tMedium:\t38.9714\tFew:\t8.6667\n    > [Best ]\tAcc:\t40.2300\tMany:\t68.5429\tMedium:\t38.9714\tFew:\t8.6667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.423897119056817\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([5., 5., 5.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [169 | 200]\n---> Epoch: [169 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8081\n    > [Train]\tLoss:\t0.8081\n    > [Test ]\tLoss:\t3.2661\tAcc:\t40.3800\n    > [Test ]\tLoss:\t3.2661\tAcc:\t40.3800\n    > [Stats]\tMany:\t0.6843\tMedium:\t0.3937\tFew:\t0.0883\n    > [Stats]\tMany:\t0.6843\tMedium:\t0.3937\tFew:\t0.0883\n    > [Best ]\tAcc:\t40.3800\tMany:\t68.4286\tMedium:\t39.3714\tFew:\t8.8333\n    > [Best ]\tAcc:\t40.3800\tMany:\t68.4286\tMedium:\t39.3714\tFew:\t8.8333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.4239086617388663\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [170 | 200]\n---> Epoch: [170 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8318\n    > [Train]\tLoss:\t0.8318\n    > [Test ]\tLoss:\t3.2775\tAcc:\t40.4900\n    > [Test ]\tLoss:\t3.2775\tAcc:\t40.4900\n    > [Stats]\tMany:\t0.6889\tMedium:\t0.3923\tFew:\t0.0883\n    > [Stats]\tMany:\t0.6889\tMedium:\t0.3923\tFew:\t0.0883\n    > [Best ]\tAcc:\t40.4900\tMany:\t68.8857\tMedium:\t39.2286\tFew:\t8.8333\n    > [Best ]\tAcc:\t40.4900\tMany:\t68.8857\tMedium:\t39.2286\tFew:\t8.8333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.423322545808504\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([5., 5., 5.,  ..., 3., 3., 3.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [171 | 200]\n---> Epoch: [171 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8168\n    > [Train]\tLoss:\t0.8168\n    > [Test ]\tLoss:\t3.2983\tAcc:\t40.6300\n    > [Test ]\tLoss:\t3.2983\tAcc:\t40.6300\n    > [Stats]\tMany:\t0.6874\tMedium:\t0.3974\tFew:\t0.0887\n    > [Stats]\tMany:\t0.6874\tMedium:\t0.3974\tFew:\t0.0887\n    > [Best ]\tAcc:\t40.6300\tMany:\t68.7429\tMedium:\t39.7429\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.6300\tMany:\t68.7429\tMedium:\t39.7429\tFew:\t8.8667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.4226266778522896\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 9 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [172 | 200]\n---> Epoch: [172 | 200]\n    > Max_state: 9, min_state: 0\n    > Max_state: 9, min_state: 0\n    > [Train]\tLoss:\t0.7971\n    > [Train]\tLoss:\t0.7971\n    > [Test ]\tLoss:\t3.2959\tAcc:\t40.5400\n    > [Test ]\tLoss:\t3.2959\tAcc:\t40.5400\n    > [Stats]\tMany:\t0.6877\tMedium:\t0.3943\tFew:\t0.0890\n    > [Stats]\tMany:\t0.6877\tMedium:\t0.3943\tFew:\t0.0890\n    > [Best ]\tAcc:\t40.6300\tMany:\t68.7429\tMedium:\t39.7429\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.6300\tMany:\t68.7429\tMedium:\t39.7429\tFew:\t8.8667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.4220201878474765\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([5., 5., 5.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [173 | 200]\n---> Epoch: [173 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8146\n    > [Train]\tLoss:\t0.8146\n    > [Test ]\tLoss:\t3.3328\tAcc:\t40.4000\n    > [Test ]\tLoss:\t3.3328\tAcc:\t40.4000\n    > [Stats]\tMany:\t0.6897\tMedium:\t0.3903\tFew:\t0.0867\n    > [Stats]\tMany:\t0.6897\tMedium:\t0.3903\tFew:\t0.0867\n    > [Best ]\tAcc:\t40.6300\tMany:\t68.7429\tMedium:\t39.7429\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.6300\tMany:\t68.7429\tMedium:\t39.7429\tFew:\t8.8667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.4217192266222463\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [174 | 200]\n---> Epoch: [174 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8443\n    > [Train]\tLoss:\t0.8443\n    > [Test ]\tLoss:\t3.2839\tAcc:\t40.5700\n    > [Test ]\tLoss:\t3.2839\tAcc:\t40.5700\n    > [Stats]\tMany:\t0.6897\tMedium:\t0.3937\tFew:\t0.0883\n    > [Stats]\tMany:\t0.6897\tMedium:\t0.3937\tFew:\t0.0883\n    > [Best ]\tAcc:\t40.6300\tMany:\t68.7429\tMedium:\t39.7429\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.6300\tMany:\t68.7429\tMedium:\t39.7429\tFew:\t8.8667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.421114472286276\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([5., 5., 5.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [175 | 200]\n---> Epoch: [175 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.7975\n    > [Train]\tLoss:\t0.7975\n    > [Test ]\tLoss:\t3.2948\tAcc:\t40.8000\n    > [Test ]\tLoss:\t3.2948\tAcc:\t40.8000\n    > [Stats]\tMany:\t0.6923\tMedium:\t0.3994\tFew:\t0.0863\n    > [Stats]\tMany:\t0.6923\tMedium:\t0.3994\tFew:\t0.0863\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.2286\tMedium:\t39.9429\tFew:\t8.6333\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.2286\tMedium:\t39.9429\tFew:\t8.6333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.420407992553624\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 9 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [176 | 200]\n---> Epoch: [176 | 200]\n    > Max_state: 9, min_state: 0\n    > Max_state: 9, min_state: 0\n    > [Train]\tLoss:\t0.8362\n    > [Train]\tLoss:\t0.8362\n    > [Test ]\tLoss:\t3.3005\tAcc:\t40.7500\n    > [Test ]\tLoss:\t3.3005\tAcc:\t40.7500\n    > [Stats]\tMany:\t0.6869\tMedium:\t0.3994\tFew:\t0.0910\n    > [Stats]\tMany:\t0.6869\tMedium:\t0.3994\tFew:\t0.0910\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.2286\tMedium:\t39.9429\tFew:\t8.6333\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.2286\tMedium:\t39.9429\tFew:\t8.6333\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.4196777450279736\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([7., 7., 7.,  ..., 3., 3., 3.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [177 | 200]\n---> Epoch: [177 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8076\n    > [Train]\tLoss:\t0.8076\n    > [Test ]\tLoss:\t3.3058\tAcc:\t40.8000\n    > [Test ]\tLoss:\t3.3058\tAcc:\t40.8000\n    > [Stats]\tMany:\t0.6906\tMedium:\t0.3991\tFew:\t0.0887\n    > [Stats]\tMany:\t0.6906\tMedium:\t0.3991\tFew:\t0.0887\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.4194326705711418\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 9 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [178 | 200]\n---> Epoch: [178 | 200]\n    > Max_state: 9, min_state: 0\n    > Max_state: 9, min_state: 0\n    > [Train]\tLoss:\t0.8093\n    > [Train]\tLoss:\t0.8093\n    > [Test ]\tLoss:\t3.3028\tAcc:\t40.6400\n    > [Test ]\tLoss:\t3.3028\tAcc:\t40.6400\n    > [Stats]\tMany:\t0.6903\tMedium:\t0.3934\tFew:\t0.0903\n    > [Stats]\tMany:\t0.6903\tMedium:\t0.3934\tFew:\t0.0903\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.4196085938679412\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([5., 5., 5.,  ..., 3., 3., 3.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [179 | 200]\n---> Epoch: [179 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8018\n    > [Train]\tLoss:\t0.8018\n    > [Test ]\tLoss:\t3.3157\tAcc:\t40.5300\n    > [Test ]\tLoss:\t3.3157\tAcc:\t40.5300\n    > [Stats]\tMany:\t0.6903\tMedium:\t0.3949\tFew:\t0.0850\n    > [Stats]\tMany:\t0.6903\tMedium:\t0.3949\tFew:\t0.0850\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.419037910462952\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [180 | 200]\n---> Epoch: [180 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8613\n    > [Train]\tLoss:\t0.8613\n    > [Test ]\tLoss:\t3.3068\tAcc:\t40.7800\n    > [Test ]\tLoss:\t3.3068\tAcc:\t40.7800\n    > [Stats]\tMany:\t0.6914\tMedium:\t0.3969\tFew:\t0.0897\n    > [Stats]\tMany:\t0.6914\tMedium:\t0.3969\tFew:\t0.0897\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00100000\n    > [Param]\tLR:\t0.00100000\n","output_type":"stream"},{"name":"stdout","text":"2.4193961948683156\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([7., 7., 7.,  ..., 3., 3., 3.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [181 | 200]\n---> Epoch: [181 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8121\n    > [Train]\tLoss:\t0.8121\n    > [Test ]\tLoss:\t3.2955\tAcc:\t40.6500\n    > [Test ]\tLoss:\t3.2955\tAcc:\t40.6500\n    > [Stats]\tMany:\t0.6931\tMedium:\t0.3937\tFew:\t0.0870\n    > [Stats]\tMany:\t0.6931\tMedium:\t0.3937\tFew:\t0.0870\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.4193962777886453\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [182 | 200]\n---> Epoch: [182 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8118\n    > [Train]\tLoss:\t0.8118\n    > [Test ]\tLoss:\t3.3012\tAcc:\t40.6900\n    > [Test ]\tLoss:\t3.3012\tAcc:\t40.6900\n    > [Stats]\tMany:\t0.6920\tMedium:\t0.3951\tFew:\t0.0880\n    > [Stats]\tMany:\t0.6920\tMedium:\t0.3951\tFew:\t0.0880\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.419395109870664\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([5., 5., 5.,  ..., 3., 3., 3.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [183 | 200]\n---> Epoch: [183 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8011\n    > [Train]\tLoss:\t0.8011\n    > [Test ]\tLoss:\t3.2931\tAcc:\t40.6600\n    > [Test ]\tLoss:\t3.2931\tAcc:\t40.6600\n    > [Stats]\tMany:\t0.6914\tMedium:\t0.3949\tFew:\t0.0880\n    > [Stats]\tMany:\t0.6914\tMedium:\t0.3949\tFew:\t0.0880\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.4193921132198213\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([4., 4., 4.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [184 | 200]\n---> Epoch: [184 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8137\n    > [Train]\tLoss:\t0.8137\n    > [Test ]\tLoss:\t3.3053\tAcc:\t40.6700\n    > [Test ]\tLoss:\t3.3053\tAcc:\t40.6700\n    > [Stats]\tMany:\t0.6937\tMedium:\t0.3920\tFew:\t0.0890\n    > [Stats]\tMany:\t0.6937\tMedium:\t0.3920\tFew:\t0.0890\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.419390107781187\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([5., 5., 5.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [185 | 200]\n---> Epoch: [185 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.7506\n    > [Train]\tLoss:\t0.7506\n    > [Test ]\tLoss:\t3.2864\tAcc:\t40.5400\n    > [Test ]\tLoss:\t3.2864\tAcc:\t40.5400\n    > [Stats]\tMany:\t0.6946\tMedium:\t0.3911\tFew:\t0.0847\n    > [Stats]\tMany:\t0.6946\tMedium:\t0.3911\tFew:\t0.0847\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.4193878734640073\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [186 | 200]\n---> Epoch: [186 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8107\n    > [Train]\tLoss:\t0.8107\n    > [Test ]\tLoss:\t3.3116\tAcc:\t40.6600\n    > [Test ]\tLoss:\t3.3116\tAcc:\t40.6600\n    > [Stats]\tMany:\t0.6920\tMedium:\t0.3960\tFew:\t0.0860\n    > [Stats]\tMany:\t0.6920\tMedium:\t0.3960\tFew:\t0.0860\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.419379334159367\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([7., 7., 7.,  ..., 3., 3., 3.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [187 | 200]\n---> Epoch: [187 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.7573\n    > [Train]\tLoss:\t0.7573\n    > [Test ]\tLoss:\t3.2878\tAcc:\t40.6400\n    > [Test ]\tLoss:\t3.2878\tAcc:\t40.6400\n    > [Stats]\tMany:\t0.6903\tMedium:\t0.3954\tFew:\t0.0880\n    > [Stats]\tMany:\t0.6903\tMedium:\t0.3954\tFew:\t0.0880\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.419372476321679\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [188 | 200]\n---> Epoch: [188 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8339\n    > [Train]\tLoss:\t0.8339\n    > [Test ]\tLoss:\t3.3062\tAcc:\t40.6800\n    > [Test ]\tLoss:\t3.3062\tAcc:\t40.6800\n    > [Stats]\tMany:\t0.6900\tMedium:\t0.3963\tFew:\t0.0887\n    > [Stats]\tMany:\t0.6900\tMedium:\t0.3963\tFew:\t0.0887\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.419371556628564\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([5., 5., 5.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [189 | 200]\n---> Epoch: [189 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8291\n    > [Train]\tLoss:\t0.8291\n    > [Test ]\tLoss:\t3.3152\tAcc:\t40.6800\n    > [Test ]\tLoss:\t3.3152\tAcc:\t40.6800\n    > [Stats]\tMany:\t0.6946\tMedium:\t0.3926\tFew:\t0.0877\n    > [Stats]\tMany:\t0.6946\tMedium:\t0.3926\tFew:\t0.0877\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.419368553008506\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [190 | 200]\n---> Epoch: [190 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8155\n    > [Train]\tLoss:\t0.8155\n    > [Test ]\tLoss:\t3.3192\tAcc:\t40.7900\n    > [Test ]\tLoss:\t3.3192\tAcc:\t40.7900\n    > [Stats]\tMany:\t0.6897\tMedium:\t0.4003\tFew:\t0.0880\n    > [Stats]\tMany:\t0.6897\tMedium:\t0.4003\tFew:\t0.0880\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.419366013463849\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([7., 7., 7.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [191 | 200]\n---> Epoch: [191 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.7788\n    > [Train]\tLoss:\t0.7788\n    > [Test ]\tLoss:\t3.3006\tAcc:\t40.5500\n    > [Test ]\tLoss:\t3.3006\tAcc:\t40.5500\n    > [Stats]\tMany:\t0.6926\tMedium:\t0.3937\tFew:\t0.0843\n    > [Stats]\tMany:\t0.6926\tMedium:\t0.3937\tFew:\t0.0843\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.4193659454734404\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [192 | 200]\n---> Epoch: [192 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8243\n    > [Train]\tLoss:\t0.8243\n    > [Test ]\tLoss:\t3.3101\tAcc:\t40.6400\n    > [Test ]\tLoss:\t3.3101\tAcc:\t40.6400\n    > [Stats]\tMany:\t0.6926\tMedium:\t0.3946\tFew:\t0.0863\n    > [Stats]\tMany:\t0.6926\tMedium:\t0.3946\tFew:\t0.0863\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.4193629419624454\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([7., 7., 7.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [193 | 200]\n---> Epoch: [193 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.7802\n    > [Train]\tLoss:\t0.7802\n    > [Test ]\tLoss:\t3.3003\tAcc:\t40.6200\n    > [Test ]\tLoss:\t3.3003\tAcc:\t40.6200\n    > [Stats]\tMany:\t0.6940\tMedium:\t0.3940\tFew:\t0.0847\n    > [Stats]\tMany:\t0.6940\tMedium:\t0.3940\tFew:\t0.0847\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.4193645917287845\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [194 | 200]\n---> Epoch: [194 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.7972\n    > [Train]\tLoss:\t0.7972\n    > [Test ]\tLoss:\t3.3149\tAcc:\t40.7100\n    > [Test ]\tLoss:\t3.3149\tAcc:\t40.7100\n    > [Stats]\tMany:\t0.6946\tMedium:\t0.3949\tFew:\t0.0860\n    > [Stats]\tMany:\t0.6946\tMedium:\t0.3949\tFew:\t0.0860\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.4193596662706436\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([7., 7., 7.,  ..., 3., 3., 3.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [195 | 200]\n---> Epoch: [195 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.7981\n    > [Train]\tLoss:\t0.7981\n    > [Test ]\tLoss:\t3.3180\tAcc:\t40.6500\n    > [Test ]\tLoss:\t3.3180\tAcc:\t40.6500\n    > [Stats]\tMany:\t0.6914\tMedium:\t0.3951\tFew:\t0.0873\n    > [Stats]\tMany:\t0.6914\tMedium:\t0.3951\tFew:\t0.0873\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.419356166181532\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [196 | 200]\n---> Epoch: [196 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.7782\n    > [Train]\tLoss:\t0.7782\n    > [Test ]\tLoss:\t3.2900\tAcc:\t40.5700\n    > [Test ]\tLoss:\t3.2900\tAcc:\t40.5700\n    > [Stats]\tMany:\t0.6926\tMedium:\t0.3931\tFew:\t0.0857\n    > [Stats]\tMany:\t0.6926\tMedium:\t0.3931\tFew:\t0.0857\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.419354165333579\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([7., 7., 7.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [197 | 200]\n---> Epoch: [197 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.7794\n    > [Train]\tLoss:\t0.7794\n    > [Test ]\tLoss:\t3.3009\tAcc:\t40.5700\n    > [Test ]\tLoss:\t3.3009\tAcc:\t40.5700\n    > [Stats]\tMany:\t0.6917\tMedium:\t0.3940\tFew:\t0.0857\n    > [Stats]\tMany:\t0.6917\tMedium:\t0.3940\tFew:\t0.0857\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.4193481926847116\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [198 | 200]\n---> Epoch: [198 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.7390\n    > [Train]\tLoss:\t0.7390\n    > [Test ]\tLoss:\t3.3013\tAcc:\t40.7500\n    > [Test ]\tLoss:\t3.3013\tAcc:\t40.7500\n    > [Stats]\tMany:\t0.6940\tMedium:\t0.3951\tFew:\t0.0877\n    > [Stats]\tMany:\t0.6940\tMedium:\t0.3951\tFew:\t0.0877\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Best ]\tAcc:\t40.8000\tMany:\t69.0571\tMedium:\t39.9143\tFew:\t8.8667\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.419342772965774\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([7., 7., 7.,  ..., 1., 1., 1.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [199 | 200]\n---> Epoch: [199 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8189\n    > [Train]\tLoss:\t0.8189\n    > [Test ]\tLoss:\t3.2779\tAcc:\t40.8300\n    > [Test ]\tLoss:\t3.2779\tAcc:\t40.8300\n    > [Stats]\tMany:\t0.6934\tMedium:\t0.3983\tFew:\t0.0873\n    > [Stats]\tMany:\t0.6934\tMedium:\t0.3983\tFew:\t0.0873\n    > [Best ]\tAcc:\t40.8300\tMany:\t69.3429\tMedium:\t39.8286\tFew:\t8.7333\n    > [Best ]\tAcc:\t40.8300\tMany:\t69.3429\tMedium:\t39.8286\tFew:\t8.7333\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n","output_type":"stream"},{"name":"stdout","text":"2.4193420503480834\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"name":"stdout","text":"Max state: 10 // Min state: 0\ntensor([6., 6., 6.,  ..., 2., 2., 2.])\n","output_type":"stream"},{"name":"stderr","text":"---> Epoch: [200 | 200]\n---> Epoch: [200 | 200]\n    > Max_state: 10, min_state: 0\n    > Max_state: 10, min_state: 0\n    > [Train]\tLoss:\t0.8617\n    > [Train]\tLoss:\t0.8617\n    > [Test ]\tLoss:\t3.3248\tAcc:\t40.6500\n    > [Test ]\tLoss:\t3.3248\tAcc:\t40.6500\n    > [Stats]\tMany:\t0.6889\tMedium:\t0.3960\tFew:\t0.0893\n    > [Stats]\tMany:\t0.6889\tMedium:\t0.3960\tFew:\t0.0893\n    > [Best ]\tAcc:\t40.8300\tMany:\t69.3429\tMedium:\t39.8286\tFew:\t8.7333\n    > [Best ]\tAcc:\t40.8300\tMany:\t69.3429\tMedium:\t39.8286\tFew:\t8.7333\n    > [Param]\tLR:\t0.00001000\n    > [Param]\tLR:\t0.00001000\n---> Final performance...\n---> Final performance...\n    > best bAcc (test):\t40.83\n    > best bAcc (test):\t40.83\n    > best statistics:\tMany:\t0.6934285759925842\tMed:\t0.39828571677207947\tFew:\t0.0873333290219307\n    > best statistics:\tMany:\t0.6934285759925842\tMed:\t0.39828571677207947\tFew:\t0.0873333290219307\n---> Training Time: 0:27:10.79\n---> Training Time: 0:27:10.79\n","output_type":"stream"},{"name":"stdout","text":"2.419335040533885\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming variance_l1_norm is the calculated variance # If you have multiple variances, create a list\n\nplt.figure(figsize=(8, 6))\nplt.plot(range(len(variance_list1)), variance_list1, color='blue', linestyle='-', linewidth=2)\nplt.plot(range(len(variance_list)), variance_list, color='red', linestyle='-', linewidth=2)\nplt.xlabel('Classes')\nplt.ylabel('Variance of L1-norm')\nplt.title('Variance of L1-norm Among Classes')\nplt.xticks(range(len(variance_list1)), [f'Class_{i}' for i in range(len(variance_list1))])\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-24T13:03:40.751303Z","iopub.execute_input":"2024-01-24T13:03:40.751714Z","iopub.status.idle":"2024-01-24T13:03:43.141103Z","shell.execute_reply.started":"2024-01-24T13:03:40.751662Z","shell.execute_reply":"2024-01-24T13:03:43.140080Z"},"trusted":true},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 1 Axes>","image/png":"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"},"metadata":{}}]}]}